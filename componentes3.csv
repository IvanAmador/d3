text,nome,categoria,descricao
"from crewai import Agent  # type: ignore

from langflow.custom import Component
from langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output


class CrewAIAgentComponent(Component):
    display_name = ""CrewAI Agent""
    description = ""Represents an agent of CrewAI.""
    documentation: str = ""https://docs.crewai.com/how-to/LLM-Connections/""
    icon = ""CrewAI""

    inputs = [
        MultilineInput(name=""role"", display_name=""Role"", info=""The role of the agent.""),
        MultilineInput(name=""goal"", display_name=""Goal"", info=""The objective of the agent.""),
        MultilineInput(name=""backstory"", display_name=""Backstory"", info=""The backstory of the agent.""),
        HandleInput(
            name=""tools"",
            display_name=""Tools"",
            input_types=[""Tool""],
            is_list=True,
            info=""Tools at agents disposal"",
            value=[],
        ),
        HandleInput(
            name=""llm"",
            display_name=""Language Model"",
            info=""Language model that will run the agent."",
            input_types=[""LanguageModel""],
        ),
        BoolInput(
            name=""memory"",
            display_name=""Memory"",
            info=""Whether the agent should have memory or not"",
            advanced=True,
            value=True,
        ),
        BoolInput(
            name=""verbose"",
            display_name=""Verbose"",
            advanced=True,
            value=False,
        ),
        BoolInput(
            name=""allow_delegation"",
            display_name=""Allow Delegation"",
            info=""Whether the agent is allowed to delegate tasks to other agents."",
            value=True,
        ),
        BoolInput(
            name=""allow_code_execution"",
            display_name=""Allow Code Execution"",
            info=""Whether the agent is allowed to execute code."",
            value=False,
            advanced=True,
        ),
        DictInput(
            name=""kwargs"",
            display_name=""kwargs"",
            info=""kwargs of agent."",
            is_list=True,
            advanced=True,
        ),
    ]

    outputs = [
        Output(display_name=""Agent"", name=""output"", method=""build_output""),
    ]

    def build_output(self) -> Agent:
        kwargs = self.kwargs if self.kwargs else {}
        agent = Agent(
            role=self.role,
            goal=self.goal,
            backstory=self.backstory,
            llm=self.llm,
            verbose=self.verbose,
            memory=self.memory,
            tools=self.tools if self.tools else [],
            allow_delegation=self.allow_delegation,
            allow_code_execution=self.allow_code_execution,
            **kwargs,
        )
        self.status = repr(agent)
        return agent
",CrewAIAgent,agents,Representa um agente da CrewAI.
"from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent

from langflow.base.agents.agent import LCAgentComponent
from langflow.field_typing import AgentExecutor
from langflow.inputs import HandleInput, FileInput, DropdownInput


class CSVAgentComponent(LCAgentComponent):
    display_name = ""CSVAgent""
    description = ""Construct a CSV agent from a CSV and tools.""
    documentation = ""https://python.langchain.com/docs/modules/agents/toolkits/csv""
    name = ""CSVAgent""

    inputs = LCAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        FileInput(name=""path"", display_name=""File Path"", file_types=[""csv""], required=True),
        DropdownInput(
            name=""agent_type"",
            display_name=""Agent Type"",
            advanced=True,
            options=[""zero-shot-react-description"", ""openai-functions"", ""openai-tools""],
            value=""openai-tools"",
        ),
    ]

    def build_agent(self) -> AgentExecutor:
        return create_csv_agent(llm=self.llm, path=self.path, agent_type=self.agent_type, **self.get_agent_kwargs())
",CSVAgent,agents,Constrói um agente CSV a partir de um CSV e ferramentas.
"from crewai import Crew, Process  # type: ignore

from langflow.base.agents.crewai.crew import BaseCrewComponent
from langflow.io import HandleInput


class HierarchicalCrewComponent(BaseCrewComponent):
    display_name: str = ""Hierarchical Crew""
    description: str = (
        ""Represents a group of agents, defining how they should collaborate and the tasks they should perform.""
    )
    documentation: str = ""https://docs.crewai.com/how-to/Hierarchical/""
    icon = ""CrewAI""

    inputs = BaseCrewComponent._base_inputs + [
        HandleInput(name=""agents"", display_name=""Agents"", input_types=[""Agent""], is_list=True),
        HandleInput(name=""tasks"", display_name=""Tasks"", input_types=[""HierarchicalTask""], is_list=True),
        HandleInput(name=""manager_llm"", display_name=""Manager LLM"", input_types=[""LanguageModel""], required=False),
        HandleInput(name=""manager_agent"", display_name=""Manager Agent"", input_types=[""Agent""], required=False),
    ]

    def build_crew(self) -> Crew:
        tasks, agents = self.get_tasks_and_agents()
        crew = Crew(
            agents=agents,
            tasks=tasks,
            process=Process.hierarchical,
            verbose=self.verbose,
            memory=self.memory,
            cache=self.use_cache,
            max_rpm=self.max_rpm,
            share_crew=self.share_crew,
            function_calling_llm=self.function_calling_llm,
            manager_agent=self.manager_agent,
            manager_llm=self.manager_llm,
            step_callback=self.get_step_callback(),
            task_callback=self.get_task_callback(),
        )
        return crew
",HierarchicalCrew,agents,"Representa um grupo de agentes, definindo como eles devem colaborar e as tarefas que devem executar."
"from pathlib import Path

import yaml
from langchain.agents import AgentExecutor
from langchain_community.agent_toolkits import create_json_agent
from langchain_community.agent_toolkits.json.toolkit import JsonToolkit
from langchain_community.tools.json.tool import JsonSpec

from langflow.base.agents.agent import LCAgentComponent
from langflow.inputs import HandleInput, FileInput


class JsonAgentComponent(LCAgentComponent):
    display_name = ""JsonAgent""
    description = ""Construct a json agent from an LLM and tools.""
    name = ""JsonAgent""

    inputs = LCAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        FileInput(name=""path"", display_name=""File Path"", file_types=[""json"", ""yaml"", ""yml""], required=True),
    ]

    def build_agent(self) -> AgentExecutor:
        if self.path.endswith(""yaml"") or self.path.endswith(""yml""):
            yaml_dict = yaml.load(open(self.path, ""r""), Loader=yaml.FullLoader)
            spec = JsonSpec(dict_=yaml_dict)
        else:
            spec = JsonSpec.from_file(Path(self.path))
        toolkit = JsonToolkit(spec=spec)

        return create_json_agent(llm=self.llm, toolkit=toolkit, **self.get_agent_kwargs())
",JsonAgent,agents,Constrói um agente json a partir de um LLM e ferramentas.
"from typing import Optional, List

from langchain.agents import create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate

from langflow.base.agents.agent import LCToolsAgentComponent
from langflow.inputs import MultilineInput
from langflow.inputs.inputs import HandleInput, DataInput
from langflow.schema import Data


class OpenAIToolsAgentComponent(LCToolsAgentComponent):
    display_name: str = ""OpenAI Tools Agent""
    description: str = ""Agent that uses tools via openai-tools.""
    icon = ""LangChain""
    beta = True
    name = ""OpenAIToolsAgent""

    inputs = LCToolsAgentComponent._base_inputs + [
        HandleInput(
            name=""llm"",
            display_name=""Language Model"",
            input_types=[""LanguageModel"", ""ToolEnabledLanguageModel""],
            required=True,
        ),
        MultilineInput(
            name=""system_prompt"",
            display_name=""System Prompt"",
            info=""System prompt for the agent."",
            value=""You are a helpful assistant"",
        ),
        MultilineInput(
            name=""user_prompt"", display_name=""Prompt"", info=""This prompt must contain 'input' key."", value=""{input}""
        ),
        DataInput(name=""chat_history"", display_name=""Chat History"", is_list=True, advanced=True),
    ]

    def get_chat_history_data(self) -> Optional[List[Data]]:
        return self.chat_history

    def create_agent_runnable(self):
        if ""input"" not in self.user_prompt:
            raise ValueError(""Prompt must contain 'input' key."")
        messages = [
            (""system"", self.system_prompt),
            (""placeholder"", ""{chat_history}""),
            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[""input""], template=self.user_prompt)),
            (""placeholder"", ""{agent_scratchpad}""),
        ]
        prompt = ChatPromptTemplate.from_messages(messages)
        return create_openai_tools_agent(self.llm, self.tools, prompt)
",OpenAIToolsAgent,agents,Agente que usa ferramentas via openai-tools.
"from pathlib import Path

import yaml
from langchain.agents import AgentExecutor
from langchain_community.agent_toolkits import create_openapi_agent
from langchain_community.tools.json.tool import JsonSpec
from langchain_community.agent_toolkits.openapi.toolkit import OpenAPIToolkit

from langflow.base.agents.agent import LCAgentComponent
from langflow.inputs import BoolInput, HandleInput, FileInput
from langchain_community.utilities.requests import TextRequestsWrapper


class OpenAPIAgentComponent(LCAgentComponent):
    display_name = ""OpenAPI Agent""
    description = ""Agent to interact with OpenAPI API.""
    name = ""OpenAPIAgent""

    inputs = LCAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        FileInput(name=""path"", display_name=""File Path"", file_types=[""json"", ""yaml"", ""yml""], required=True),
        BoolInput(name=""allow_dangerous_requests"", display_name=""Allow Dangerous Requests"", value=False, required=True),
    ]

    def build_agent(self) -> AgentExecutor:
        if self.path.endswith(""yaml"") or self.path.endswith(""yml""):
            yaml_dict = yaml.load(open(self.path, ""r""), Loader=yaml.FullLoader)
            spec = JsonSpec(dict_=yaml_dict)
        else:
            spec = JsonSpec.from_file(Path(self.path))
        requests_wrapper = TextRequestsWrapper()
        toolkit = OpenAPIToolkit.from_llm(
            llm=self.llm,
            json_spec=spec,
            requests_wrapper=requests_wrapper,
            allow_dangerous_requests=self.allow_dangerous_requests,
        )

        agent_args = self.get_agent_kwargs()

        # This is bit weird - generally other create_*_agent functions have max_iterations in the
        # `agent_executor_kwargs`, but openai has this parameter passed directly.
        agent_args[""max_iterations""] = agent_args[""agent_executor_kwargs""][""max_iterations""]
        del agent_args[""agent_executor_kwargs""][""max_iterations""]
        return create_openapi_agent(llm=self.llm, toolkit=toolkit, **agent_args)
",OpenAPIAgent,agents,Agente para interagir com a API OpenAPI.
"from crewai import Agent, Crew, Process, Task  # type: ignore

from langflow.base.agents.crewai.crew import BaseCrewComponent
from langflow.io import HandleInput
from langflow.schema.message import Message


class SequentialCrewComponent(BaseCrewComponent):
    display_name: str = ""Sequential Crew""
    description: str = ""Represents a group of agents with tasks that are executed sequentially.""
    documentation: str = ""https://docs.crewai.com/how-to/Sequential/""
    icon = ""CrewAI""

    inputs = BaseCrewComponent._base_inputs + [
        HandleInput(name=""tasks"", display_name=""Tasks"", input_types=[""SequentialTask""], is_list=True),
    ]

    def get_tasks_and_agents(self) -> tuple[list[Task], list[Agent]]:
        return self.tasks, [task.agent for task in self.tasks]

    def build_crew(self) -> Message:
        tasks, agents = self.get_tasks_and_agents()
        crew = Crew(
            agents=agents,
            tasks=tasks,
            process=Process.sequential,
            verbose=self.verbose,
            memory=self.memory,
            cache=self.use_cache,
            max_rpm=self.max_rpm,
            share_crew=self.share_crew,
            function_calling_llm=self.function_calling_llm,
            step_callback=self.get_step_callback(),
            task_callback=self.get_task_callback(),
        )
        return crew
",SequentialCrew,agents,Representa um grupo de agentes com tarefas que são executadas sequencialmente.
"from langchain.agents import AgentExecutor
from langchain_community.agent_toolkits import SQLDatabaseToolkit
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain_community.utilities import SQLDatabase

from langflow.base.agents.agent import LCAgentComponent
from langflow.inputs import MessageTextInput, HandleInput


class SQLAgentComponent(LCAgentComponent):
    display_name = ""SQLAgent""
    description = ""Construct an SQL agent from an LLM and tools.""
    name = ""SQLAgent""

    inputs = LCAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        MessageTextInput(name=""database_uri"", display_name=""Database URI"", required=True),
        HandleInput(
            name=""extra_tools"",
            display_name=""Extra Tools"",
            input_types=[""Tool"", ""BaseTool""],
            is_list=True,
            advanced=True,
        ),
    ]

    def build_agent(self) -> AgentExecutor:
        db = SQLDatabase.from_uri(self.database_uri)
        toolkit = SQLDatabaseToolkit(db=db, llm=self.llm)
        agent_args = self.get_agent_kwargs()
        agent_args[""max_iterations""] = agent_args[""agent_executor_kwargs""][""max_iterations""]
        del agent_args[""agent_executor_kwargs""][""max_iterations""]
        return create_sql_agent(llm=self.llm, toolkit=toolkit, extra_tools=self.extra_tools or [], **agent_args)
",SQLAgent,agents,Constrói um agente SQL a partir de um LLM e ferramentas.
"from typing import Optional, List

from langchain.agents import create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate
from langflow.base.agents.agent import LCToolsAgentComponent
from langflow.inputs import MultilineInput
from langflow.inputs.inputs import HandleInput, DataInput
from langflow.schema import Data


class ToolCallingAgentComponent(LCToolsAgentComponent):
    display_name: str = ""Tool Calling Agent""
    description: str = ""Agent that uses tools""
    icon = ""LangChain""
    beta = True
    name = ""ToolCallingAgent""

    inputs = LCToolsAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        MultilineInput(
            name=""system_prompt"",
            display_name=""System Prompt"",
            info=""System prompt for the agent."",
            value=""You are a helpful assistant"",
        ),
        MultilineInput(
            name=""user_prompt"", display_name=""Prompt"", info=""This prompt must contain 'input' key."", value=""{input}""
        ),
        DataInput(name=""chat_history"", display_name=""Chat History"", is_list=True, advanced=True),
    ]

    def get_chat_history_data(self) -> Optional[List[Data]]:
        return self.chat_history

    def create_agent_runnable(self):
        if ""input"" not in self.user_prompt:
            raise ValueError(""Prompt must contain 'input' key."")
        messages = [
            (""system"", self.system_prompt),
            (""placeholder"", ""{chat_history}""),
            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[""input""], template=self.user_prompt)),
            (""placeholder"", ""{agent_scratchpad}""),
        ]
        prompt = ChatPromptTemplate.from_messages(messages)
        return create_tool_calling_agent(self.llm, self.tools, prompt)
",ToolCallingAgent,agents,Agente que usa ferramentas
"from langchain.agents import AgentExecutor, create_vectorstore_agent
from langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreToolkit
from langflow.base.agents.agent import LCAgentComponent
from langflow.inputs import HandleInput


class VectorStoreAgentComponent(LCAgentComponent):
    display_name = ""VectorStoreAgent""
    description = ""Construct an agent from a Vector Store.""
    name = ""VectorStoreAgent""

    inputs = LCAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        HandleInput(name=""vectorstore"", display_name=""Vector Store"", input_types=[""VectorStoreInfo""], required=True),
    ]

    def build_agent(self) -> AgentExecutor:
        toolkit = VectorStoreToolkit(vectorstore_info=self.vectorstore, llm=self.llm)
        return create_vectorstore_agent(llm=self.llm, toolkit=toolkit, **self.get_agent_kwargs())
",VectorStoreAgent,agents,Construir um agente a partir de um Vector Store.
"from langchain.agents import create_vectorstore_router_agent
from langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreRouterToolkit

from langflow.base.agents.agent import LCAgentComponent
from langchain.agents import AgentExecutor
from langflow.inputs import HandleInput


class VectorStoreRouterAgentComponent(LCAgentComponent):
    display_name = ""VectorStoreRouterAgent""
    description = ""Construct an agent from a Vector Store Router.""
    name = ""VectorStoreRouterAgent""

    inputs = LCAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        HandleInput(
            name=""vectorstores"",
            display_name=""Vector Stores"",
            input_types=[""VectorStoreInfo""],
            is_list=True,
            required=True,
        ),
    ]

    def build_agent(self) -> AgentExecutor:
        toolkit = VectorStoreRouterToolkit(vectorstores=self.vectorstores, llm=self.llm)
        return create_vectorstore_router_agent(llm=self.llm, toolkit=toolkit, **self.get_agent_kwargs())
",VectorStoreRouterAgent,agents,Construir um agente a partir de um Vector Store Router.
"from langchain.agents import create_xml_agent
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate

from langflow.base.agents.agent import LCToolsAgentComponent
from langflow.inputs import MultilineInput
from langflow.inputs.inputs import HandleInput


class XMLAgentComponent(LCToolsAgentComponent):
    display_name: str = ""XML Agent""
    description: str = ""Agent that uses tools formatting instructions as xml to the Language Model.""
    icon = ""LangChain""
    beta = True
    name = ""XMLAgent""

    inputs = LCToolsAgentComponent._base_inputs + [
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        MultilineInput(
            name=""user_prompt"",
            display_name=""Prompt"",
            value=""""""
You are a helpful assistant. Help the user answer any questions.

You have access to the following tools:

{tools}

In order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>

For example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:

<tool>search</tool><tool_input>weather in SF</tool_input>

<observation>64 degrees</observation>

When you are done, respond with a final answer between <final_answer></final_answer>. For example:

<final_answer>The weather in SF is 64 degrees</final_answer>

Begin!

Question: {input}

{agent_scratchpad}
            """""",
        ),
    ]

    def create_agent_runnable(self):
        messages = [
            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[""input""], template=self.user_prompt))
        ]
        prompt = ChatPromptTemplate.from_messages(messages)
        return create_xml_agent(self.llm, self.tools, prompt)
",XMLAgent,agents,Agente que usa ferramentas formatando instruções como xml para o Language Model.
"from langchain.chains import ConversationChain

from langflow.base.chains.model import LCChainComponent
from langflow.field_typing import Message
from langflow.inputs import MultilineInput, HandleInput


class ConversationChainComponent(LCChainComponent):
    display_name = ""ConversationChain""
    description = ""Chain to have a conversation and load context from memory.""
    name = ""ConversationChain""

    inputs = [
        MultilineInput(
            name=""input_value"", display_name=""Input"", info=""The input value to pass to the chain."", required=True
        ),
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        HandleInput(
            name=""memory"",
            display_name=""Memory"",
            input_types=[""BaseChatMemory""],
        ),
    ]

    def invoke_chain(self) -> Message:
        if not self.memory:
            chain = ConversationChain(llm=self.llm)
        else:
            chain = ConversationChain(llm=self.llm, memory=self.memory)

        result = chain.invoke({""input"": self.input_value})
        if isinstance(result, dict):
            result = result.get(chain.output_key, """")  # type: ignore

        elif isinstance(result, str):
            result = result
        else:
            result = result.get(""response"")
        result = str(result)
        self.status = result
        return Message(text=result)
",ConversationChain,chains,Cadeia para ter uma conversa e carregar contexto da memória.
"from langchain.chains import LLMCheckerChain

from langflow.base.chains.model import LCChainComponent
from langflow.field_typing import Message
from langflow.inputs import MultilineInput, HandleInput


class LLMCheckerChainComponent(LCChainComponent):
    display_name = ""LLMCheckerChain""
    description = ""Chain for question-answering with self-verification.""
    documentation = ""https://python.langchain.com/docs/modules/chains/additional/llm_checker""
    name = ""LLMCheckerChain""

    inputs = [
        MultilineInput(
            name=""input_value"", display_name=""Input"", info=""The input value to pass to the chain."", required=True
        ),
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
    ]

    def invoke_chain(self) -> Message:
        chain = LLMCheckerChain.from_llm(llm=self.llm)
        response = chain.invoke({chain.input_key: self.input_value})
        result = response.get(chain.output_key, """")
        result = str(result)
        self.status = result
        return Message(text=result)
",LLMCheckerChain,chains,Cadeia para perguntas e respostas com auto-verificação.
"from langchain.chains import LLMMathChain

from langflow.base.chains.model import LCChainComponent
from langflow.field_typing import Message
from langflow.inputs import MultilineInput, HandleInput
from langflow.template import Output


class LLMMathChainComponent(LCChainComponent):
    display_name = ""LLMMathChain""
    description = ""Chain that interprets a prompt and executes python code to do math.""
    documentation = ""https://python.langchain.com/docs/modules/chains/additional/llm_math""
    name = ""LLMMathChain""

    inputs = [
        MultilineInput(
            name=""input_value"", display_name=""Input"", info=""The input value to pass to the chain."", required=True
        ),
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
    ]

    outputs = [Output(display_name=""Text"", name=""text"", method=""invoke_chain"")]

    def invoke_chain(self) -> Message:
        chain = LLMMathChain.from_llm(llm=self.llm)
        response = chain.invoke({chain.input_key: self.input_value})
        result = response.get(chain.output_key, """")
        result = str(result)
        self.status = result
        return Message(text=result)
",LLMMathChain,chains,Cadeia que interpreta um prompt e executa código python para fazer matemática.
"from langchain.chains import RetrievalQA

from langflow.base.chains.model import LCChainComponent
from langflow.field_typing import Message
from langflow.inputs import HandleInput, MultilineInput, BoolInput, DropdownInput


class RetrievalQAComponent(LCChainComponent):
    display_name = ""Retrieval QA""
    description = ""Chain for question-answering querying sources from a retriever.""
    name = ""RetrievalQA""

    inputs = [
        MultilineInput(
            name=""input_value"", display_name=""Input"", info=""The input value to pass to the chain."", required=True
        ),
        DropdownInput(
            name=""chain_type"",
            display_name=""Chain Type"",
            info=""Chain type to use."",
            options=[""Stuff"", ""Map Reduce"", ""Refine"", ""Map Rerank""],
            value=""Stuff"",
            advanced=True,
        ),
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        HandleInput(name=""retriever"", display_name=""Retriever"", input_types=[""Retriever""], required=True),
        HandleInput(
            name=""memory"",
            display_name=""Memory"",
            input_types=[""BaseChatMemory""],
        ),
        BoolInput(
            name=""return_source_documents"",
            display_name=""Return Source Documents"",
            value=False,
        ),
    ]

    def invoke_chain(self) -> Message:
        chain_type = self.chain_type.lower().replace("" "", ""_"")
        if self.memory:
            self.memory.input_key = ""query""
            self.memory.output_key = ""result""

        runnable = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type=chain_type,
            retriever=self.retriever,
            memory=self.memory,
            # always include to help debugging
            #
            return_source_documents=True,
        )

        result = runnable.invoke({""query"": self.input_value})

        source_docs = self.to_data(result.get(""source_documents"", []))
        result_str = str(result.get(""result"", """"))
        if self.return_source_documents and len(source_docs):
            references_str = self.create_references_from_data(source_docs)
            result_str = ""\n"".join([result_str, references_str])
        # put the entire result to debug history, query and content
        self.status = {**result, ""source_documents"": source_docs, ""output"": result_str}
        return result_str
",RetrievalQA,chains,Cadeia para perguntas e respostas consultando fontes de um recuperador.
"from langchain.chains import create_sql_query_chain
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langflow.base.chains.model import LCChainComponent
from langflow.field_typing import Message
from langflow.inputs import MultilineInput, HandleInput, IntInput
from langflow.template import Output


class SQLGeneratorComponent(LCChainComponent):
    display_name = ""Natural Language to SQL""
    description = ""Generate SQL from natural language.""
    name = ""SQLGenerator""

    inputs = [
        MultilineInput(
            name=""input_value"", display_name=""Input"", info=""The input value to pass to the chain."", required=True
        ),
        HandleInput(name=""llm"", display_name=""Language Model"", input_types=[""LanguageModel""], required=True),
        HandleInput(name=""db"", display_name=""SQLDatabase"", input_types=[""SQLDatabase""], required=True),
        IntInput(
            name=""top_k"", display_name=""Top K"", info=""The number of results per select statement to return."", value=5
        ),
        MultilineInput(name=""prompt"", display_name=""Prompt"", info=""The prompt must contain `{question}`.""),
    ]

    outputs = [Output(display_name=""Text"", name=""text"", method=""invoke_chain"")]

    def invoke_chain(self) -> Message:
        if self.prompt:
            prompt_template = PromptTemplate.from_template(template=self.prompt)
        else:
            prompt_template = None

        if self.top_k < 1:
            raise ValueError(""Top K must be greater than 0."")

        if not prompt_template:
            sql_query_chain = create_sql_query_chain(llm=self.llm, db=self.db, k=self.top_k)
        else:
            # Check if {question} is in the prompt
            if ""{question}"" not in prompt_template.template or ""question"" not in prompt_template.input_variables:
                raise ValueError(""Prompt must contain `{question}` to be used with Natural Language to SQL."")
            sql_query_chain = create_sql_query_chain(llm=self.llm, db=self.db, prompt=prompt_template, k=self.top_k)
        query_writer: Runnable = sql_query_chain | {""query"": lambda x: x.replace(""SQLQuery:"", """").strip()}
        response = query_writer.invoke({""question"": self.input_value})
        query = response.get(""query"")
        self.status = query
        return query
",SQLGenerator,chains,Gerar SQL a partir de linguagem natural.
"import asyncio
import json
from typing import Any, List, Optional
from urllib.parse import parse_qsl, urlencode, urlparse, urlunparse

import httpx
from loguru import logger

from langflow.base.curl.parse import parse_context
from langflow.custom import Component
from langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output
from langflow.schema import Data
from langflow.schema.dotdict import dotdict


class APIRequestComponent(Component):
    display_name = ""API Request""
    description = (
        ""This component allows you to make HTTP requests to one or more URLs. ""
        ""You can provide headers and body as either dictionaries or Data objects. ""
        ""Additionally, you can append query parameters to the URLs.\n\n""
        ""**Note:** Check advanced options for more settings.""
    )
    icon = ""Globe""
    name = ""APIRequest""

    inputs = [
        MessageTextInput(
            name=""urls"",
            display_name=""URLs"",
            is_list=True,
            info=""Enter one or more URLs, separated by commas."",
        ),
        MessageTextInput(
            name=""curl"",
            display_name=""Curl"",
            info=""Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body."",
            advanced=False,
            refresh_button=True,
        ),
        DropdownInput(
            name=""method"",
            display_name=""Method"",
            options=[""GET"", ""POST"", ""PATCH"", ""PUT""],
            value=""GET"",
            info=""The HTTP method to use (GET, POST, PATCH, PUT)."",
        ),
        NestedDictInput(
            name=""headers"",
            display_name=""Headers"",
            info=""The headers to send with the request as a dictionary. This is populated when using the CURL field."",
            input_types=[""Data""],
        ),
        NestedDictInput(
            name=""body"",
            display_name=""Body"",
            info=""The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field."",
            input_types=[""Data""],
        ),
        DataInput(
            name=""query_params"",
            display_name=""Query Parameters"",
            info=""The query parameters to append to the URL."",
        ),
        IntInput(
            name=""timeout"",
            display_name=""Timeout"",
            value=5,
            info=""The timeout to use for the request."",
        ),
    ]

    outputs = [
        Output(display_name=""Data"", name=""data"", method=""make_requests""),
    ]

    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:
        try:
            parsed = parse_context(curl)
            build_config[""urls""][""value""] = [parsed.url]
            build_config[""method""][""value""] = parsed.method.upper()
            build_config[""headers""][""value""] = dict(parsed.headers)

            if parsed.data:
                try:
                    json_data = json.loads(parsed.data)
                    build_config[""body""][""value""] = json_data
                except json.JSONDecodeError as e:
                    logger.error(f""Error decoding JSON data: {e}"")
            else:
                build_config[""body""][""value""] = {}
        except Exception as exc:
            logger.error(f""Error parsing curl: {exc}"")
            raise ValueError(f""Error parsing curl: {exc}"")
        return build_config

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        if field_name == ""curl"" and field_value:
            build_config = self.parse_curl(field_value, build_config)
        return build_config

    async def make_request(
        self,
        client: httpx.AsyncClient,
        method: str,
        url: str,
        headers: Optional[dict] = None,
        body: Optional[dict] = None,
        timeout: int = 5,
    ) -> Data:
        method = method.upper()
        if method not in [""GET"", ""POST"", ""PATCH"", ""PUT"", ""DELETE""]:
            raise ValueError(f""Unsupported method: {method}"")

        if isinstance(body, str) and body:
            try:
                body = json.loads(body)
            except Exception as e:
                logger.error(f""Error decoding JSON data: {e}"")
                body = None
                raise ValueError(f""Error decoding JSON data: {e}"")

        data = body if body else None

        try:
            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)
            try:
                result = response.json()
            except Exception:
                result = response.text
            return Data(
                data={
                    ""source"": url,
                    ""headers"": headers,
                    ""status_code"": response.status_code,
                    ""result"": result,
                },
            )
        except httpx.TimeoutException:
            return Data(
                data={
                    ""source"": url,
                    ""headers"": headers,
                    ""status_code"": 408,
                    ""error"": ""Request timed out"",
                },
            )
        except Exception as exc:
            return Data(
                data={
                    ""source"": url,
                    ""headers"": headers,
                    ""status_code"": 500,
                    ""error"": str(exc),
                },
            )

    def add_query_params(self, url: str, params: dict) -> str:
        url_parts = list(urlparse(url))
        query = dict(parse_qsl(url_parts[4]))
        query.update(params)
        url_parts[4] = urlencode(query)
        return urlunparse(url_parts)

    async def make_requests(self) -> List[Data]:
        method = self.method
        urls = [url.strip() for url in self.urls if url.strip()]
        curl = self.curl
        headers = self.headers or {}
        body = self.body or {}
        timeout = self.timeout
        query_params = self.query_params.data if self.query_params else {}

        if curl:
            self._build_config = self.parse_curl(curl, dotdict())

        if isinstance(headers, Data):
            headers = headers.data

        if isinstance(body, Data):
            body = body.data

        bodies = [body] * len(urls)

        urls = [self.add_query_params(url, query_params) for url in urls]

        async with httpx.AsyncClient() as client:
            results = await asyncio.gather(
                *[self.make_request(client, method, u, headers, rec, timeout) for u, rec in zip(urls, bodies)]
            )
        self.status = results
        return results
",APIRequest,data,"Este componente permite que você faça solicitações HTTP para uma ou mais URLs. Você pode fornecer cabeçalhos e corpo como dicionários ou objetos Data. Além disso, você pode anexar parâmetros de consulta às URLs."
"from typing import List

from langflow.base.data.utils import parallel_load_data, parse_text_file_to_data, retrieve_file_paths
from langflow.custom import Component
from langflow.io import BoolInput, IntInput, MessageTextInput
from langflow.schema import Data
from langflow.template import Output


class DirectoryComponent(Component):
    display_name = ""Directory""
    description = ""Recursively load files from a directory.""
    icon = ""folder""
    name = ""Directory""

    inputs = [
        MessageTextInput(
            name=""path"",
            display_name=""Path"",
            info=""Path to the directory to load files from."",
        ),
        MessageTextInput(
            name=""types"",
            display_name=""Types"",
            info=""File types to load. Leave empty to load all types."",
            is_list=True,
        ),
        IntInput(
            name=""depth"",
            display_name=""Depth"",
            info=""Depth to search for files."",
            value=0,
        ),
        IntInput(
            name=""max_concurrency"",
            display_name=""Max Concurrency"",
            advanced=True,
            info=""Maximum concurrency for loading files."",
            value=2,
        ),
        BoolInput(
            name=""load_hidden"",
            display_name=""Load Hidden"",
            advanced=True,
            info=""If true, hidden files will be loaded."",
        ),
        BoolInput(
            name=""recursive"",
            display_name=""Recursive"",
            advanced=True,
            info=""If true, the search will be recursive."",
        ),
        BoolInput(
            name=""silent_errors"",
            display_name=""Silent Errors"",
            advanced=True,
            info=""If true, errors will not raise an exception."",
        ),
        BoolInput(
            name=""use_multithreading"",
            display_name=""Use Multithreading"",
            advanced=True,
            info=""If true, multithreading will be used."",
        ),
    ]

    outputs = [
        Output(display_name=""Data"", name=""data"", method=""load_directory""),
    ]

    def load_directory(self) -> List[Data]:
        path = self.path
        types = self.types or []  # self.types is already a list due to is_list=True
        depth = self.depth
        max_concurrency = self.max_concurrency
        load_hidden = self.load_hidden
        recursive = self.recursive
        silent_errors = self.silent_errors
        use_multithreading = self.use_multithreading

        resolved_path = self.resolve_path(path)
        file_paths = retrieve_file_paths(resolved_path, load_hidden, recursive, depth)

        if types:
            file_paths = [fp for fp in file_paths if any(fp.endswith(ext) for ext in types)]

        loaded_data = []

        if use_multithreading:
            loaded_data = parallel_load_data(file_paths, silent_errors, max_concurrency)
        else:
            loaded_data = [parse_text_file_to_data(file_path, silent_errors) for file_path in file_paths]
        loaded_data = list(filter(None, loaded_data))
        self.status = loaded_data
        return loaded_data  # type: ignore
",Directory,data,Carregar arquivos recursivamente de um diretório.
"from pathlib import Path

from langflow.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data
from langflow.custom import Component
from langflow.io import BoolInput, FileInput, Output
from langflow.schema import Data


class FileComponent(Component):
    display_name = ""File""
    description = ""A generic file loader.""
    icon = ""file-text""
    name = ""File""

    inputs = [
        FileInput(
            name=""path"",
            display_name=""Path"",
            file_types=TEXT_FILE_TYPES,
            info=f""Supported file types: {', '.join(TEXT_FILE_TYPES)}"",
        ),
        BoolInput(
            name=""silent_errors"",
            display_name=""Silent Errors"",
            advanced=True,
            info=""If true, errors will not raise an exception."",
        ),
    ]

    outputs = [
        Output(display_name=""Data"", name=""data"", method=""load_file""),
    ]

    def load_file(self) -> Data:
        if not self.path:
            raise ValueError(""Please, upload a file to use this component."")
        resolved_path = self.resolve_path(self.path)
        silent_errors = self.silent_errors

        extension = Path(resolved_path).suffix[1:].lower()

        if extension == ""doc"":
            raise ValueError(""doc files are not supported. Please save as .docx"")
        if extension not in TEXT_FILE_TYPES:
            raise ValueError(f""Unsupported file type: {extension}"")

        data = parse_text_file_to_data(resolved_path, silent_errors)
        self.status = data if data else ""No data""
        return data or Data()
",File,data,Um carregador de arquivos genérico.
"import re

from langchain_community.document_loaders.web_base import WebBaseLoader

from langflow.custom import Component
from langflow.io import MessageTextInput, Output
from langflow.schema import Data


class URLComponent(Component):
    display_name = ""URL""
    description = ""Fetch content from one or more URLs.""
    icon = ""layout-template""
    name = ""URL""

    inputs = [
        MessageTextInput(
            name=""urls"",
            display_name=""URLs"",
            info=""Enter one or more URLs, separated by commas."",
            is_list=True,
        ),
    ]

    outputs = [
        Output(display_name=""Data"", name=""data"", method=""fetch_content""),
    ]

    def ensure_url(self, string: str) -> str:
        """"""
        Ensures the given string is a URL by adding 'http://' if it doesn't start with 'http://' or 'https://'.
        Raises an error if the string is not a valid URL.

        Parameters:
            string (str): The string to be checked and possibly modified.

        Returns:
            str: The modified string that is ensured to be a URL.

        Raises:
            ValueError: If the string is not a valid URL.
        """"""
        if not string.startswith((""http://"", ""https://"")):
            string = ""http://"" + string

        # Basic URL validation regex
        url_regex = re.compile(
            r""^(https?:\/\/)?""  # optional protocol
            r""(www\.)?""  # optional www
            r""([a-zA-Z0-9.-]+)""  # domain
            r""(\.[a-zA-Z]{2,})?""  # top-level domain
            r""(:\d+)?""  # optional port
            r""(\/[^\s]*)?$"",  # optional path
            re.IGNORECASE,
        )

        if not url_regex.match(string):
            raise ValueError(f""Invalid URL: {string}"")

        return string

    def fetch_content(self) -> list[Data]:
        urls = [self.ensure_url(url.strip()) for url in self.urls if url.strip()]
        loader = WebBaseLoader(web_paths=urls, encoding=""utf-8"")
        docs = loader.load()
        data = [Data(text=doc.page_content, **doc.metadata) for doc in docs]
        self.status = data
        return data
",URL,data,Buscar conteúdo de uma ou mais URLs.
"import json

from langflow.custom import Component
from langflow.io import MultilineInput, Output
from langflow.schema import Data


class WebhookComponent(Component):
    display_name = ""Webhook Input""
    description = ""Defines a webhook input for the flow.""
    name = ""Webhook""

    inputs = [
        MultilineInput(
            name=""data"",
            display_name=""Data"",
            info=""Use this field to quickly test the webhook component by providing a JSON payload."",
        )
    ]
    outputs = [
        Output(display_name=""Data"", name=""output_data"", method=""build_data""),
    ]

    def build_data(self) -> Data:
        message: str | Data = """"
        if not self.data:
            self.status = ""No data provided.""
            return Data(data={})
        try:
            body = json.loads(self.data or ""{}"")
        except json.JSONDecodeError:
            body = {""payload"": self.data}
            message = f""Invalid JSON payload. Please check the format.\n\n{self.data}""
        data = Data(data=body)
        if not message:
            message = data
        self.status = message
        return data
",Webhook,data,Define uma entrada de webhook para o fluxo.
"from typing import List
from langflow.custom import Component
from langflow.io import StrInput, SecretStrInput, BoolInput, DropdownInput, Output, IntInput
from langflow.schema import Data
from langchain_community.document_loaders import ConfluenceLoader
from langchain_community.document_loaders.confluence import ContentFormat


class ConfluenceComponent(Component):
    display_name = ""Confluence""
    description = ""Confluence wiki collaboration platform""
    documentation = ""https://python.langchain.com/v0.2/docs/integrations/document_loaders/confluence/""
    trace_type = ""tool""
    icon = ""Confluence""
    name = ""Confluence""

    inputs = [
        StrInput(
            name=""url"",
            display_name=""Site URL"",
            required=True,
            info=""The base URL of the Confluence Space. Example: https://<company>.atlassian.net/wiki."",
        ),
        StrInput(
            name=""username"",
            display_name=""Username"",
            required=True,
            info=""Atlassian User E-mail. Example: email@example.com"",
        ),
        SecretStrInput(
            name=""api_key"",
            display_name=""API Key"",
            required=True,
            info=""Atlassian Key. Create at: https://id.atlassian.com/manage-profile/security/api-tokens"",
        ),
        StrInput(name=""space_key"", display_name=""Space Key"", required=True),
        BoolInput(name=""cloud"", display_name=""Use Cloud?"", required=True, value=True, advanced=True),
        DropdownInput(
            name=""content_format"",
            display_name=""Content Format"",
            options=[
                ContentFormat.EDITOR.value,
                ContentFormat.EXPORT_VIEW.value,
                ContentFormat.ANONYMOUS_EXPORT_VIEW.value,
                ContentFormat.STORAGE.value,
                ContentFormat.VIEW.value,
            ],
            value=ContentFormat.STORAGE.value,
            required=True,
            advanced=True,
            info=""Specify content format, defaults to ContentFormat.STORAGE"",
        ),
        IntInput(
            name=""max_pages"",
            display_name=""Max Pages"",
            required=False,
            value=1000,
            advanced=True,
            info=""Maximum number of pages to retrieve in total, defaults 1000"",
        ),
    ]

    outputs = [
        Output(name=""data"", display_name=""Data"", method=""load_documents""),
    ]

    def build_confluence(self) -> ConfluenceLoader:
        content_format = ContentFormat(self.content_format)
        loader = ConfluenceLoader(
            url=self.url,
            username=self.username,
            api_key=self.api_key,
            cloud=self.cloud,
            space_key=self.space_key,
            content_format=content_format,
            max_pages=self.max_pages,
        )
        return loader

    def load_documents(self) -> List[Data]:
        confluence = self.build_confluence()
        documents = confluence.load()
        data = [Data.from_document(doc) for doc in documents]  # Using the from_document method of Data
        self.status = data
        return data
",Confluence,documentloaders,Plataforma de colaboração wiki da Confluence
"from pathlib import Path
from typing import List
import re

from langchain_community.document_loaders.git import GitLoader
from langflow.custom import Component
from langflow.io import MessageTextInput, Output
from langflow.schema import Data


class GitLoaderComponent(Component):
    display_name = ""GitLoader""
    description = ""Load files from a Git repository""
    documentation = ""https://python.langchain.com/v0.2/docs/integrations/document_loaders/git/""
    trace_type = ""tool""
    icon = ""GitLoader""
    name = ""GitLoader""

    inputs = [
        MessageTextInput(
            name=""repo_path"",
            display_name=""Repository Path"",
            required=True,
            info=""The local path to the Git repository."",
        ),
        MessageTextInput(
            name=""clone_url"",
            display_name=""Clone URL"",
            required=False,
            info=""The URL to clone the Git repository from."",
        ),
        MessageTextInput(
            name=""branch"",
            display_name=""Branch"",
            required=False,
            value=""main"",
            info=""The branch to load files from. Defaults to 'main'."",
        ),
        MessageTextInput(
            name=""file_filter"",
            display_name=""File Filter"",
            required=False,
            advanced=True,
            info=""A list of patterns to filter files. Example to include only .py files: '*.py'. ""
            ""Example to exclude .py files: '!*.py'. Multiple patterns can be separated by commas."",
        ),
        MessageTextInput(
            name=""content_filter"",
            display_name=""Content Filter"",
            required=False,
            advanced=True,
            info=""A regex pattern to filter files based on their content."",
        ),
    ]

    outputs = [
        Output(name=""data"", display_name=""Data"", method=""load_documents""),
    ]

    @staticmethod
    def is_binary(file_path: str) -> bool:
        """"""
        Check if a file is binary by looking for null bytes.
        This is necessary because when searches are performed using
        the content_filter, binary files need to be ignored.
        """"""
        with open(file_path, ""rb"") as file:
            return b""\x00"" in file.read(1024)

    def build_gitloader(self) -> GitLoader:
        file_filter_patterns = getattr(self, ""file_filter"", None)
        content_filter_pattern = getattr(self, ""content_filter"", None)

        file_filters = []
        if file_filter_patterns:
            patterns = [pattern.strip() for pattern in file_filter_patterns.split("","")]

            def file_filter(file_path: Path) -> bool:
                if len(patterns) == 1 and patterns[0].startswith(""!""):
                    return not file_path.match(patterns[0][1:])
                included = any(file_path.match(pattern) for pattern in patterns if not pattern.startswith(""!""))
                excluded = any(file_path.match(pattern[1:]) for pattern in patterns if pattern.startswith(""!""))
                return included and not excluded

            file_filters.append(file_filter)

        if content_filter_pattern:
            content_regex = re.compile(content_filter_pattern)

            def content_filter(file_path: Path) -> bool:
                with file_path.open(""r"", encoding=""utf-8"", errors=""ignore"") as file:
                    content = file.read()
                    return bool(content_regex.search(content))

            file_filters.append(content_filter)

        def combined_filter(file_path: str) -> bool:
            path = Path(file_path)
            if self.is_binary(file_path):
                return False
            return all(f(path) for f in file_filters)

        loader = GitLoader(
            repo_path=self.repo_path,
            clone_url=self.clone_url,
            branch=self.branch,
            file_filter=combined_filter,
        )
        return loader

    def load_documents(self) -> List[Data]:
        gitloader = self.build_gitloader()
        documents = list(gitloader.lazy_load())
        data = [Data.from_document(doc) for doc in documents]
        self.status = data
        return data
",GitLoader,documentloaders,Carregar arquivos de um repositório Git
"from langflow.base.embeddings.model import LCEmbeddingsModel
from langflow.base.models.aiml_constants import AIML_EMBEDDING_MODELS
from langflow.components.embeddings.util.AIMLEmbeddingsImpl import AIMLEmbeddingsImpl
from langflow.field_typing import Embeddings
from langflow.inputs.inputs import DropdownInput
from langflow.io import SecretStrInput


class AIMLEmbeddingsComponent(LCEmbeddingsModel):
    display_name = ""AI/ML Embeddings""
    description = ""Generate embeddings using the AI/ML API.""
    icon = ""AI/ML""
    name = ""AIMLEmbeddings""

    inputs = [
        DropdownInput(
            name=""model_name"",
            display_name=""Model Name"",
            options=AIML_EMBEDDING_MODELS,
            required=True,
        ),
        SecretStrInput(
            name=""aiml_api_key"",
            display_name=""AI/ML API Key"",
            value=""AIML_API_KEY"",
            required=True,
        ),
    ]

    def build_embeddings(self) -> Embeddings:
        return AIMLEmbeddingsImpl(
            api_key=self.aiml_api_key,
            model=self.model_name,
        )
",AIMLEmbeddings,embeddings,Gera embeddings usando a API AI/ML.
"from langchain_community.embeddings import BedrockEmbeddings

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.inputs import SecretStrInput
from langflow.io import DropdownInput, MessageTextInput, Output


class AmazonBedrockEmbeddingsComponent(LCModelComponent):
    display_name: str = ""Amazon Bedrock Embeddings""
    description: str = ""Generate embeddings using Amazon Bedrock models.""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/text_embedding/integrations/bedrock""
    icon = ""Amazon""
    name = ""AmazonBedrockEmbeddings""

    inputs = [
        DropdownInput(
            name=""model_id"",
            display_name=""Model Id"",
            options=[""amazon.titan-embed-text-v1""],
            value=""amazon.titan-embed-text-v1"",
        ),
        SecretStrInput(name=""aws_access_key"", display_name=""Access Key""),
        SecretStrInput(name=""aws_secret_key"", display_name=""Secret Key""),
        MessageTextInput(
            name=""credentials_profile_name"",
            display_name=""Credentials Profile Name"",
            advanced=True,
        ),
        MessageTextInput(name=""region_name"", display_name=""Region Name"", value=""us-east-1""),
        MessageTextInput(name=""endpoint_url"", display_name="" Endpoint URL"", advanced=True),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        if self.aws_access_key:
            import boto3  # type: ignore

            session = boto3.Session(
                aws_access_key_id=self.aws_access_key,
                aws_secret_access_key=self.aws_secret_key,
            )
        elif self.credentials_profile_name:
            import boto3

            session = boto3.Session(profile_name=self.credentials_profile_name)
        else:
            import boto3

            session = boto3.Session()

        client_params = {}
        if self.endpoint_url:
            client_params[""endpoint_url""] = self.endpoint_url
        if self.region_name:
            client_params[""region_name""] = self.region_name

        boto3_client = session.client(""bedrock-runtime"", **client_params)
        output = BedrockEmbeddings(
            credentials_profile_name=self.credentials_profile_name,
            client=boto3_client,
            model_id=self.model_id,
            endpoint_url=self.endpoint_url,
            region_name=self.region_name,
        )  # type: ignore
        return output
",AmazonBedrockEmbeddings,embeddings,Gera embeddings usando modelos da Amazon Bedrock.
"from typing import Any
from langflow.custom import Component
from langflow.inputs.inputs import DictInput, SecretStrInput, MessageTextInput, DropdownInput
from langflow.template.field.base import Output


class AstraVectorizeComponent(Component):
    display_name: str = ""Astra Vectorize""
    description: str = ""Configuration options for Astra Vectorize server-side embeddings.""
    documentation: str = ""https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html""
    icon = ""AstraDB""
    name = ""AstraVectorize""

    VECTORIZE_PROVIDERS_MAPPING = {
        ""Azure OpenAI"": [""azureOpenAI"", [""text-embedding-3-small"", ""text-embedding-3-large"", ""text-embedding-ada-002""]],
        ""Hugging Face - Dedicated"": [""huggingfaceDedicated"", [""endpoint-defined-model""]],
        ""Hugging Face - Serverless"": [
            ""huggingface"",
            [
                ""sentence-transformers/all-MiniLM-L6-v2"",
                ""intfloat/multilingual-e5-large"",
                ""intfloat/multilingual-e5-large-instruct"",
                ""BAAI/bge-small-en-v1.5"",
                ""BAAI/bge-base-en-v1.5"",
                ""BAAI/bge-large-en-v1.5"",
            ],
        ],
        ""Jina AI"": [
            ""jinaAI"",
            [
                ""jina-embeddings-v2-base-en"",
                ""jina-embeddings-v2-base-de"",
                ""jina-embeddings-v2-base-es"",
                ""jina-embeddings-v2-base-code"",
                ""jina-embeddings-v2-base-zh"",
            ],
        ],
        ""Mistral AI"": [""mistral"", [""mistral-embed""]],
        ""NVIDIA"": [""nvidia"", [""NV-Embed-QA""]],
        ""OpenAI"": [""openai"", [""text-embedding-3-small"", ""text-embedding-3-large"", ""text-embedding-ada-002""]],
        ""Upstage"": [""upstageAI"", [""solar-embedding-1-large""]],
        ""Voyage AI"": [
            ""voyageAI"",
            [""voyage-large-2-instruct"", ""voyage-law-2"", ""voyage-code-2"", ""voyage-large-2"", ""voyage-2""],
        ],
    }
    VECTORIZE_MODELS_STR = ""\n\n"".join(
        [provider + "": "" + ("", "".join(models[1])) for provider, models in VECTORIZE_PROVIDERS_MAPPING.items()]
    )

    inputs = [
        DropdownInput(
            name=""provider"",
            display_name=""Provider"",
            options=VECTORIZE_PROVIDERS_MAPPING.keys(),
            value="""",
            required=True,
        ),
        MessageTextInput(
            name=""model_name"",
            display_name=""Model Name"",
            info=f""The embedding model to use for the selected provider. Each provider has a different set of models ""
            f""available (https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html):\n\n{VECTORIZE_MODELS_STR}"",
            required=True,
        ),
        MessageTextInput(
            name=""api_key_name"",
            display_name=""Provider API Key Name"",
            info=""The name of the embeddings provider API key stored on Astra."",
        ),
        SecretStrInput(
            name=""provider_api_key"",
            display_name=""Provider API Key"",
            info=""An alternative to the Astra Authentication that passes an API key for the provider with each request to Astra DB. This may be used when Vectorize is configured for the collection, but no corresponding provider secret is stored within Astra's key management system."",
            advanced=True,
        ),
        DictInput(
            name=""authentication"",
            display_name=""Authentication Parameters"",
            is_list=True,
            advanced=True,
        ),
        DictInput(
            name=""model_parameters"",
            display_name=""Model Parameters"",
            advanced=True,
            is_list=True,
        ),
    ]
    outputs = [
        Output(display_name=""Vectorize"", name=""config"", method=""build_options"", types=[""dict""]),
    ]

    def build_options(self) -> dict[str, Any]:
        provider_value = self.VECTORIZE_PROVIDERS_MAPPING[self.provider][0]
        authentication = {**(self.authentication or {})}
        api_key_name = self.api_key_name
        if api_key_name:
            authentication[""providerKey""] = api_key_name
        return {
            # must match astrapy.info.CollectionVectorServiceOptions
            ""collection_vector_service_options"": {
                ""provider"": provider_value,
                ""modelName"": self.model_name,
                ""authentication"": authentication,
                ""parameters"": self.model_parameters or {},
            },
            ""collection_embedding_api_key"": self.provider_api_key,
        }
",AstraVectorize,embeddings,Opções de configuração para embeddings do lado do servidor da Astra Vectorize.
"from langchain_openai import AzureOpenAIEmbeddings

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput


class AzureOpenAIEmbeddingsComponent(LCModelComponent):
    display_name: str = ""Azure OpenAI Embeddings""
    description: str = ""Generate embeddings using Azure OpenAI models.""
    documentation: str = ""https://python.langchain.com/docs/integrations/text_embedding/azureopenai""
    icon = ""Azure""
    name = ""AzureOpenAIEmbeddings""

    API_VERSION_OPTIONS = [
        ""2022-12-01"",
        ""2023-03-15-preview"",
        ""2023-05-15"",
        ""2023-06-01-preview"",
        ""2023-07-01-preview"",
        ""2023-08-01-preview"",
    ]

    inputs = [
        MessageTextInput(
            name=""azure_endpoint"",
            display_name=""Azure Endpoint"",
            required=True,
            info=""Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`"",
        ),
        MessageTextInput(
            name=""azure_deployment"",
            display_name=""Deployment Name"",
            required=True,
        ),
        DropdownInput(
            name=""api_version"",
            display_name=""API Version"",
            options=API_VERSION_OPTIONS,
            value=API_VERSION_OPTIONS[-1],
            advanced=True,
        ),
        SecretStrInput(
            name=""api_key"",
            display_name=""API Key"",
            required=True,
        ),
        IntInput(
            name=""dimensions"",
            display_name=""Dimensions"",
            info=""The number of dimensions the resulting output embeddings should have. Only supported by certain models."",
            advanced=True,
        ),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        try:
            embeddings = AzureOpenAIEmbeddings(
                azure_endpoint=self.azure_endpoint,
                azure_deployment=self.azure_deployment,
                api_version=self.api_version,
                api_key=self.api_key,
                dimensions=self.dimensions or None,
            )
        except Exception as e:
            raise ValueError(f""Could not connect to AzureOpenAIEmbeddings API: {str(e)}"") from e

        return embeddings
",AzureOpenAIEmbeddings,embeddings,Gera embeddings usando modelos da Azure OpenAI.
"from langchain_community.embeddings.cohere import CohereEmbeddings

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput


class CohereEmbeddingsComponent(LCModelComponent):
    display_name = ""Cohere Embeddings""
    description = ""Generate embeddings using Cohere models.""
    icon = ""Cohere""
    name = ""CohereEmbeddings""

    inputs = [
        SecretStrInput(name=""cohere_api_key"", display_name=""Cohere API Key""),
        DropdownInput(
            name=""model"",
            display_name=""Model"",
            advanced=True,
            options=[
                ""embed-english-v2.0"",
                ""embed-multilingual-v2.0"",
                ""embed-english-light-v2.0"",
                ""embed-multilingual-light-v2.0"",
            ],
            value=""embed-english-v2.0"",
        ),
        MessageTextInput(name=""truncate"", display_name=""Truncate"", advanced=True),
        IntInput(name=""max_retries"", display_name=""Max Retries"", value=3, advanced=True),
        MessageTextInput(name=""user_agent"", display_name=""User Agent"", advanced=True, value=""langchain""),
        FloatInput(name=""request_timeout"", display_name=""Request Timeout"", advanced=True),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        return CohereEmbeddings(  # type: ignore
            cohere_api_key=self.cohere_api_key,
            model=self.model,
            truncate=self.truncate,
            max_retries=self.max_retries,
            user_agent=self.user_agent,
            request_timeout=self.request_timeout or None,
        )
",CohereEmbeddings,embeddings,Gera embeddings usando modelos da Cohere.
"from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.io import BoolInput, DictInput, MessageTextInput, Output


class HuggingFaceEmbeddingsComponent(LCModelComponent):
    display_name = ""Hugging Face Embeddings""
    description = ""Generate embeddings using HuggingFace models.""
    documentation = (
        ""https://python.langchain.com/docs/modules/data_connection/text_embedding/integrations/sentence_transformers""
    )
    icon = ""HuggingFace""
    name = ""HuggingFaceEmbeddings""

    inputs = [
        MessageTextInput(name=""cache_folder"", display_name=""Cache Folder"", advanced=True),
        DictInput(name=""encode_kwargs"", display_name=""Encode Kwargs"", advanced=True),
        DictInput(name=""model_kwargs"", display_name=""Model Kwargs"", advanced=True),
        MessageTextInput(name=""model_name"", display_name=""Model Name"", value=""sentence-transformers/all-mpnet-base-v2""),
        BoolInput(name=""multi_process"", display_name=""Multi Process"", advanced=True),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        return HuggingFaceEmbeddings(
            cache_folder=self.cache_folder,
            encode_kwargs=self.encode_kwargs,
            model_kwargs=self.model_kwargs,
            model_name=self.model_name,
            multi_process=self.multi_process,
        )
",HuggingFaceEmbeddings,embeddings,Gera embeddings usando modelos da HuggingFace.
"from langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings
from pydantic.v1.types import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.io import MessageTextInput, Output, SecretStrInput


class HuggingFaceInferenceAPIEmbeddingsComponent(LCModelComponent):
    display_name = ""Hugging Face API Embeddings""
    description = ""Generate embeddings using Hugging Face Inference API models.""
    documentation = ""https://github.com/huggingface/text-embeddings-inference""
    icon = ""HuggingFace""
    name = ""HuggingFaceInferenceAPIEmbeddings""

    inputs = [
        SecretStrInput(name=""api_key"", display_name=""API Key"", advanced=True),
        MessageTextInput(name=""api_url"", display_name=""API URL"", advanced=True, value=""http://localhost:8080""),
        MessageTextInput(name=""model_name"", display_name=""Model Name"", value=""BAAI/bge-large-en-v1.5""),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        if not self.api_key:
            raise ValueError(""API Key is required"")

        api_key = SecretStr(self.api_key)

        return HuggingFaceInferenceAPIEmbeddings(api_key=api_key, api_url=self.api_url, model_name=self.model_name)
",HuggingFaceInferenceAPIEmbeddings,embeddings,Gera embeddings usando modelos da Hugging Face Inference API.
"from langchain_mistralai.embeddings import MistralAIEmbeddings
from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput


class MistralAIEmbeddingsComponent(LCModelComponent):
    display_name = ""MistralAI Embeddings""
    description = ""Generate embeddings using MistralAI models.""
    icon = ""MistralAI""
    name = ""MistalAIEmbeddings""

    inputs = [
        DropdownInput(
            name=""model"",
            display_name=""Model"",
            advanced=False,
            options=[""mistral-embed""],
            value=""mistral-embed"",
        ),
        SecretStrInput(name=""mistral_api_key"", display_name=""Mistral API Key""),
        IntInput(
            name=""max_concurrent_requests"",
            display_name=""Max Concurrent Requests"",
            advanced=True,
            value=64,
        ),
        IntInput(name=""max_retries"", display_name=""Max Retries"", advanced=True, value=5),
        IntInput(name=""timeout"", display_name=""Request Timeout"", advanced=True, value=120),
        MessageTextInput(
            name=""endpoint"",
            display_name=""API Endpoint"",
            advanced=True,
            value=""https://api.mistral.ai/v1/"",
        ),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        if not self.mistral_api_key:
            raise ValueError(""Mistral API Key is required"")

        api_key = SecretStr(self.mistral_api_key)

        return MistralAIEmbeddings(
            api_key=api_key,
            model=self.model,
            endpoint=self.endpoint,
            max_concurrent_requests=self.max_concurrent_requests,
            max_retries=self.max_retries,
            timeout=self.timeout,
        )
",MistalAIEmbeddings,embeddings,Gera embeddings usando modelos da MistralAI.
"from typing import Any

from langflow.base.embeddings.model import LCEmbeddingsModel
from langflow.field_typing import Embeddings
from langflow.inputs.inputs import DropdownInput, SecretStrInput
from langflow.io import FloatInput, MessageTextInput
from langflow.schema.dotdict import dotdict


class NVIDIAEmbeddingsComponent(LCEmbeddingsModel):
    display_name: str = ""NVIDIA Embeddings""
    description: str = ""Generate embeddings using NVIDIA models.""
    icon = ""NVIDIA""

    inputs = [
        DropdownInput(
            name=""model"",
            display_name=""Model"",
            options=[
                ""nvidia/nv-embed-v1"",
                ""snowflake/arctic-embed-I"",
            ],
            value=""nvidia/nv-embed-v1"",
        ),
        MessageTextInput(
            name=""base_url"",
            display_name=""NVIDIA Base URL"",
            refresh_button=True,
            value=""https://integrate.api.nvidia.com/v1"",
        ),
        SecretStrInput(
            name=""nvidia_api_key"",
            display_name=""NVIDIA API Key"",
            info=""The NVIDIA API Key."",
            advanced=False,
            value=""NVIDIA_API_KEY"",
        ),
        FloatInput(
            name=""temperature"",
            display_name=""Model Temperature"",
            value=0.1,
            advanced=True,
        ),
    ]

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        if field_name == ""base_url"" and field_value:
            try:
                build_model = self.build_embeddings()
                ids = [model.id for model in build_model.available_models]  # type: ignore
                build_config[""model""][""options""] = ids
                build_config[""model""][""value""] = ids[0]
            except Exception as e:
                raise ValueError(f""Error getting model names: {e}"")
        return build_config

    def build_embeddings(self) -> Embeddings:
        try:
            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings
        except ImportError:
            raise ImportError(""Please install langchain-nvidia-ai-endpoints to use the Nvidia model."")
        try:
            output = NVIDIAEmbeddings(
                model=self.model,
                base_url=self.base_url,
                temperature=self.temperature,
                nvidia_api_key=self.nvidia_api_key,
            )  # type: ignore
        except Exception as e:
            raise ValueError(f""Could not connect to NVIDIA API. Error: {e}"") from e
        return output
",NVIDIAEmbeddings,embeddings,Gera embeddings usando modelos da NVIDIA.
"from langchain_community.embeddings import OllamaEmbeddings

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.io import FloatInput, MessageTextInput, Output


class OllamaEmbeddingsComponent(LCModelComponent):
    display_name: str = ""Ollama Embeddings""
    description: str = ""Generate embeddings using Ollama models.""
    documentation = ""https://python.langchain.com/docs/integrations/text_embedding/ollama""
    icon = ""Ollama""
    name = ""OllamaEmbeddings""

    inputs = [
        MessageTextInput(
            name=""model"",
            display_name=""Ollama Model"",
            value=""llama2"",
        ),
        MessageTextInput(
            name=""base_url"",
            display_name=""Ollama Base URL"",
            value=""http://localhost:11434"",
        ),
        FloatInput(
            name=""temperature"",
            display_name=""Model Temperature"",
            value=0.1,
            advanced=True,
        ),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        try:
            output = OllamaEmbeddings(
                model=self.model,
                base_url=self.base_url,
                temperature=self.temperature,
            )  # type: ignore
        except Exception as e:
            raise ValueError(""Could not connect to Ollama API."") from e
        return output
",OllamaEmbeddings,embeddings,Gera embeddings usando modelos da Ollama.
"from langchain_openai.embeddings.base import OpenAIEmbeddings

from langflow.base.embeddings.model import LCEmbeddingsModel
from langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES
from langflow.field_typing import Embeddings
from langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput


class OpenAIEmbeddingsComponent(LCEmbeddingsModel):
    display_name = ""OpenAI Embeddings""
    description = ""Generate embeddings using OpenAI models.""
    icon = ""OpenAI""
    name = ""OpenAIEmbeddings""

    inputs = [
        DictInput(
            name=""default_headers"",
            display_name=""Default Headers"",
            advanced=True,
            info=""Default headers to use for the API request."",
        ),
        DictInput(
            name=""default_query"",
            display_name=""Default Query"",
            advanced=True,
            info=""Default query parameters to use for the API request."",
        ),
        IntInput(name=""chunk_size"", display_name=""Chunk Size"", advanced=True, value=1000),
        MessageTextInput(name=""client"", display_name=""Client"", advanced=True),
        MessageTextInput(name=""deployment"", display_name=""Deployment"", advanced=True),
        IntInput(name=""embedding_ctx_length"", display_name=""Embedding Context Length"", advanced=True, value=1536),
        IntInput(name=""max_retries"", display_name=""Max Retries"", value=3, advanced=True),
        DropdownInput(
            name=""model"",
            display_name=""Model"",
            advanced=False,
            options=OPENAI_EMBEDDING_MODEL_NAMES,
            value=""text-embedding-3-small"",
        ),
        DictInput(name=""model_kwargs"", display_name=""Model Kwargs"", advanced=True),
        SecretStrInput(name=""openai_api_base"", display_name=""OpenAI API Base"", advanced=True),
        SecretStrInput(name=""openai_api_key"", display_name=""OpenAI API Key"", value=""OPENAI_API_KEY""),
        SecretStrInput(name=""openai_api_type"", display_name=""OpenAI API Type"", advanced=True),
        MessageTextInput(name=""openai_api_version"", display_name=""OpenAI API Version"", advanced=True),
        MessageTextInput(
            name=""openai_organization"",
            display_name=""OpenAI Organization"",
            advanced=True,
        ),
        MessageTextInput(name=""openai_proxy"", display_name=""OpenAI Proxy"", advanced=True),
        FloatInput(name=""request_timeout"", display_name=""Request Timeout"", advanced=True),
        BoolInput(name=""show_progress_bar"", display_name=""Show Progress Bar"", advanced=True),
        BoolInput(name=""skip_empty"", display_name=""Skip Empty"", advanced=True),
        MessageTextInput(
            name=""tiktoken_model_name"",
            display_name=""TikToken Model Name"",
            advanced=True,
        ),
        BoolInput(
            name=""tiktoken_enable"",
            display_name=""TikToken Enable"",
            advanced=True,
            value=True,
            info=""If False, you must have transformers installed."",
        ),
        IntInput(
            name=""dimensions"",
            display_name=""Dimensions"",
            info=""The number of dimensions the resulting output embeddings should have. Only supported by certain models."",
            advanced=True,
        ),
    ]

    def build_embeddings(self) -> Embeddings:
        return OpenAIEmbeddings(
            tiktoken_enabled=self.tiktoken_enable,
            default_headers=self.default_headers,
            default_query=self.default_query,
            allowed_special=""all"",
            disallowed_special=""all"",
            chunk_size=self.chunk_size,
            deployment=self.deployment,
            embedding_ctx_length=self.embedding_ctx_length,
            max_retries=self.max_retries,
            model=self.model,
            model_kwargs=self.model_kwargs,
            base_url=self.openai_api_base,
            api_key=self.openai_api_key,
            openai_api_type=self.openai_api_type,
            api_version=self.openai_api_version,
            organization=self.openai_organization,
            openai_proxy=self.openai_proxy,
            timeout=self.request_timeout or None,
            show_progress_bar=self.show_progress_bar,
            skip_empty=self.skip_empty,
            tiktoken_model_name=self.tiktoken_model_name,
            dimensions=self.dimensions or None,
        )
",OpenAIEmbeddings,embeddings,Gera embeddings usando modelos da OpenAI.
"from langflow.base.models.model import LCModelComponent
from langflow.field_typing import Embeddings
from langflow.io import BoolInput, FileInput, FloatInput, IntInput, MessageTextInput, Output


class VertexAIEmbeddingsComponent(LCModelComponent):
    display_name = ""VertexAI Embeddings""
    description = ""Generate embeddings using Google Cloud VertexAI models.""
    icon = ""VertexAI""
    name = ""VertexAIEmbeddings""

    inputs = [
        FileInput(
            name=""credentials"",
            display_name=""Credentials"",
            info=""JSON credentials file. Leave empty to fallback to environment variables"",
            value="""",
            file_types=[""json""],
        ),
        MessageTextInput(name=""location"", display_name=""Location"", value=""us-central1"", advanced=True),
        MessageTextInput(name=""project"", display_name=""Project"", info=""The project ID."", advanced=True),
        IntInput(name=""max_output_tokens"", display_name=""Max Output Tokens"", advanced=True),
        IntInput(name=""max_retries"", display_name=""Max Retries"", value=1, advanced=True),
        MessageTextInput(name=""model_name"", display_name=""Model Name"", value=""textembedding-gecko""),
        IntInput(name=""n"", display_name=""N"", value=1, advanced=True),
        IntInput(name=""request_parallelism"", value=5, display_name=""Request Parallelism"", advanced=True),
        MessageTextInput(name=""stop_sequences"", display_name=""Stop"", advanced=True, is_list=True),
        BoolInput(name=""streaming"", display_name=""Streaming"", value=False, advanced=True),
        FloatInput(name=""temperature"", value=0.0, display_name=""Temperature""),
        IntInput(name=""top_k"", display_name=""Top K"", advanced=True),
        FloatInput(name=""top_p"", display_name=""Top P"", value=0.95, advanced=True),
    ]

    outputs = [
        Output(display_name=""Embeddings"", name=""embeddings"", method=""build_embeddings""),
    ]

    def build_embeddings(self) -> Embeddings:
        try:
            from langchain_google_vertexai import VertexAIEmbeddings
        except ImportError:
            raise ImportError(
                ""Please install the langchain-google-vertexai package to use the VertexAIEmbeddings component.""
            )

        from google.oauth2 import service_account

        if self.credentials:
            gcloud_credentials = service_account.Credentials.from_service_account_file(self.credentials)
        else:
            # will fallback to environment variable or inferred from gcloud CLI
            gcloud_credentials = None
        return VertexAIEmbeddings(
            credentials=gcloud_credentials,
            location=self.location,
            max_output_tokens=self.max_output_tokens or None,
            max_retries=self.max_retries,
            model_name=self.model_name,
            n=self.n,
            project=self.project,
            request_parallelism=self.request_parallelism,
            stop=self.stop_sequences or None,
            streaming=self.streaming,
            temperature=self.temperature,
            top_k=self.top_k or None,
            top_p=self.top_p,
        )
",VertexAIEmbeddings,embeddings,Gera embeddings usando modelos da Google Cloud VertexAI.
"from langflow.custom import Component
from langflow.io import MessageTextInput, Output
from langflow.schema.message import Message


class CombineTextComponent(Component):
    display_name = ""Combine Text""
    description = ""Concatenate two text sources into a single text chunk using a specified delimiter.""
    icon = ""merge""
    name = ""CombineText""

    inputs = [
        MessageTextInput(
            name=""text1"",
            display_name=""First Text"",
            info=""The first text input to concatenate."",
        ),
        MessageTextInput(
            name=""text2"",
            display_name=""Second Text"",
            info=""The second text input to concatenate."",
        ),
        MessageTextInput(
            name=""delimiter"",
            display_name=""Delimiter"",
            info=""A string used to separate the two text inputs. Defaults to a whitespace."",
            value="" "",
        ),
    ]

    outputs = [
        Output(display_name=""Combined Text"", name=""combined_text"", method=""combine_texts""),
    ]

    def combine_texts(self) -> Message:
        combined = self.delimiter.join([self.text1, self.text2])
        self.status = combined
        return Message(text=combined)
",CombineText,helpers,Concatena duas fontes de texto em um único bloco de texto usando um delimitador especificado.
"from langflow.custom import Component
from langflow.inputs import StrInput
from langflow.schema import Data
from langflow.template import Output


class CreateListComponent(Component):
    display_name = ""Create List""
    description = ""Creates a list of texts.""
    icon = ""list""
    name = ""CreateList""

    inputs = [
        StrInput(
            name=""texts"",
            display_name=""Texts"",
            info=""Enter one or more texts."",
            is_list=True,
        ),
    ]

    outputs = [
        Output(display_name=""Data List"", name=""list"", method=""create_list""),
    ]

    def create_list(self) -> list[Data]:
        data = [Data(text=text) for text in self.texts]
        self.status = data
        return data
",CreateList,helpers,Cria uma lista de textos.
"# from langflow.field_typing import Data
from langflow.custom import Component
from langflow.io import MessageTextInput, Output
from langflow.schema import Data


class CustomComponent(Component):
    display_name = ""Custom Component""
    description = ""Use as a template to create your own component.""
    documentation: str = ""http://docs.langflow.org/components/custom""
    icon = ""custom_components""
    name = ""CustomComponent""

    inputs = [
        MessageTextInput(name=""input_value"", display_name=""Input Value"", value=""Hello, World!""),
    ]

    outputs = [
        Output(display_name=""Output"", name=""output"", method=""build_output""),
    ]

    def build_output(self) -> Data:
        data = Data(value=self.input_value)
        self.status = data
        return data
",CustomComponent,helpers,Use como um modelo para criar seu próprio componente.
"from typing import List

from langflow.custom import Component
from langflow.io import DataInput, MessageTextInput, Output
from langflow.schema import Data


class FilterDataComponent(Component):
    display_name = ""Filter Data""
    description = ""Filters a Data object based on a list of keys.""
    icon = ""filter""
    beta = True
    name = ""FilterData""

    inputs = [
        DataInput(
            name=""data"",
            display_name=""Data"",
            info=""Data object to filter."",
        ),
        MessageTextInput(
            name=""filter_criteria"",
            display_name=""Filter Criteria"",
            info=""List of keys to filter by."",
            is_list=True,
        ),
    ]

    outputs = [
        Output(display_name=""Filtered Data"", name=""filtered_data"", method=""filter_data""),
    ]

    def filter_data(self) -> Data:
        filter_criteria: List[str] = self.filter_criteria
        data = self.data.data if isinstance(self.data, Data) else {}

        # Filter the data
        filtered = {key: value for key, value in data.items() if key in filter_criteria}

        # Create a new Data object with the filtered data
        filtered_data = Data(data=filtered)
        self.status = filtered_data
        return filtered_data
",FilterData,helpers,Filtra um objeto Data com base em uma lista de chaves.
"from langflow.base.agents.crewai.tasks import HierarchicalTask
from langflow.custom import Component
from langflow.io import HandleInput, MultilineInput, Output


class HierarchicalTaskComponent(Component):
    display_name: str = ""Hierarchical Task""
    description: str = ""Each task must have a description, an expected output and an agent responsible for execution.""
    icon = ""CrewAI""
    inputs = [
        MultilineInput(
            name=""task_description"",
            display_name=""Description"",
            info=""Descriptive text detailing task's purpose and execution."",
        ),
        MultilineInput(
            name=""expected_output"",
            display_name=""Expected Output"",
            info=""Clear definition of expected task outcome."",
        ),
        HandleInput(
            name=""tools"",
            display_name=""Tools"",
            input_types=[""Tool""],
            is_list=True,
            info=""List of tools/resources limited for task execution. Uses the Agent tools by default."",
            required=False,
            advanced=True,
        ),
    ]

    outputs = [
        Output(display_name=""Task"", name=""task_output"", method=""build_task""),
    ]

    def build_task(self) -> HierarchicalTask:
        task = HierarchicalTask(
            description=self.task_description,
            expected_output=self.expected_output,
            tools=self.tools or [],
        )
        self.status = task
        return task
",HierarchicalTask,helpers,"Cada tarefa deve ter uma descrição, uma saída esperada e um agente responsável pela execução."
"import uuid
from typing import Any, Optional

from langflow.custom import CustomComponent
from langflow.schema.dotdict import dotdict


class IDGeneratorComponent(CustomComponent):
    display_name = ""ID Generator""
    description = ""Generates a unique ID.""
    name = ""IDGenerator""

    def update_build_config(  # type: ignore
        self, build_config: dotdict, field_value: Any, field_name: Optional[str] = None
    ):
        if field_name == ""unique_id"":
            build_config[field_name][""value""] = str(uuid.uuid4())
        return build_config

    def build_config(self):
        return {
            ""unique_id"": {
                ""display_name"": ""Value"",
                ""refresh_button"": True,
            }
        }

    def build(self, unique_id: str) -> str:
        return unique_id
",IDGenerator,helpers,Gera um ID exclusivo.
"from langflow.custom import Component
from langflow.helpers.data import data_to_text
from langflow.inputs import HandleInput
from langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output
from langflow.memory import get_messages, LCBuiltinChatMemory
from langflow.schema import Data
from langflow.schema.message import Message
from langflow.field_typing import BaseChatMemory
from langchain.memory import ConversationBufferMemory

from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER


class MemoryComponent(Component):
    display_name = ""Chat Memory""
    description = ""Retrieves stored chat messages from Langflow tables or an external memory.""
    icon = ""message-square-more""
    name = ""Memory""

    inputs = [
        HandleInput(
            name=""memory"",
            display_name=""External Memory"",
            input_types=[""BaseChatMessageHistory""],
            info=""Retrieve messages from an external memory. If empty, it will use the Langflow tables."",
        ),
        DropdownInput(
            name=""sender"",
            display_name=""Sender Type"",
            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, ""Machine and User""],
            value=""Machine and User"",
            info=""Filter by sender type."",
            advanced=True,
        ),
        MessageTextInput(
            name=""sender_name"",
            display_name=""Sender Name"",
            info=""Filter by sender name."",
            advanced=True,
        ),
        IntInput(
            name=""n_messages"",
            display_name=""Number of Messages"",
            value=100,
            info=""Number of messages to retrieve."",
            advanced=True,
        ),
        MessageTextInput(
            name=""session_id"",
            display_name=""Session ID"",
            info=""The session ID of the chat. If empty, the current session ID parameter will be used."",
            advanced=True,
        ),
        DropdownInput(
            name=""order"",
            display_name=""Order"",
            options=[""Ascending"", ""Descending""],
            value=""Ascending"",
            info=""Order of the messages."",
            advanced=True,
        ),
        MultilineInput(
            name=""template"",
            display_name=""Template"",
            info=""The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data."",
            value=""{sender_name}: {text}"",
            advanced=True,
        ),
    ]

    outputs = [
        Output(display_name=""Messages (Data)"", name=""messages"", method=""retrieve_messages""),
        Output(display_name=""Messages (Text)"", name=""messages_text"", method=""retrieve_messages_as_text""),
        Output(display_name=""Memory"", name=""lc_memory"", method=""build_lc_memory""),
    ]

    def retrieve_messages(self) -> Data:
        sender = self.sender
        sender_name = self.sender_name
        session_id = self.session_id
        n_messages = self.n_messages
        order = ""DESC"" if self.order == ""Descending"" else ""ASC""

        if sender == ""Machine and User"":
            sender = None

        if self.memory:
            # override session_id
            self.memory.session_id = session_id

            stored = self.memory.messages
            if order == ""ASC"":
                stored = stored[::-1]
            if n_messages:
                stored = stored[:n_messages]
            stored = [Message.from_lc_message(m) for m in stored]
            if sender:
                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER
                stored = [m for m in stored if m.type == expected_type]
        else:
            stored = get_messages(
                sender=sender,
                sender_name=sender_name,
                session_id=session_id,
                limit=n_messages,
                order=order,
            )
        self.status = stored
        return stored

    def retrieve_messages_as_text(self) -> Message:
        stored_text = data_to_text(self.template, self.retrieve_messages())
        self.status = stored_text
        return Message(text=stored_text)

    def build_lc_memory(self) -> BaseChatMemory:
        if self.memory:
            chat_memory = self.memory
        else:
            chat_memory = LCBuiltinChatMemory(flow_id=self.graph.flow_id, session_id=self.session_id)
        return ConversationBufferMemory(chat_memory=chat_memory)
",Memory,helpers,Recupera mensagens de bate-papo armazenadas em tabelas Langflow ou uma memória externa.
"from langflow.custom import CustomComponent
from langflow.schema import Data


class MergeDataComponent(CustomComponent):
    display_name = ""Merge Data""
    description = ""Combines multiple data sources into a single unified Data object.""
    beta: bool = True
    name = ""MergeData""

    field_config = {
        ""data"": {""display_name"": ""Data""},
    }

    def build(self, data: list[Data]) -> Data:
        if not data:
            return Data()
        if len(data) == 1:
            return data[0]
        merged_data = Data()
        for value in data:
            if merged_data is None:
                merged_data = value
            else:
                merged_data += value
        self.status = merged_data
        return merged_data
",MergeData,helpers,Combina várias fontes de dados em um único objeto Data unificado.
"from langflow.custom import Component
from langflow.helpers.data import data_to_text
from langflow.io import DataInput, MultilineInput, Output, StrInput
from langflow.schema.message import Message


class ParseDataComponent(Component):
    display_name = ""Parse Data""
    description = ""Convert Data into plain text following a specified template.""
    icon = ""braces""
    name = ""ParseData""

    inputs = [
        DataInput(name=""data"", display_name=""Data"", info=""The data to convert to text.""),
        MultilineInput(
            name=""template"",
            display_name=""Template"",
            info=""The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data."",
            value=""{text}"",
        ),
        StrInput(name=""sep"", display_name=""Separator"", advanced=True, value=""\n""),
    ]

    outputs = [
        Output(display_name=""Text"", name=""text"", method=""parse_data""),
    ]

    def parse_data(self) -> Message:
        data = self.data if isinstance(self.data, list) else [self.data]
        template = self.template

        result_string = data_to_text(template, data, sep=self.sep)
        self.status = result_string
        return Message(text=result_string)
",ParseData,helpers,Converte Data em texto simples seguindo um modelo especificado.
"from langflow.base.agents.crewai.tasks import SequentialTask
from langflow.custom import Component
from langflow.io import BoolInput, HandleInput, MultilineInput, Output


class SequentialTaskComponent(Component):
    display_name: str = ""Sequential Task""
    description: str = ""Each task must have a description, an expected output and an agent responsible for execution.""
    icon = ""CrewAI""
    inputs = [
        MultilineInput(
            name=""task_description"",
            display_name=""Description"",
            info=""Descriptive text detailing task's purpose and execution."",
        ),
        MultilineInput(
            name=""expected_output"",
            display_name=""Expected Output"",
            info=""Clear definition of expected task outcome."",
        ),
        HandleInput(
            name=""tools"",
            display_name=""Tools"",
            input_types=[""Tool""],
            is_list=True,
            info=""List of tools/resources limited for task execution. Uses the Agent tools by default."",
            required=False,
            advanced=True,
        ),
        HandleInput(
            name=""agent"",
            display_name=""Agent"",
            input_types=[""Agent""],
            info=""CrewAI Agent that will perform the task"",
            required=True,
        ),
        HandleInput(
            name=""task"",
            display_name=""Task"",
            input_types=[""SequentialTask""],
            info=""CrewAI Task that will perform the task"",
        ),
        BoolInput(
            name=""async_execution"",
            display_name=""Async Execution"",
            value=True,
            advanced=True,
            info=""Boolean flag indicating asynchronous task execution."",
        ),
    ]

    outputs = [
        Output(display_name=""Task"", name=""task_output"", method=""build_task""),
    ]

    def build_task(self) -> list[SequentialTask]:
        tasks: list[SequentialTask] = []
        task = SequentialTask(
            description=self.task_description,
            expected_output=self.expected_output,
            tools=self.agent.tools,
            async_execution=False,
            agent=self.agent,
        )
        tasks.append(task)
        self.status = task
        if self.task:
            if isinstance(self.task, list) and all(isinstance(task, SequentialTask) for task in self.task):
                tasks = self.task + tasks
            elif isinstance(self.task, SequentialTask):
                tasks = [self.task] + tasks
        return tasks
",SequentialTask,helpers,"Cada tarefa deve ter uma descrição, uma saída esperada e um agente responsável pela execução."
"from typing import List

from langchain_text_splitters import CharacterTextSplitter

from langflow.custom import Component
from langflow.io import HandleInput, IntInput, MessageTextInput, Output
from langflow.schema import Data
from langflow.utils.util import unescape_string


class SplitTextComponent(Component):
    display_name: str = ""Split Text""
    description: str = ""Split text into chunks based on specified criteria.""
    icon = ""scissors-line-dashed""
    name = ""SplitText""

    inputs = [
        HandleInput(
            name=""data_inputs"",
            display_name=""Data Inputs"",
            info=""The data to split."",
            input_types=[""Data""],
            is_list=True,
        ),
        IntInput(
            name=""chunk_overlap"",
            display_name=""Chunk Overlap"",
            info=""Number of characters to overlap between chunks."",
            value=200,
        ),
        IntInput(
            name=""chunk_size"",
            display_name=""Chunk Size"",
            info=""The maximum number of characters in each chunk."",
            value=1000,
        ),
        MessageTextInput(
            name=""separator"",
            display_name=""Separator"",
            info=""The character to split on. Defaults to newline."",
            value=""\n"",
        ),
    ]

    outputs = [
        Output(display_name=""Chunks"", name=""chunks"", method=""split_text""),
    ]

    def _docs_to_data(self, docs):
        data = []
        for doc in docs:
            data.append(Data(text=doc.page_content, data=doc.metadata))
        return data

    def split_text(self) -> List[Data]:
        separator = unescape_string(self.separator)

        documents = []
        for _input in self.data_inputs:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())

        splitter = CharacterTextSplitter(
            chunk_overlap=self.chunk_overlap,
            chunk_size=self.chunk_size,
            separator=separator,
        )
        docs = splitter.split_documents(documents)
        data = self._docs_to_data(docs)
        self.status = data
        return data
",SplitText,helpers,Divide o texto em chunks com base em critérios específicos.
"from langflow.custom import Component
from langflow.inputs import MessageInput, StrInput, HandleInput
from langflow.schema.message import Message
from langflow.template import Output
from langflow.memory import get_messages, store_message
from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI


class StoreMessageComponent(Component):
    display_name = ""Store Message""
    description = ""Stores a chat message or text into Langflow tables or an external memory.""
    icon = ""save""
    name = ""StoreMessage""

    inputs = [
        MessageInput(name=""message"", display_name=""Message"", info=""The chat message to be stored."", required=True),
        HandleInput(
            name=""memory"",
            display_name=""External Memory"",
            input_types=[""BaseChatMessageHistory""],
            info=""The external memory to store the message. If empty, it will use the Langflow tables."",
        ),
        StrInput(
            name=""sender"",
            display_name=""Sender"",
            info=""The sender of the message. Might be Machine or User. If empty, the current sender parameter will be used."",
            advanced=True,
        ),
        StrInput(
            name=""sender_name"",
            display_name=""Sender Name"",
            info=""The name of the sender. Might be AI or User. If empty, the current sender parameter will be used."",
            advanced=True,
        ),
        StrInput(
            name=""session_id"",
            display_name=""Session ID"",
            info=""The session ID of the chat. If empty, the current session ID parameter will be used."",
            value="""",
        ),
    ]

    outputs = [
        Output(display_name=""Stored Messages"", name=""stored_messages"", method=""store_message""),
    ]

    def store_message(self) -> Message:
        message = self.message

        message.session_id = self.session_id or message.session_id
        message.sender = self.sender or message.sender or MESSAGE_SENDER_AI
        message.sender_name = self.sender_name or message.sender_name or MESSAGE_SENDER_NAME_AI

        if self.memory:
            # override session_id
            self.memory.session_id = message.session_id
            lc_message = message.to_lc_message()
            self.memory.add_messages([lc_message])
            stored = self.memory.messages
            stored = [Message.from_lc_message(m) for m in stored]
            if message.sender:
                stored = [m for m in stored if m.sender == message.sender]
        else:
            store_message(message, flow_id=self.graph.flow_id)
            stored = get_messages(session_id=message.session_id, sender_name=message.sender_name, sender=message.sender)
        self.status = stored
        return stored
",StoreMessage,helpers,Armazena uma mensagem de bate-papo ou texto em tabelas Langflow ou uma memória externa.
"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES
from langflow.base.io.chat import ChatComponent
from langflow.inputs import BoolInput
from langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output
from langflow.memory import store_message
from langflow.schema.message import Message
from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER


class ChatInput(ChatComponent):
    display_name = ""Chat Input""
    description = ""Get chat inputs from the Playground.""
    icon = ""ChatInput""
    name = ""ChatInput""

    inputs = [
        MultilineInput(
            name=""input_value"",
            display_name=""Text"",
            value="""",
            info=""Message to be passed as input."",
        ),
        BoolInput(
            name=""should_store_message"",
            display_name=""Store Messages"",
            info=""Store the message in the history."",
            value=True,
            advanced=True,
        ),
        DropdownInput(
            name=""sender"",
            display_name=""Sender Type"",
            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],
            value=MESSAGE_SENDER_USER,
            info=""Type of sender."",
            advanced=True,
        ),
        MessageTextInput(
            name=""sender_name"",
            display_name=""Sender Name"",
            info=""Name of the sender."",
            value=MESSAGE_SENDER_NAME_USER,
            advanced=True,
        ),
        MessageTextInput(
            name=""session_id"",
            display_name=""Session ID"",
            info=""The session ID of the chat. If empty, the current session ID parameter will be used."",
            advanced=True,
        ),
        FileInput(
            name=""files"",
            display_name=""Files"",
            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,
            info=""Files to be sent with the message."",
            advanced=True,
            is_list=True,
        ),
    ]
    outputs = [
        Output(display_name=""Message"", name=""message"", method=""message_response""),
    ]

    def message_response(self) -> Message:
        message = Message(
            text=self.input_value,
            sender=self.sender,
            sender_name=self.sender_name,
            session_id=self.session_id,
            files=self.files,
        )

        if (
            self.session_id
            and isinstance(message, Message)
            and isinstance(message.text, str)
            and self.should_store_message
        ):
            store_message(
                message,
                flow_id=self.graph.flow_id,
            )
            self.message.value = message

        self.status = message
        return message
",ChatInput,inputs,Obter entradas de bate-papo do Playground.
"from langflow.base.io.text import TextComponent
from langflow.io import MessageTextInput, Output
from langflow.schema.message import Message


class TextInputComponent(TextComponent):
    display_name = ""Text Input""
    description = ""Get text inputs from the Playground.""
    icon = ""type""
    name = ""TextInput""

    inputs = [
        MessageTextInput(
            name=""input_value"",
            display_name=""Text"",
            info=""Text to be passed as input."",
        ),
    ]
    outputs = [
        Output(display_name=""Text"", name=""text"", method=""text_response""),
    ]

    def text_response(self) -> Message:
        message = Message(
            text=self.input_value,
        )
        return message
",TextInput,inputs,Obter entradas de texto do Playground.
"import uuid
from typing import Optional

from langflow.custom import CustomComponent
from langflow.schema import Data


class FirecrawlCrawlApi(CustomComponent):
    display_name: str = ""FirecrawlCrawlApi""
    description: str = ""Firecrawl Crawl API.""
    name = ""FirecrawlCrawlApi""

    output_types: list[str] = [""Document""]
    documentation: str = ""https://docs.firecrawl.dev/api-reference/endpoint/crawl""
    field_config = {
        ""api_key"": {
            ""display_name"": ""API Key"",
            ""field_type"": ""str"",
            ""required"": True,
            ""password"": True,
            ""info"": ""The API key to use Firecrawl API."",
        },
        ""url"": {
            ""display_name"": ""URL"",
            ""field_type"": ""str"",
            ""required"": True,
            ""info"": ""The base URL to start crawling from."",
        },
        ""timeout"": {
            ""display_name"": ""Timeout"",
            ""field_type"": ""int"",
            ""info"": ""The timeout in milliseconds."",
        },
        ""crawlerOptions"": {
            ""display_name"": ""Crawler Options"",
            ""info"": ""Options for the crawler behavior."",
        },
        ""pageOptions"": {
            ""display_name"": ""Page Options"",
            ""info"": ""The page options to send with the request."",
        },
        ""idempotency_key"": {
            ""display_name"": ""Idempotency Key"",
            ""field_type"": ""str"",
            ""info"": ""Optional idempotency key to ensure unique requests."",
        },
    }

    def build(
        self,
        api_key: str,
        url: str,
        timeout: int = 30000,
        crawlerOptions: Optional[Data] = None,
        pageOptions: Optional[Data] = None,
        idempotency_key: Optional[str] = None,
    ) -> Data:
        try:
            from firecrawl.firecrawl import FirecrawlApp  # type: ignore
        except ImportError:
            raise ImportError(
                ""Could not import firecrawl integration package. "" ""Please install it with `pip install firecrawl-py`.""
            )
        if crawlerOptions:
            crawler_options_dict = crawlerOptions.__dict__[""data""][""text""]
        else:
            crawler_options_dict = {}

        if pageOptions:
            page_options_dict = pageOptions.__dict__[""data""][""text""]
        else:
            page_options_dict = {}

        if not idempotency_key:
            idempotency_key = str(uuid.uuid4())

        app = FirecrawlApp(api_key=api_key)
        crawl_result = app.crawl_url(
            url,
            {
                ""crawlerOptions"": crawler_options_dict,
                ""pageOptions"": page_options_dict,
            },
            True,
            int(timeout / 1000),
            idempotency_key,
        )

        records = Data(data={""results"": crawl_result})
        return records
",FirecrawlCrawlApi,langchain_utilities,API Firecrawl Crawl.
"from typing import Optional

from langflow.custom import CustomComponent
from langflow.schema import Data


class FirecrawlScrapeApi(CustomComponent):
    display_name: str = ""FirecrawlScrapeApi""
    description: str = ""Firecrawl Scrape API.""
    name = ""FirecrawlScrapeApi""

    output_types: list[str] = [""Document""]
    documentation: str = ""https://docs.firecrawl.dev/api-reference/endpoint/scrape""
    field_config = {
        ""api_key"": {
            ""display_name"": ""API Key"",
            ""field_type"": ""str"",
            ""required"": True,
            ""password"": True,
            ""info"": ""The API key to use Firecrawl API."",
        },
        ""url"": {
            ""display_name"": ""URL"",
            ""field_type"": ""str"",
            ""required"": True,
            ""info"": ""The URL to scrape."",
        },
        ""timeout"": {
            ""display_name"": ""Timeout"",
            ""info"": ""Timeout in milliseconds for the request."",
            ""field_type"": ""int"",
            ""default_value"": 10000,
        },
        ""pageOptions"": {
            ""display_name"": ""Page Options"",
            ""info"": ""The page options to send with the request."",
        },
        ""extractorOptions"": {
            ""display_name"": ""Extractor Options"",
            ""info"": ""The extractor options to send with the request."",
        },
    }

    def build(
        self,
        api_key: str,
        url: str,
        timeout: int = 10000,
        pageOptions: Optional[Data] = None,
        extractorOptions: Optional[Data] = None,
    ) -> Data:
        try:
            from firecrawl.firecrawl import FirecrawlApp  # type: ignore
        except ImportError:
            raise ImportError(
                ""Could not import firecrawl integration package. "" ""Please install it with `pip install firecrawl-py`.""
            )
        if extractorOptions:
            extractor_options_dict = extractorOptions.__dict__[""data""][""text""]
        else:
            extractor_options_dict = {}

        if pageOptions:
            page_options_dict = pageOptions.__dict__[""data""][""text""]
        else:
            page_options_dict = {}

        app = FirecrawlApp(api_key=api_key)
        results = app.scrape_url(
            url,
            {
                ""timeout"": str(timeout),
                ""extractorOptions"": extractor_options_dict,
                ""pageOptions"": page_options_dict,
            },
        )

        record = Data(data=results)
        return record
",FirecrawlScrapeApi,langchain_utilities,API Firecrawl Scrape.
"### JSON Document Builder

# Build a Document containing a JSON object using a key and another Document page content.

# **Params**

# - **Key:** The key to use for the JSON object.
# - **Document:** The Document page to use for the JSON object.

# **Output**

# - **Document:** The Document containing the JSON object.

from langchain_core.documents import Document

from langflow.custom import CustomComponent
from langflow.services.database.models.base import orjson_dumps


class JSONDocumentBuilder(CustomComponent):
    display_name: str = ""JSON Document Builder""
    description: str = ""Build a Document containing a JSON object using a key and another Document page content.""
    name = ""JSONDocumentBuilder""

    output_types: list[str] = [""Document""]
    documentation: str = ""https://docs.langflow.org/components/utilities#json-document-builder""

    field_config = {
        ""key"": {""display_name"": ""Key""},
        ""document"": {""display_name"": ""Document""},
    }

    def build(
        self,
        key: str,
        document: Document,
    ) -> Document:
        documents = None
        if isinstance(document, list):
            documents = [
                Document(page_content=orjson_dumps({key: doc.page_content}, indent_2=False)) for doc in document
            ]
        elif isinstance(document, Document):
            documents = Document(page_content=orjson_dumps({key: document.page_content}, indent_2=False))
        else:
            raise TypeError(f""Expected Document or list of Documents, got {type(document)}"")
        self.repr_value = documents
        return documents
",JSONDocumentBuilder,langchain_utilities,Constrói um Documento contendo um objeto JSON usando uma chave e o conteúdo da página de outro Documento.
"from langchain_experimental.sql.base import SQLDatabase

from langflow.custom import CustomComponent


class SQLDatabaseComponent(CustomComponent):
    display_name = ""SQLDatabase""
    description = ""SQL Database""
    name = ""SQLDatabase""

    def build_config(self):
        return {
            ""uri"": {""display_name"": ""URI"", ""info"": ""URI to the database.""},
        }

    def clean_up_uri(self, uri: str) -> str:
        if uri.startswith(""postgresql://""):
            uri = uri.replace(""postgresql://"", ""postgres://"")
        return uri.strip()

    def build(self, uri: str) -> SQLDatabase:
        uri = self.clean_up_uri(uri)
        return SQLDatabase.from_uri(uri)
",SQLDatabase,langchain_utilities,Banco de Dados SQL
"from langflow.base.memory.model import LCChatMemoryComponent
from langflow.inputs import MessageTextInput, StrInput, SecretStrInput
from langflow.field_typing import BaseChatMessageHistory


class AstraDBChatMemory(LCChatMemoryComponent):
    display_name = ""Astra DB Chat Memory""
    description = ""Retrieves and store chat messages from Astra DB.""
    name = ""AstraDBChatMemory""
    icon: str = ""AstraDB""

    inputs = [
        StrInput(
            name=""collection_name"",
            display_name=""Collection Name"",
            info=""The name of the collection within Astra DB where the vectors will be stored."",
            required=True,
        ),
        SecretStrInput(
            name=""token"",
            display_name=""Astra DB Application Token"",
            info=""Authentication token for accessing Astra DB."",
            value=""ASTRA_DB_APPLICATION_TOKEN"",
            required=True,
        ),
        SecretStrInput(
            name=""api_endpoint"",
            display_name=""API Endpoint"",
            info=""API endpoint URL for the Astra DB service."",
            value=""ASTRA_DB_API_ENDPOINT"",
            required=True,
        ),
        StrInput(
            name=""namespace"",
            display_name=""Namespace"",
            info=""Optional namespace within Astra DB to use for the collection."",
            advanced=True,
        ),
        MessageTextInput(
            name=""session_id"",
            display_name=""Session ID"",
            info=""The session ID of the chat. If empty, the current session ID parameter will be used."",
            advanced=True,
        ),
    ]

    def build_message_history(self) -> BaseChatMessageHistory:
        try:
            from langchain_astradb.chat_message_histories import AstraDBChatMessageHistory
        except ImportError:
            raise ImportError(
                ""Could not import langchain Astra DB integration package. ""
                ""Please install it with `pip install langchain-astradb`.""
            )

        memory = AstraDBChatMessageHistory(
            session_id=self.session_id,
            collection_name=self.collection_name,
            token=self.token,
            api_endpoint=self.api_endpoint,
            namespace=self.namespace or None,
        )
        return memory
",AstraDBChatMemory,memories,Recupera e armazena mensagens de bate-papo do Astra DB.
"from langflow.base.memory.model import LCChatMemoryComponent
from langflow.inputs import MessageTextInput, SecretStrInput, DictInput
from langflow.field_typing import BaseChatMessageHistory


class CassandraChatMemory(LCChatMemoryComponent):
    display_name = ""Cassandra Chat Memory""
    description = ""Retrieves and store chat messages from Apache Cassandra.""
    name = ""CassandraChatMemory""
    icon = ""Cassandra""

    inputs = [
        MessageTextInput(
            name=""database_ref"",
            display_name=""Contact Points / Astra Database ID"",
            info=""Contact points for the database (or AstraDB database ID)"",
            required=True,
        ),
        MessageTextInput(
            name=""username"", display_name=""Username"", info=""Username for the database (leave empty for AstraDB).""
        ),
        SecretStrInput(
            name=""token"",
            display_name=""Password / AstraDB Token"",
            info=""User password for the database (or AstraDB token)."",
            required=True,
        ),
        MessageTextInput(
            name=""keyspace"",
            display_name=""Keyspace"",
            info=""Table Keyspace (or AstraDB namespace)."",
            required=True,
        ),
        MessageTextInput(
            name=""table_name"",
            display_name=""Table Name"",
            info=""The name of the table (or AstraDB collection) where vectors will be stored."",
            required=True,
        ),
        MessageTextInput(
            name=""session_id"", display_name=""Session ID"", info=""Session ID for the message."", advanced=True
        ),
        DictInput(
            name=""cluster_kwargs"",
            display_name=""Cluster arguments"",
            info=""Optional dictionary of additional keyword arguments for the Cassandra cluster."",
            advanced=True,
            is_list=True,
        ),
    ]

    def build_message_history(self) -> BaseChatMessageHistory:
        from langchain_community.chat_message_histories import CassandraChatMessageHistory

        try:
            import cassio
        except ImportError:
            raise ImportError(
                ""Could not import cassio integration package. "" ""Please install it with `pip install cassio`.""
            )

        from uuid import UUID

        database_ref = self.database_ref

        try:
            UUID(self.database_ref)
            is_astra = True
        except ValueError:
            is_astra = False
            if "","" in self.database_ref:
                # use a copy because we can't change the type of the parameter
                database_ref = self.database_ref.split("","")

        if is_astra:
            cassio.init(
                database_id=database_ref,
                token=self.token,
                cluster_kwargs=self.cluster_kwargs,
            )
        else:
            cassio.init(
                contact_points=database_ref,
                username=self.username,
                password=self.token,
                cluster_kwargs=self.cluster_kwargs,
            )

        return CassandraChatMessageHistory(
            session_id=self.session_id,
            table_name=self.table_name,
            keyspace=self.keyspace,
        )
",CassandraChatMemory,memories,Recupera e armazena mensagens de bate-papo do Apache Cassandra.
"from langflow.base.memory.model import LCChatMemoryComponent
from langflow.inputs import MessageTextInput, SecretStrInput, DropdownInput
from langflow.field_typing import BaseChatMessageHistory


class ZepChatMemory(LCChatMemoryComponent):
    display_name = ""Zep Chat Memory""
    description = ""Retrieves and store chat messages from Zep.""
    name = ""ZepChatMemory""

    inputs = [
        MessageTextInput(name=""url"", display_name=""Zep URL"", info=""URL of the Zep instance.""),
        SecretStrInput(name=""api_key"", display_name=""API Key"", info=""API Key for the Zep instance.""),
        DropdownInput(
            name=""api_base_path"",
            display_name=""API Base Path"",
            options=[""api/v1"", ""api/v2""],
            value=""api/v1"",
            advanced=True,
        ),
        MessageTextInput(
            name=""session_id"", display_name=""Session ID"", info=""Session ID for the message."", advanced=True
        ),
    ]

    def build_message_history(self) -> BaseChatMessageHistory:
        try:
            # Monkeypatch API_BASE_PATH to
            # avoid 404
            # This is a workaround for the local Zep instance
            # cloud Zep works with v2
            import zep_python.zep_client
            from zep_python import ZepClient
            from zep_python.langchain import ZepChatMessageHistory

            zep_python.zep_client.API_BASE_PATH = self.api_base_path
        except ImportError:
            raise ImportError(
                ""Could not import zep-python package. "" ""Please install it with `pip install zep-python`.""
            )

        zep_client = ZepClient(api_url=self.url, api_key=self.api_key)
        return ZepChatMessageHistory(session_id=self.session_id, zep_client=zep_client)
",ZepChatMemory,memories,Recupera e armazena mensagens de bate-papo do Zep.
"import json
import httpx
from langflow.base.models.aiml_constants import AIML_CHAT_MODELS
from langflow.custom.custom_component.component import Component

from langflow.inputs.inputs import FloatInput, IntInput, MessageInput, SecretStrInput
from langflow.schema.message import Message
from langflow.template.field.base import Output
from loguru import logger
from pydantic.v1 import SecretStr

from langflow.inputs import (
    DropdownInput,
    StrInput,
)


class AIMLModelComponent(Component):
    display_name = ""AI/ML API""
    description = ""Generates text using the AI/ML API""
    icon = ""AI/ML""
    chat_completion_url = ""https://api.aimlapi.com/v1/chat/completions""

    outputs = [
        Output(display_name=""Text"", name=""text_output"", method=""make_request""),
    ]

    inputs = [
        DropdownInput(
            name=""model_name"",
            display_name=""Model Name"",
            options=AIML_CHAT_MODELS,
            required=True,
        ),
        SecretStrInput(
            name=""aiml_api_key"",
            display_name=""AI/ML API Key"",
            value=""AIML_API_KEY"",
        ),
        MessageInput(name=""input_value"", display_name=""Input"", required=True),
        IntInput(
            name=""max_tokens"",
            display_name=""Max Tokens"",
            advanced=True,
            info=""The maximum number of tokens to generate. Set to 0 for unlimited tokens."",
        ),
        StrInput(
            name=""stop_tokens"",
            display_name=""Stop Tokens"",
            info=""Comma-separated list of tokens to signal the model to stop generating text."",
            advanced=True,
        ),
        IntInput(
            name=""top_k"",
            display_name=""Top K"",
            info=""Limits token selection to top K. (Default: 40)"",
            advanced=True,
        ),
        FloatInput(
            name=""top_p"",
            display_name=""Top P"",
            info=""Works together with top-k. (Default: 0.9)"",
            advanced=True,
        ),
        FloatInput(
            name=""repeat_penalty"",
            display_name=""Repeat Penalty"",
            info=""Penalty for repetitions in generated text. (Default: 1.1)"",
            advanced=True,
        ),
        FloatInput(
            name=""temperature"",
            display_name=""Temperature"",
            value=0.2,
            info=""Controls the creativity of model responses."",
        ),
        StrInput(
            name=""system_message"",
            display_name=""System Message"",
            info=""System message to pass to the model."",
            advanced=True,
        ),
    ]

    def make_request(self) -> Message:
        api_key = SecretStr(self.aiml_api_key) if self.aiml_api_key else None

        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"" if api_key else """",
        }

        messages = []
        if self.system_message:
            messages.append({""role"": ""system"", ""content"": self.system_message})

        if self.input_value:
            if isinstance(self.input_value, Message):
                # Though we aren't using langchain here, the helper method is useful
                message = self.input_value.to_lc_message()
                if message.type == ""human"":
                    messages.append({""role"": ""user"", ""content"": message.content})
                else:
                    raise ValueError(f""Expected user message, saw: {message.type}"")
            else:
                raise TypeError(f""Expected Message type, saw: {type(self.input_value)}"")
        else:
            raise ValueError(""Please provide an input value"")

        payload = {
            ""model"": self.model_name,
            ""messages"": messages,
            ""max_tokens"": self.max_tokens or None,
            ""temperature"": self.temperature or 0.2,
            ""top_k"": self.top_k or 40,
            ""top_p"": self.top_p or 0.9,
            ""repeat_penalty"": self.repeat_penalty or 1.1,
            ""stop_tokens"": self.stop_tokens or None,
        }

        try:
            response = httpx.post(self.chat_completion_url, headers=headers, json=payload)
            try:
                response.raise_for_status()
                result_data = response.json()
                choice = result_data[""choices""][0]
                result = choice[""message""][""content""]
            except httpx.HTTPStatusError as http_err:
                logger.error(f""HTTP error occurred: {http_err}"")
                raise http_err
            except httpx.RequestError as req_err:
                logger.error(f""Request error occurred: {req_err}"")
                raise req_err
            except json.JSONDecodeError:
                logger.warning(""Failed to decode JSON, response text: {response.text}"")
                result = response.text
            except KeyError as key_err:
                logger.warning(f""Key error: {key_err}, response content: {result_data}"")
                raise key_err

            self.status = result
        except httpx.TimeoutException:
            return Message(text=""Request timed out."")
        except Exception as exc:
            logger.error(f""Error: {exc}"")
            raise

        return Message(text=result)
",AIMLModel,models,Gera texto usando a API AI/ML
"from langchain_aws import ChatBedrock

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.inputs import MessageTextInput, SecretStrInput
from langflow.io import DictInput, DropdownInput


class AmazonBedrockComponent(LCModelComponent):
    display_name: str = ""Amazon Bedrock""
    description: str = ""Generate text using Amazon Bedrock LLMs.""
    icon = ""Amazon""
    name = ""AmazonBedrockModel""

    inputs = LCModelComponent._base_inputs + [
        DropdownInput(
            name=""model_id"",
            display_name=""Model ID"",
            options=[
                ""amazon.titan-text-express-v1"",
                ""amazon.titan-text-lite-v1"",
                ""amazon.titan-text-premier-v1:0"",
                ""amazon.titan-embed-text-v1"",
                ""amazon.titan-embed-text-v2:0"",
                ""amazon.titan-embed-image-v1"",
                ""amazon.titan-image-generator-v1"",
                ""anthropic.claude-v2"",
                ""anthropic.claude-v2:1"",
                ""anthropic.claude-3-sonnet-20240229-v1:0"",
                ""anthropic.claude-3-haiku-20240307-v1:0"",
                ""anthropic.claude-3-opus-20240229-v1:0"",
                ""anthropic.claude-instant-v1"",
                ""ai21.j2-mid-v1"",
                ""ai21.j2-ultra-v1"",
                ""cohere.command-text-v14"",
                ""cohere.command-light-text-v14"",
                ""cohere.command-r-v1:0"",
                ""cohere.command-r-plus-v1:0"",
                ""cohere.embed-english-v3"",
                ""cohere.embed-multilingual-v3"",
                ""meta.llama2-13b-chat-v1"",
                ""meta.llama2-70b-chat-v1"",
                ""meta.llama3-8b-instruct-v1:0"",
                ""meta.llama3-70b-instruct-v1:0"",
                ""mistral.mistral-7b-instruct-v0:2"",
                ""mistral.mixtral-8x7b-instruct-v0:1"",
                ""mistral.mistral-large-2402-v1:0"",
                ""mistral.mistral-small-2402-v1:0"",
                ""stability.stable-diffusion-xl-v0"",
                ""stability.stable-diffusion-xl-v1"",
            ],
            value=""anthropic.claude-3-haiku-20240307-v1:0"",
        ),
        SecretStrInput(name=""aws_access_key"", display_name=""Access Key""),
        SecretStrInput(name=""aws_secret_key"", display_name=""Secret Key""),
        MessageTextInput(name=""credentials_profile_name"", display_name=""Credentials Profile Name"", advanced=True),
        MessageTextInput(name=""region_name"", display_name=""Region Name"", value=""us-east-1""),
        DictInput(name=""model_kwargs"", display_name=""Model Kwargs"", advanced=True, is_list=True),
        MessageTextInput(name=""endpoint_url"", display_name=""Endpoint URL"", advanced=True),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        if self.aws_access_key:
            import boto3  # type: ignore

            session = boto3.Session(
                aws_access_key_id=self.aws_access_key,
                aws_secret_access_key=self.aws_secret_key,
            )
        elif self.credentials_profile_name:
            import boto3

            session = boto3.Session(profile_name=self.credentials_profile_name)
        else:
            import boto3

            session = boto3.Session()

        client_params = {}
        if self.endpoint_url:
            client_params[""endpoint_url""] = self.endpoint_url
        if self.region_name:
            client_params[""region_name""] = self.region_name

        boto3_client = session.client(""bedrock-runtime"", **client_params)
        try:
            output = ChatBedrock(  # type: ignore
                client=boto3_client,
                model_id=self.model_id,
                region_name=self.region_name,
                model_kwargs=self.model_kwargs,
                endpoint_url=self.endpoint_url,
                streaming=self.stream,
            )
        except Exception as e:
            raise ValueError(""Could not connect to AmazonBedrock API."") from e
        return output  # type: ignore
",AmazonBedrockModel,models,Gera texto usando modelos LLM da Amazon Bedrock.
"from langchain_anthropic.chat_models import ChatAnthropic
from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput


class AnthropicModelComponent(LCModelComponent):
    display_name = ""Anthropic""
    description = ""Generate text using Anthropic Chat&Completion LLMs with prefill support.""
    icon = ""Anthropic""
    name = ""AnthropicModel""

    inputs = LCModelComponent._base_inputs + [
        IntInput(
            name=""max_tokens"",
            display_name=""Max Tokens"",
            advanced=True,
            value=4096,
            info=""The maximum number of tokens to generate. Set to 0 for unlimited tokens."",
        ),
        DropdownInput(
            name=""model"",
            display_name=""Model Name"",
            options=[
                ""claude-3-5-sonnet-20240620"",
                ""claude-3-opus-20240229"",
                ""claude-3-sonnet-20240229"",
                ""claude-3-haiku-20240307"",
            ],
            info=""https://python.langchain.com/docs/integrations/chat/anthropic"",
            value=""claude-3-5-sonnet-20240620"",
        ),
        SecretStrInput(
            name=""anthropic_api_key"",
            display_name=""Anthropic API Key"",
            info=""Your Anthropic API key."",
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", value=0.1),
        MessageTextInput(
            name=""anthropic_api_url"",
            display_name=""Anthropic API URL"",
            advanced=True,
            info=""Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified."",
        ),
        MessageTextInput(
            name=""prefill"",
            display_name=""Prefill"",
            info=""Prefill text to guide the model's response."",
            advanced=True,
        ),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        model = self.model
        anthropic_api_key = self.anthropic_api_key
        max_tokens = self.max_tokens
        temperature = self.temperature
        anthropic_api_url = self.anthropic_api_url or ""https://api.anthropic.com""

        try:
            output = ChatAnthropic(
                model=model,
                anthropic_api_key=(SecretStr(anthropic_api_key) if anthropic_api_key else None),
                max_tokens_to_sample=max_tokens,  # type: ignore
                temperature=temperature,
                anthropic_api_url=anthropic_api_url,
                streaming=self.stream,
            )
        except Exception as e:
            raise ValueError(""Could not connect to Anthropic API."") from e

        return output  # type: ignore

    def _get_exception_message(self, exception: Exception) -> str | None:
        """"""
        Get a message from an Anthropic exception.

        Args:
            exception (Exception): The exception to get the message from.

        Returns:
            str: The message from the exception.
        """"""
        try:
            from anthropic import BadRequestError
        except ImportError:
            return None
        if isinstance(exception, BadRequestError):
            message = exception.body.get(""error"", {}).get(""message"")  # type: ignore
            if message:
                return message
        return None
",AnthropicModel,models,Gera texto usando modelos LLM do Anthropic Chat&Completion com suporte a prefill.
"from langchain_openai import AzureChatOpenAI
from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.inputs import MessageTextInput
from langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput


class AzureChatOpenAIComponent(LCModelComponent):
    display_name: str = ""Azure OpenAI""
    description: str = ""Generate text using Azure OpenAI LLMs.""
    documentation: str = ""https://python.langchain.com/docs/integrations/llms/azure_openai""
    beta = False
    icon = ""Azure""
    name = ""AzureOpenAIModel""

    AZURE_OPENAI_API_VERSIONS = [
        ""2023-03-15-preview"",
        ""2023-05-15"",
        ""2023-06-01-preview"",
        ""2023-07-01-preview"",
        ""2023-08-01-preview"",
        ""2023-09-01-preview"",
        ""2023-12-01-preview"",
        ""2024-04-09"",
        ""2024-05-13"",
    ]

    inputs = LCModelComponent._base_inputs + [
        MessageTextInput(
            name=""azure_endpoint"",
            display_name=""Azure Endpoint"",
            info=""Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`"",
            required=True,
        ),
        MessageTextInput(name=""azure_deployment"", display_name=""Deployment Name"", required=True),
        SecretStrInput(name=""api_key"", display_name=""API Key""),
        DropdownInput(
            name=""api_version"",
            display_name=""API Version"",
            options=AZURE_OPENAI_API_VERSIONS,
            value=AZURE_OPENAI_API_VERSIONS[-1],
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", value=0.7),
        IntInput(
            name=""max_tokens"",
            display_name=""Max Tokens"",
            advanced=True,
            info=""The maximum number of tokens to generate. Set to 0 for unlimited tokens."",
        ),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        azure_endpoint = self.azure_endpoint
        azure_deployment = self.azure_deployment
        api_version = self.api_version
        api_key = self.api_key
        temperature = self.temperature
        max_tokens = self.max_tokens
        stream = self.stream

        try:
            output = AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                azure_deployment=azure_deployment,
                api_version=api_version,
                api_key=api_key,
                temperature=temperature,
                max_tokens=max_tokens or None,
                streaming=stream,
            )
        except Exception as e:
            raise ValueError(f""Could not connect to AzureOpenAI API: {str(e)}"") from e

        return output  # type: ignore
",AzureOpenAIModel,models,Gera texto usando modelos LLM da Azure OpenAI.
"from langchain_community.chat_models.baidu_qianfan_endpoint import QianfanChatEndpoint
from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing.constants import LanguageModel
from langflow.io import DropdownInput, FloatInput, MessageTextInput, SecretStrInput


class QianfanChatEndpointComponent(LCModelComponent):
    display_name: str = ""Qianfan""
    description: str = ""Generate text using Baidu Qianfan LLMs.""
    documentation: str = ""https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint""
    icon = ""BaiduQianfan""
    name = ""BaiduQianfanChatModel""

    inputs = LCModelComponent._base_inputs + [
        DropdownInput(
            name=""model"",
            display_name=""Model Name"",
            options=[
                ""ERNIE-Bot"",
                ""ERNIE-Bot-turbo"",
                ""BLOOMZ-7B"",
                ""Llama-2-7b-chat"",
                ""Llama-2-13b-chat"",
                ""Llama-2-70b-chat"",
                ""Qianfan-BLOOMZ-7B-compressed"",
                ""Qianfan-Chinese-Llama-2-7B"",
                ""ChatGLM2-6B-32K"",
                ""AquilaChat-7B"",
            ],
            info=""https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint"",
            value=""ERNIE-Bot-turbo"",
        ),
        SecretStrInput(
            name=""qianfan_ak"",
            display_name=""Qianfan Ak"",
            info=""which you could get from  https://cloud.baidu.com/product/wenxinworkshop"",
        ),
        SecretStrInput(
            name=""qianfan_sk"",
            display_name=""Qianfan Sk"",
            info=""which you could get from  https://cloud.baidu.com/product/wenxinworkshop"",
        ),
        FloatInput(
            name=""top_p"",
            display_name=""Top p"",
            info=""Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo"",
            value=0.8,
            advanced=True,
        ),
        FloatInput(
            name=""temperature"",
            display_name=""Temperature"",
            info=""Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo"",
            value=0.95,
        ),
        FloatInput(
            name=""penalty_score"",
            display_name=""Penalty Score"",
            info=""Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo"",
            value=1.0,
            advanced=True,
        ),
        MessageTextInput(
            name=""endpoint"",
            display_name=""Endpoint"",
            info=""Endpoint of the Qianfan LLM, required if custom model used."",
        ),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        model = self.model
        qianfan_ak = self.qianfan_ak
        qianfan_sk = self.qianfan_sk
        top_p = self.top_p
        temperature = self.temperature
        penalty_score = self.penalty_score
        endpoint = self.endpoint

        try:
            output = QianfanChatEndpoint(  # type: ignore
                model=model,
                qianfan_ak=SecretStr(qianfan_ak) if qianfan_ak else None,
                qianfan_sk=SecretStr(qianfan_sk) if qianfan_sk else None,
                top_p=top_p,
                temperature=temperature,
                penalty_score=penalty_score,
                endpoint=endpoint,
            )
        except Exception as e:
            raise ValueError(""Could not connect to Baidu Qianfan API."") from e

        return output  # type: ignore
",BaiduQianfanChatModel,models,Gera texto usando modelos LLM da Baidu Qianfan.
"from langchain_cohere import ChatCohere
from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.io import FloatInput, SecretStrInput


class CohereComponent(LCModelComponent):
    display_name = ""Cohere""
    description = ""Generate text using Cohere LLMs.""
    documentation = ""https://python.langchain.com/docs/modules/model_io/models/llms/integrations/cohere""
    icon = ""Cohere""
    name = ""CohereModel""

    inputs = LCModelComponent._base_inputs + [
        SecretStrInput(
            name=""cohere_api_key"",
            display_name=""Cohere API Key"",
            info=""The Cohere API Key to use for the Cohere model."",
            advanced=False,
            value=""COHERE_API_KEY"",
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", value=0.75),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        cohere_api_key = self.cohere_api_key
        temperature = self.temperature

        if cohere_api_key:
            api_key = SecretStr(cohere_api_key)
        else:
            api_key = None

        output = ChatCohere(
            temperature=temperature or 0.75,
            cohere_api_key=api_key,
        )

        return output  # type: ignore
",CohereModel,models,Gera texto usando modelos LLM da Cohere.
"from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput


class GoogleGenerativeAIComponent(LCModelComponent):
    display_name = ""Google Generative AI""
    description = ""Generate text using Google Generative AI.""
    icon = ""GoogleGenerativeAI""
    name = ""GoogleGenerativeAIModel""

    inputs = LCModelComponent._base_inputs + [
        IntInput(
            name=""max_output_tokens"",
            display_name=""Max Output Tokens"",
            info=""The maximum number of tokens to generate."",
        ),
        DropdownInput(
            name=""model"",
            display_name=""Model"",
            info=""The name of the model to use."",
            options=[""gemini-1.5-pro"", ""gemini-1.5-flash"", ""gemini-1.0-pro"", ""gemini-1.0-pro-vision""],
            value=""gemini-1.5-pro"",
        ),
        SecretStrInput(
            name=""google_api_key"",
            display_name=""Google API Key"",
            info=""The Google API Key to use for the Google Generative AI."",
        ),
        FloatInput(
            name=""top_p"",
            display_name=""Top P"",
            info=""The maximum cumulative probability of tokens to consider when sampling."",
            advanced=True,
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", value=0.1),
        IntInput(
            name=""n"",
            display_name=""N"",
            info=""Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated."",
            advanced=True,
        ),
        IntInput(
            name=""top_k"",
            display_name=""Top K"",
            info=""Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive."",
            advanced=True,
        ),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
        except ImportError:
            raise ImportError(""The 'langchain_google_genai' package is required to use the Google Generative AI model."")

        google_api_key = self.google_api_key
        model = self.model
        max_output_tokens = self.max_output_tokens
        temperature = self.temperature
        top_k = self.top_k
        top_p = self.top_p
        n = self.n

        output = ChatGoogleGenerativeAI(  # type: ignore
            model=model,
            max_output_tokens=max_output_tokens or None,
            temperature=temperature,
            top_k=top_k or None,
            top_p=top_p or None,
            n=n or 1,
            google_api_key=SecretStr(google_api_key),
        )

        return output  # type: ignore
",GoogleGenerativeAIModel,models,Gera texto usando o Generative AI do Google.
"import requests
from typing import List
from langchain_groq import ChatGroq
from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput


class GroqModel(LCModelComponent):
    display_name: str = ""Groq""
    description: str = ""Generate text using Groq.""
    icon = ""Groq""
    name = ""GroqModel""

    inputs = LCModelComponent._base_inputs + [
        SecretStrInput(
            name=""groq_api_key"",
            display_name=""Groq API Key"",
            info=""API key for the Groq API."",
        ),
        MessageTextInput(
            name=""groq_api_base"",
            display_name=""Groq API Base"",
            info=""Base URL path for API requests, leave blank if not using a proxy or service emulator."",
            advanced=True,
            value=""https://api.groq.com"",
        ),
        IntInput(
            name=""max_tokens"",
            display_name=""Max Output Tokens"",
            info=""The maximum number of tokens to generate."",
            advanced=True,
        ),
        FloatInput(
            name=""temperature"",
            display_name=""Temperature"",
            info=""Run inference with this temperature. Must by in the closed interval [0.0, 1.0]."",
            value=0.1,
        ),
        IntInput(
            name=""n"",
            display_name=""N"",
            info=""Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated."",
            advanced=True,
        ),
        DropdownInput(
            name=""model_name"",
            display_name=""Model"",
            info=""The name of the model to use."",
            options=[],
            refresh_button=True,
        ),
    ]

    def get_models(self) -> List[str]:
        api_key = self.groq_api_key
        base_url = self.groq_api_base or ""https://api.groq.com""
        url = f""{base_url}/openai/v1/models""

        headers = {""Authorization"": f""Bearer {api_key}"", ""Content-Type"": ""application/json""}

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            model_list = response.json()
            return [model[""id""] for model in model_list.get(""data"", [])]
        except requests.RequestException as e:
            self.status = f""Error fetching models: {str(e)}""
            return []

    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):
        if field_name == ""groq_api_key"" or field_name == ""groq_api_base"" or field_name == ""model_name"":
            models = self.get_models()
            build_config[""model_name""][""options""] = models
        return build_config

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        groq_api_key = self.groq_api_key
        model_name = self.model_name
        max_tokens = self.max_tokens
        temperature = self.temperature
        groq_api_base = self.groq_api_base
        n = self.n
        stream = self.stream

        output = ChatGroq(  # type: ignore
            model=model_name,
            max_tokens=max_tokens or None,
            temperature=temperature,
            base_url=groq_api_base,
            n=n or 1,
            api_key=SecretStr(groq_api_key),
            streaming=stream,
        )

        return output  # type: ignore
",GroqModel,models,Gera texto usando a Groq.
"from langchain_community.chat_models.huggingface import ChatHuggingFace
from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.io import DictInput, DropdownInput, SecretStrInput, StrInput


class HuggingFaceEndpointsComponent(LCModelComponent):
    display_name: str = ""Hugging Face API""
    description: str = ""Generate text using Hugging Face Inference APIs.""
    icon = ""HuggingFace""
    name = ""HuggingFaceModel""

    inputs = LCModelComponent._base_inputs + [
        SecretStrInput(name=""endpoint_url"", display_name=""Endpoint URL"", password=True),
        StrInput(
            name=""model_id"",
            display_name=""Model Id"",
            info=""Id field of endpoint_url response."",
        ),
        DropdownInput(
            name=""task"",
            display_name=""Task"",
            options=[""text2text-generation"", ""text-generation"", ""summarization""],
        ),
        SecretStrInput(name=""huggingfacehub_api_token"", display_name=""API token"", password=True),
        DictInput(name=""model_kwargs"", display_name=""Model Keyword Arguments"", advanced=True),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        endpoint_url = self.endpoint_url
        task = self.task
        huggingfacehub_api_token = self.huggingfacehub_api_token
        model_kwargs = self.model_kwargs or {}

        try:
            llm = HuggingFaceEndpoint(  # type: ignore
                endpoint_url=endpoint_url,
                task=task,
                huggingfacehub_api_token=huggingfacehub_api_token,
                model_kwargs=model_kwargs,
            )
        except Exception as e:
            raise ValueError(""Could not connect to HuggingFace Endpoints API."") from e

        output = ChatHuggingFace(llm=llm, model_id=self.model_id)
        return output  # type: ignore
",HuggingFaceModel,models,Gera texto usando as APIs de Inferência da Hugging Face.
"from langchain_community.chat_models import ChatMaritalk

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.field_typing.range_spec import RangeSpec
from langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput


class MaritalkModelComponent(LCModelComponent):
    display_name = ""Maritalk""
    description = ""Generates text using Maritalk LLMs.""
    icon = ""Maritalk""
    name = ""Maritalk""
    inputs = LCModelComponent._base_inputs + [
        IntInput(
            name=""max_tokens"",
            display_name=""Max Tokens"",
            advanced=True,
            value=512,
            info=""The maximum number of tokens to generate. Set to 0 for unlimited tokens."",
        ),
        DropdownInput(
            name=""model_name"",
            display_name=""Model Name"",
            advanced=False,
            options=[""sabia-2-small"", ""sabia-2-medium""],
            value=[""sabia-2-small""],
        ),
        SecretStrInput(
            name=""api_key"",
            display_name=""Maritalk API Key"",
            info=""The Maritalk API Key to use for the OpenAI model."",
            advanced=False,
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", value=0.1, range_spec=RangeSpec(min=0, max=1)),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        # self.output_schea is a list of dictionarie s
        # let's convert it to a dictionary
        api_key = self.api_key
        temperature = self.temperature
        model_name: str = self.model_name
        max_tokens = self.max_tokens

        output = ChatMaritalk(
            max_tokens=max_tokens,
            model=model_name,
            api_key=api_key,
            temperature=temperature or 0.1,
        )
        return output  # type: ignore
",Maritalk,models,Gera texto usando modelos LLM da Maritalk.
"from langchain_mistralai import ChatMistralAI
from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput


class MistralAIModelComponent(LCModelComponent):
    display_name = ""MistralAI""
    description = ""Generates text using MistralAI LLMs.""
    icon = ""MistralAI""
    name = ""MistralModel""

    inputs = LCModelComponent._base_inputs + [
        IntInput(
            name=""max_tokens"",
            display_name=""Max Tokens"",
            advanced=True,
            info=""The maximum number of tokens to generate. Set to 0 for unlimited tokens."",
        ),
        DropdownInput(
            name=""model_name"",
            display_name=""Model Name"",
            advanced=False,
            options=[
                ""open-mixtral-8x7b"",
                ""open-mixtral-8x22b"",
                ""mistral-small-latest"",
                ""mistral-medium-latest"",
                ""mistral-large-latest"",
                ""codestral-latest"",
            ],
            value=""codestral-latest"",
        ),
        StrInput(
            name=""mistral_api_base"",
            display_name=""Mistral API Base"",
            advanced=True,
            info=(
                ""The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. ""
                ""You can change this to use other APIs like JinaChat, LocalAI and Prem.""
            ),
        ),
        SecretStrInput(
            name=""api_key"",
            display_name=""Mistral API Key"",
            info=""The Mistral API Key to use for the Mistral model."",
            advanced=False,
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", advanced=False, value=0.5),
        IntInput(name=""max_retries"", display_name=""Max Retries"", advanced=True, value=5),
        IntInput(name=""timeout"", display_name=""Timeout"", advanced=True, value=60),
        IntInput(name=""max_concurrent_requests"", display_name=""Max Concurrent Requests"", advanced=True, value=3),
        FloatInput(name=""top_p"", display_name=""Top P"", advanced=True, value=1),
        IntInput(name=""random_seed"", display_name=""Random Seed"", value=1, advanced=True),
        BoolInput(name=""safe_mode"", display_name=""Safe Mode"", advanced=True),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        mistral_api_key = self.api_key
        temperature = self.temperature
        model_name = self.model_name
        max_tokens = self.max_tokens
        mistral_api_base = self.mistral_api_base or ""https://api.mistral.ai/v1""
        max_retries = self.max_retries
        timeout = self.timeout
        max_concurrent_requests = self.max_concurrent_requests
        top_p = self.top_p
        random_seed = self.random_seed
        safe_mode = self.safe_mode

        if mistral_api_key:
            api_key = SecretStr(mistral_api_key)
        else:
            api_key = None

        output = ChatMistralAI(
            max_tokens=max_tokens or None,
            model_name=model_name,
            endpoint=mistral_api_base,
            api_key=api_key,
            temperature=temperature,
            max_retries=max_retries,
            timeout=timeout,
            max_concurrent_requests=max_concurrent_requests,
            top_p=top_p,
            random_seed=random_seed,
            safe_mode=safe_mode,
        )

        return output  # type: ignore
",MistralModel,models,Gera texto usando modelos LLM da MistralAI.
"from typing import Any

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput
from langflow.schema.dotdict import dotdict


class NVIDIAModelComponent(LCModelComponent):
    display_name = ""NVIDIA""
    description = ""Generates text using NVIDIA LLMs.""
    icon = ""NVIDIA""

    inputs = LCModelComponent._base_inputs + [
        IntInput(
            name=""max_tokens"",
            display_name=""Max Tokens"",
            advanced=True,
            info=""The maximum number of tokens to generate. Set to 0 for unlimited tokens."",
        ),
        DropdownInput(
            name=""model_name"",
            display_name=""Model Name"",
            advanced=False,
            options=[""mistralai/mixtral-8x7b-instruct-v0.1""],
            value=""mistralai/mixtral-8x7b-instruct-v0.1"",
        ),
        StrInput(
            name=""base_url"",
            display_name=""NVIDIA Base URL"",
            value=""https://integrate.api.nvidia.com/v1"",
            refresh_button=True,
            info=""The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1."",
        ),
        SecretStrInput(
            name=""nvidia_api_key"",
            display_name=""NVIDIA API Key"",
            info=""The NVIDIA API Key."",
            advanced=False,
            value=""NVIDIA_API_KEY"",
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", value=0.1),
        IntInput(
            name=""seed"",
            display_name=""Seed"",
            info=""The seed controls the reproducibility of the job."",
            advanced=True,
            value=1,
        ),
    ]

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        if field_name == ""base_url"" and field_value:
            try:
                build_model = self.build_model()
                ids = [model.id for model in build_model.available_models]  # type: ignore
                build_config[""model_name""][""options""] = ids
                build_config[""model_name""][""value""] = ids[0]
            except Exception as e:
                raise ValueError(f""Error getting model names: {e}"")
        return build_config

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        try:
            from langchain_nvidia_ai_endpoints import ChatNVIDIA
        except ImportError:
            raise ImportError(""Please install langchain-nvidia-ai-endpoints to use the NVIDIA model."")
        nvidia_api_key = self.nvidia_api_key
        temperature = self.temperature
        model_name: str = self.model_name
        max_tokens = self.max_tokens
        seed = self.seed
        output = ChatNVIDIA(
            max_tokens=max_tokens or None,
            model=model_name,
            base_url=self.base_url,
            api_key=nvidia_api_key,  # type: ignore
            temperature=temperature or 0.1,
            seed=seed,
        )
        return output  # type: ignore
",NvidiaModel,models,Gera texto usando modelos LLM da NVIDIA.
"from typing import Any

import httpx
from langchain_community.chat_models import ChatOllama

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput


class ChatOllamaComponent(LCModelComponent):
    display_name = ""Ollama""
    description = ""Generate text using Ollama Local LLMs.""
    icon = ""Ollama""
    name = ""OllamaModel""

    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):
        if field_name == ""mirostat"":
            if field_value == ""Disabled"":
                build_config[""mirostat_eta""][""advanced""] = True
                build_config[""mirostat_tau""][""advanced""] = True
                build_config[""mirostat_eta""][""value""] = None
                build_config[""mirostat_tau""][""value""] = None

            else:
                build_config[""mirostat_eta""][""advanced""] = False
                build_config[""mirostat_tau""][""advanced""] = False

                if field_value == ""Mirostat 2.0"":
                    build_config[""mirostat_eta""][""value""] = 0.2
                    build_config[""mirostat_tau""][""value""] = 10
                else:
                    build_config[""mirostat_eta""][""value""] = 0.1
                    build_config[""mirostat_tau""][""value""] = 5

        if field_name == ""model_name"":
            base_url_dict = build_config.get(""base_url"", {})
            base_url_load_from_db = base_url_dict.get(""load_from_db"", False)
            base_url_value = base_url_dict.get(""value"")
            if base_url_load_from_db:
                base_url_value = self.variables(base_url_value)
            elif not base_url_value:
                base_url_value = ""http://localhost:11434""
            build_config[""model_name""][""options""] = self.get_model(base_url_value + ""/api/tags"")

        if field_name == ""keep_alive_flag"":
            if field_value == ""Keep"":
                build_config[""keep_alive""][""value""] = ""-1""
                build_config[""keep_alive""][""advanced""] = True
            elif field_value == ""Immediately"":
                build_config[""keep_alive""][""value""] = ""0""
                build_config[""keep_alive""][""advanced""] = True
            else:
                build_config[""keep_alive""][""advanced""] = False

        return build_config

    def get_model(self, url: str) -> list[str]:
        try:
            with httpx.Client() as client:
                response = client.get(url)
                response.raise_for_status()
                data = response.json()

                model_names = [model[""name""] for model in data.get(""models"", [])]
                return model_names
        except Exception as e:
            raise ValueError(""Could not retrieve models. Please, make sure Ollama is running."") from e

    inputs = LCModelComponent._base_inputs + [
        StrInput(
            name=""base_url"",
            display_name=""Base URL"",
            info=""Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified."",
            value=""http://localhost:11434"",
        ),
        DropdownInput(
            name=""model_name"",
            display_name=""Model Name"",
            value=""llama3"",
            info=""Refer to https://ollama.ai/library for more models."",
            refresh_button=True,
        ),
        FloatInput(
            name=""temperature"",
            display_name=""Temperature"",
            value=0.2,
            info=""Controls the creativity of model responses."",
        ),
        StrInput(
            name=""format"",
            display_name=""Format"",
            info=""Specify the format of the output (e.g., json)."",
            advanced=True,
        ),
        DictInput(
            name=""metadata"",
            display_name=""Metadata"",
            info=""Metadata to add to the run trace."",
            advanced=True,
        ),
        DropdownInput(
            name=""mirostat"",
            display_name=""Mirostat"",
            options=[""Disabled"", ""Mirostat"", ""Mirostat 2.0""],
            info=""Enable/disable Mirostat sampling for controlling perplexity."",
            value=""Disabled"",
            advanced=True,
            real_time_refresh=True,
        ),
        FloatInput(
            name=""mirostat_eta"",
            display_name=""Mirostat Eta"",
            info=""Learning rate for Mirostat algorithm. (Default: 0.1)"",
            advanced=True,
        ),
        FloatInput(
            name=""mirostat_tau"",
            display_name=""Mirostat Tau"",
            info=""Controls the balance between coherence and diversity of the output. (Default: 5.0)"",
            advanced=True,
        ),
        IntInput(
            name=""num_ctx"",
            display_name=""Context Window Size"",
            info=""Size of the context window for generating tokens. (Default: 2048)"",
            advanced=True,
        ),
        IntInput(
            name=""num_gpu"",
            display_name=""Number of GPUs"",
            info=""Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)"",
            advanced=True,
        ),
        IntInput(
            name=""num_thread"",
            display_name=""Number of Threads"",
            info=""Number of threads to use during computation. (Default: detected for optimal performance)"",
            advanced=True,
        ),
        IntInput(
            name=""repeat_last_n"",
            display_name=""Repeat Last N"",
            info=""How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)"",
            advanced=True,
        ),
        FloatInput(
            name=""repeat_penalty"",
            display_name=""Repeat Penalty"",
            info=""Penalty for repetitions in generated text. (Default: 1.1)"",
            advanced=True,
        ),
        FloatInput(
            name=""tfs_z"",
            display_name=""TFS Z"",
            info=""Tail free sampling value. (Default: 1)"",
            advanced=True,
        ),
        IntInput(
            name=""timeout"",
            display_name=""Timeout"",
            info=""Timeout for the request stream."",
            advanced=True,
        ),
        IntInput(
            name=""top_k"",
            display_name=""Top K"",
            info=""Limits token selection to top K. (Default: 40)"",
            advanced=True,
        ),
        FloatInput(
            name=""top_p"",
            display_name=""Top P"",
            info=""Works together with top-k. (Default: 0.9)"",
            advanced=True,
        ),
        BoolInput(
            name=""verbose"",
            display_name=""Verbose"",
            info=""Whether to print out response text."",
        ),
        StrInput(
            name=""tags"",
            display_name=""Tags"",
            info=""Comma-separated list of tags to add to the run trace."",
            advanced=True,
        ),
        StrInput(
            name=""stop_tokens"",
            display_name=""Stop Tokens"",
            info=""Comma-separated list of tokens to signal the model to stop generating text."",
            advanced=True,
        ),
        StrInput(
            name=""system"",
            display_name=""System"",
            info=""System to use for generating text."",
            advanced=True,
        ),
        StrInput(
            name=""template"",
            display_name=""Template"",
            info=""Template to use for generating text."",
            advanced=True,
        ),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        # Mapping mirostat settings to their corresponding values
        mirostat_options = {""Mirostat"": 1, ""Mirostat 2.0"": 2}

        # Default to 0 for 'Disabled'
        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore

        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled
        if mirostat_value == 0:
            mirostat_eta = None
            mirostat_tau = None
        else:
            mirostat_eta = self.mirostat_eta
            mirostat_tau = self.mirostat_tau

        # Mapping system settings to their corresponding values
        llm_params = {
            ""base_url"": self.base_url,
            ""model"": self.model_name,
            ""mirostat"": mirostat_value,
            ""format"": self.format,
            ""metadata"": self.metadata,
            ""tags"": self.tags.split("","") if self.tags else None,
            ""mirostat_eta"": mirostat_eta,
            ""mirostat_tau"": mirostat_tau,
            ""num_ctx"": self.num_ctx or None,
            ""num_gpu"": self.num_gpu or None,
            ""num_thread"": self.num_thread or None,
            ""repeat_last_n"": self.repeat_last_n or None,
            ""repeat_penalty"": self.repeat_penalty or None,
            ""temperature"": self.temperature or None,
            ""stop"": self.stop_tokens.split("","") if self.stop_tokens else None,
            ""system"": self.system,
            ""template"": self.template,
            ""tfs_z"": self.tfs_z or None,
            ""timeout"": self.timeout or None,
            ""top_k"": self.top_k or None,
            ""top_p"": self.top_p or None,
            ""verbose"": self.verbose,
        }

        # Remove parameters with None values
        llm_params = {k: v for k, v in llm_params.items() if v is not None}

        try:
            output = ChatOllama(**llm_params)  # type: ignore
        except Exception as e:
            raise ValueError(""Could not initialize Ollama LLM."") from e

        return output  # type: ignore
",OllamaModel,models,Gera texto usando modelos LLM Locais da Ollama.
"import operator
from functools import reduce

from langflow.field_typing.range_spec import RangeSpec
from langchain_openai import ChatOpenAI
from pydantic.v1 import SecretStr

from langflow.base.models.model import LCModelComponent
from langflow.base.models.openai_constants import OPENAI_MODEL_NAMES
from langflow.field_typing import LanguageModel
from langflow.inputs import (
    BoolInput,
    DictInput,
    DropdownInput,
    FloatInput,
    IntInput,
    SecretStrInput,
    StrInput,
)


class OpenAIModelComponent(LCModelComponent):
    display_name = ""OpenAI""
    description = ""Generates text using OpenAI LLMs.""
    icon = ""OpenAI""
    name = ""OpenAIModel""

    inputs = LCModelComponent._base_inputs + [
        IntInput(
            name=""max_tokens"",
            display_name=""Max Tokens"",
            advanced=True,
            info=""The maximum number of tokens to generate. Set to 0 for unlimited tokens."",
            range_spec=RangeSpec(min=0, max=128000),
        ),
        DictInput(name=""model_kwargs"", display_name=""Model Kwargs"", advanced=True),
        BoolInput(
            name=""json_mode"",
            display_name=""JSON Mode"",
            advanced=True,
            info=""If True, it will output JSON regardless of passing a schema."",
        ),
        DictInput(
            name=""output_schema"",
            is_list=True,
            display_name=""Schema"",
            advanced=True,
            info=""The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled."",
        ),
        DropdownInput(
            name=""model_name"",
            display_name=""Model Name"",
            advanced=False,
            options=OPENAI_MODEL_NAMES,
            value=OPENAI_MODEL_NAMES[0],
        ),
        StrInput(
            name=""openai_api_base"",
            display_name=""OpenAI API Base"",
            advanced=True,
            info=""The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem."",
        ),
        SecretStrInput(
            name=""api_key"",
            display_name=""OpenAI API Key"",
            info=""The OpenAI API Key to use for the OpenAI model."",
            advanced=False,
            value=""OPENAI_API_KEY"",
        ),
        FloatInput(name=""temperature"", display_name=""Temperature"", value=0.1),
        IntInput(
            name=""seed"",
            display_name=""Seed"",
            info=""The seed controls the reproducibility of the job."",
            advanced=True,
            value=1,
        ),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        # self.output_schema is a list of dictionaries
        # let's convert it to a dictionary
        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})
        openai_api_key = self.api_key
        temperature = self.temperature
        model_name: str = self.model_name
        max_tokens = self.max_tokens
        model_kwargs = self.model_kwargs or {}
        openai_api_base = self.openai_api_base or ""https://api.openai.com/v1""
        json_mode = bool(output_schema_dict) or self.json_mode
        seed = self.seed

        if openai_api_key:
            api_key = SecretStr(openai_api_key)
        else:
            api_key = None
        output = ChatOpenAI(
            max_tokens=max_tokens or None,
            model_kwargs=model_kwargs,
            model=model_name,
            base_url=openai_api_base,
            api_key=api_key,
            temperature=temperature or 0.1,
            seed=seed,
        )
        if json_mode:
            if output_schema_dict:
                output = output.with_structured_output(schema=output_schema_dict, method=""json_mode"")  # type: ignore
            else:
                output = output.bind(response_format={""type"": ""json_object""})  # type: ignore

        return output  # type: ignore

    def _get_exception_message(self, e: Exception):
        """"""
        Get a message from an OpenAI exception.

        Args:
            exception (Exception): The exception to get the message from.

        Returns:
            str: The message from the exception.
        """"""

        try:
            from openai import BadRequestError
        except ImportError:
            return
        if isinstance(e, BadRequestError):
            message = e.body.get(""message"")  # type: ignore
            if message:
                return message
        return
",OpenAIModel,models,Gera texto usando modelos LLM da OpenAI.
"from typing import cast

from langflow.base.models.model import LCModelComponent
from langflow.field_typing import LanguageModel
from langflow.inputs import MessageTextInput
from langflow.io import BoolInput, FileInput, FloatInput, IntInput, StrInput


class ChatVertexAIComponent(LCModelComponent):
    display_name = ""Vertex AI""
    description = ""Generate text using Vertex AI LLMs.""
    icon = ""VertexAI""
    name = ""VertexAiModel""

    inputs = LCModelComponent._base_inputs + [
        FileInput(
            name=""credentials"",
            display_name=""Credentials"",
            info=""JSON credentials file. Leave empty to fallback to environment variables"",
            file_types=[""json""],
        ),
        MessageTextInput(name=""model_name"", display_name=""Model Name"", value=""gemini-1.5-pro""),
        StrInput(name=""project"", display_name=""Project"", info=""The project ID."", advanced=True),
        StrInput(name=""location"", display_name=""Location"", value=""us-central1"", advanced=True),
        IntInput(name=""max_output_tokens"", display_name=""Max Output Tokens"", advanced=True),
        IntInput(name=""max_retries"", display_name=""Max Retries"", value=1, advanced=True),
        FloatInput(name=""temperature"", value=0.0, display_name=""Temperature""),
        IntInput(name=""top_k"", display_name=""Top K"", advanced=True),
        FloatInput(name=""top_p"", display_name=""Top P"", value=0.95, advanced=True),
        BoolInput(name=""verbose"", display_name=""Verbose"", value=False, advanced=True),
    ]

    def build_model(self) -> LanguageModel:
        try:
            from langchain_google_vertexai import ChatVertexAI
        except ImportError:
            raise ImportError(
                ""Please install the langchain-google-vertexai package to use the VertexAIEmbeddings component.""
            )
        location = self.location or None
        if self.credentials:
            from google.cloud import aiplatform
            from google.oauth2 import service_account

            credentials = service_account.Credentials.from_service_account_file(self.credentials)
            project = self.project or credentials.project_id
            # ChatVertexAI sometimes skip manual credentials initialization
            aiplatform.init(
                project=project,
                location=location,
                credentials=credentials,
            )
        else:
            project = self.project or None
            credentials = None

        return cast(
            LanguageModel,
            ChatVertexAI(
                credentials=credentials,
                location=location,
                project=project,
                max_output_tokens=self.max_output_tokens or None,
                max_retries=self.max_retries,
                model_name=self.model_name,
                temperature=self.temperature,
                top_k=self.top_k or None,
                top_p=self.top_p,
                verbose=self.verbose,
            ),
        )
",VertexAiModel,models,Gera texto usando modelos LLM do Vertex AI.
"from langflow.base.io.chat import ChatComponent
from langflow.inputs import BoolInput
from langflow.io import DropdownInput, MessageTextInput, Output
from langflow.memory import store_message
from langflow.schema.message import Message
from langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI


class ChatOutput(ChatComponent):
    display_name = ""Chat Output""
    description = ""Display a chat message in the Playground.""
    icon = ""ChatOutput""
    name = ""ChatOutput""

    inputs = [
        MessageTextInput(
            name=""input_value"",
            display_name=""Text"",
            info=""Message to be passed as output."",
        ),
        BoolInput(
            name=""should_store_message"",
            display_name=""Store Messages"",
            info=""Store the message in the history."",
            value=True,
            advanced=True,
        ),
        DropdownInput(
            name=""sender"",
            display_name=""Sender Type"",
            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],
            value=MESSAGE_SENDER_AI,
            advanced=True,
            info=""Type of sender."",
        ),
        MessageTextInput(
            name=""sender_name"",
            display_name=""Sender Name"",
            info=""Name of the sender."",
            value=MESSAGE_SENDER_NAME_AI,
            advanced=True,
        ),
        MessageTextInput(
            name=""session_id"",
            display_name=""Session ID"",
            info=""The session ID of the chat. If empty, the current session ID parameter will be used."",
            advanced=True,
        ),
        MessageTextInput(
            name=""data_template"",
            display_name=""Data Template"",
            value=""{text}"",
            advanced=True,
            info=""Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key."",
        ),
    ]
    outputs = [
        Output(display_name=""Message"", name=""message"", method=""message_response""),
    ]

    def message_response(self) -> Message:
        message = Message(
            text=self.input_value,
            sender=self.sender,
            sender_name=self.sender_name,
            session_id=self.session_id,
        )
        if (
            self.session_id
            and isinstance(message, Message)
            and isinstance(message.text, str)
            and self.should_store_message
        ):
            store_message(
                message,
                flow_id=self.graph.flow_id,
            )
            self.message.value = message

        self.status = message
        return message
",ChatOutput,outputs,Exibe uma mensagem de bate-papo no Playground.
"from langflow.base.io.text import TextComponent
from langflow.io import MessageTextInput, Output
from langflow.schema.message import Message


class TextOutputComponent(TextComponent):
    display_name = ""Text Output""
    description = ""Display a text output in the Playground.""
    icon = ""type""
    name = ""TextOutput""

    inputs = [
        MessageTextInput(
            name=""input_value"",
            display_name=""Text"",
            info=""Text to be passed as output."",
        ),
    ]
    outputs = [
        Output(display_name=""Text"", name=""text"", method=""text_response""),
    ]

    def text_response(self) -> Message:
        message = Message(
            text=self.input_value,
        )
        self.status = self.input_value
        return message
",TextOutput,outputs,Exibe uma saída de texto no Playground.
"from langflow.base.prompts.api_utils import process_prompt_template
from langflow.custom import Component
from langflow.io import Output, PromptInput
from langflow.schema.message import Message
from langflow.template.utils import update_template_values


class PromptComponent(Component):
    display_name: str = ""Prompt""
    description: str = ""Create a prompt template with dynamic variables.""
    icon = ""prompts""
    trace_type = ""prompt""
    name = ""Prompt""

    inputs = [
        PromptInput(name=""template"", display_name=""Template""),
    ]

    outputs = [
        Output(display_name=""Prompt Message"", name=""prompt"", method=""build_prompt""),
    ]

    async def build_prompt(
        self,
    ) -> Message:
        prompt = await Message.from_template_and_variables(**self._attributes)
        self.status = prompt.text
        return prompt

    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):
        """"""
        This function is called after the code validation is done.
        """"""
        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)
        template = frontend_node[""template""][""template""][""value""]
        _ = process_prompt_template(
            template=template,
            name=""template"",
            custom_fields=frontend_node[""custom_fields""],
            frontend_node_template=frontend_node[""template""],
        )
        # Now that template is updated, we need to grab any values that were set in the current_frontend_node
        # and update the frontend_node with those values
        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[""template""])
        return frontend_node
",Prompt,prompts,Cria um modelo de prompt com variáveis dinâmicas.
"from langflow.custom import Component
from langflow.io import BoolInput, DropdownInput, MessageInput, MessageTextInput, Output
from langflow.schema.message import Message


class ConditionalRouterComponent(Component):
    display_name = ""Conditional Router""
    description = ""Routes an input message to a corresponding output based on text comparison.""
    icon = ""equal""
    name = ""ConditionalRouter""

    inputs = [
        MessageTextInput(
            name=""input_text"",
            display_name=""Input Text"",
            info=""The primary text input for the operation."",
        ),
        MessageTextInput(
            name=""match_text"",
            display_name=""Match Text"",
            info=""The text input to compare against."",
        ),
        DropdownInput(
            name=""operator"",
            display_name=""Operator"",
            options=[""equals"", ""not equals"", ""contains"", ""starts with"", ""ends with""],
            info=""The operator to apply for comparing the texts."",
            value=""equals"",
            advanced=True,
        ),
        BoolInput(
            name=""case_sensitive"",
            display_name=""Case Sensitive"",
            info=""If true, the comparison will be case sensitive."",
            value=False,
            advanced=True,
        ),
        MessageInput(
            name=""message"",
            display_name=""Message"",
            info=""The message to pass through either route."",
        ),
    ]

    outputs = [
        Output(display_name=""True Route"", name=""true_result"", method=""true_response""),
        Output(display_name=""False Route"", name=""false_result"", method=""false_response""),
    ]

    def evaluate_condition(self, input_text: str, match_text: str, operator: str, case_sensitive: bool) -> bool:
        if not case_sensitive:
            input_text = input_text.lower()
            match_text = match_text.lower()

        if operator == ""equals"":
            return input_text == match_text
        elif operator == ""not equals"":
            return input_text != match_text
        elif operator == ""contains"":
            return match_text in input_text
        elif operator == ""starts with"":
            return input_text.startswith(match_text)
        elif operator == ""ends with"":
            return input_text.endswith(match_text)
        return False

    def true_response(self) -> Message:
        result = self.evaluate_condition(self.input_text, self.match_text, self.operator, self.case_sensitive)
        if result:
            self.status = self.message
            return self.message
        else:
            self.stop(""true_result"")
            return None  # type: ignore

    def false_response(self) -> Message:
        result = self.evaluate_condition(self.input_text, self.match_text, self.operator, self.case_sensitive)
        if not result:
            self.status = self.message
            return self.message
        else:
            self.stop(""false_result"")
            return None  # type: ignore
",ConditionalRouter,prototypes,Encaminha uma mensagem de entrada para uma saída correspondente com base na comparação de texto.
"from typing import Any

from langflow.custom import Component
from langflow.inputs.inputs import IntInput, MessageTextInput, DictInput
from langflow.io import Output

from langflow.field_typing.range_spec import RangeSpec
from langflow.schema import Data
from langflow.schema.dotdict import dotdict


class CreateDataComponent(Component):
    display_name: str = ""Create Data""
    description: str = ""Dynamically create a Data with a specified number of fields.""
    name: str = ""CreateData""

    inputs = [
        IntInput(
            name=""number_of_fields"",
            display_name=""Number of Fields"",
            info=""Number of fields to be added to the record."",
            real_time_refresh=True,
            range_spec=RangeSpec(min=1, max=15, step=1, step_type=""int""),
        ),
        MessageTextInput(name=""text_key"", display_name=""Text Key"", info=""Key to be used as text."", advanced=True),
    ]

    outputs = [
        Output(display_name=""Data"", name=""data"", method=""build_data""),
    ]

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        if field_name == ""number_of_fields"":
            default_keys = [""code"", ""_type"", ""number_of_fields"", ""text_key""]
            try:
                field_value_int = int(field_value)
            except ValueError:
                return build_config
            existing_fields = {}
            if field_value_int > 15:
                build_config[""number_of_fields""][""value""] = 15
                raise ValueError(""Number of fields cannot exceed 15. Try using a Component to combine two Data."")
            if len(build_config) > len(default_keys):
                # back up the existing template fields
                for key in build_config.copy():
                    if key not in default_keys:
                        existing_fields[key] = build_config.pop(key)

            for i in range(1, field_value_int + 1):
                key = f""field_{i}_key""
                if key in existing_fields:
                    field = existing_fields[key]
                    build_config[key] = field
                else:
                    field = DictInput(
                        display_name=f""Field {i}"",
                        name=key,
                        info=f""Key for field {i}."",
                        input_types=[""Text"", ""Data""],
                    )
                    build_config[field.name] = field.to_dict()

            build_config[""number_of_fields""][""value""] = field_value_int
        return build_config

    async def build_data(self) -> Data:
        data = {}
        for value_dict in self._attributes.values():
            if isinstance(value_dict, dict):
                # Check if the value of the value_dict is a Data
                value_dict = {
                    key: value.get_text() if isinstance(value, Data) else value for key, value in value_dict.items()
                }
                data.update(value_dict)
        return_data = Data(data=data, text_key=self.text_key)
        self.status = return_data
        return return_data

    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):
        """"""
        This function is called after the code validation is done.
        """"""
        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)
        frontend_node[""template""] = self.update_build_config(
            frontend_node[""template""], frontend_node[""template""][""number_of_fields""][""value""], ""number_of_fields""
        )
        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)
        return frontend_node
",CreateData,prototypes,Cria dinamicamente um Data com um número especificado de campos.
"from typing import Any, List, Optional

from loguru import logger

from langflow.base.tools.flow_tool import FlowTool
from langflow.custom import CustomComponent
from langflow.field_typing import Tool
from langflow.graph.graph.base import Graph
from langflow.helpers.flow import get_flow_inputs
from langflow.schema import Data
from langflow.schema.dotdict import dotdict


class FlowToolComponent(CustomComponent):
    display_name = ""Flow as Tool""
    description = ""Construct a Tool from a function that runs the loaded Flow.""
    field_order = [""flow_name"", ""name"", ""description"", ""return_direct""]
    trace_type = ""tool""
    name = ""FlowTool""
    beta = True

    def get_flow_names(self) -> List[str]:
        flow_datas = self.list_flows()
        return [flow_data.data[""name""] for flow_data in flow_datas]

    def get_flow(self, flow_name: str) -> Optional[Data]:
        """"""
        Retrieves a flow by its name.

        Args:
            flow_name (str): The name of the flow to retrieve.

        Returns:
            Optional[Text]: The flow record if found, None otherwise.
        """"""
        flow_datas = self.list_flows()
        for flow_data in flow_datas:
            if flow_data.data[""name""] == flow_name:
                return flow_data
        return None

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        logger.debug(f""Updating build config with field value {field_value} and field name {field_name}"")
        if field_name == ""flow_name"":
            build_config[""flow_name""][""options""] = self.get_flow_names()

        return build_config

    def build_config(self):
        return {
            ""flow_name"": {
                ""display_name"": ""Flow Name"",
                ""info"": ""The name of the flow to run."",
                ""options"": [],
                ""real_time_refresh"": True,
                ""refresh_button"": True,
            },
            ""name"": {
                ""display_name"": ""Name"",
                ""description"": ""The name of the tool."",
            },
            ""description"": {
                ""display_name"": ""Description"",
                ""description"": ""The description of the tool."",
            },
            ""return_direct"": {
                ""display_name"": ""Return Direct"",
                ""description"": ""Return the result directly from the Tool."",
                ""advanced"": True,
            },
        }

    async def build(self, flow_name: str, name: str, description: str, return_direct: bool = False) -> Tool:
        FlowTool.update_forward_refs()
        flow_data = self.get_flow(flow_name)
        if not flow_data:
            raise ValueError(""Flow not found."")
        graph = Graph.from_payload(flow_data.data[""data""])
        inputs = get_flow_inputs(graph)
        tool = FlowTool(
            name=name,
            description=description,
            graph=graph,
            return_direct=return_direct,
            inputs=inputs,
            flow_id=str(flow_data.id),
            user_id=str(self._user_id),
        )
        description_repr = repr(tool.description).strip(""'"")
        args_str = ""\n"".join([f""- {arg_name}: {arg_data['description']}"" for arg_name, arg_data in tool.args.items()])
        self.status = f""{description_repr}\nArguments:\n{args_str}""
        return tool  # type: ignore
",FlowTool,prototypes,Constrói uma Ferramenta a partir de uma função que executa o Flow carregado.
"import json
import re
import unicodedata
from langflow.custom import Component
from langflow.inputs import MessageTextInput, BoolInput
from langflow.template import Output
from langflow.schema.message import Message


class JSONCleaner(Component):
    display_name = ""JSON Cleaner""
    description = ""Cleans the messy and sometimes incorrect JSON strings produced by LLMs so that they are fully compliant with the JSON spec.""
    icon = ""custom_components""

    inputs = [
        MessageTextInput(
            name=""json_str"", display_name=""JSON String"", info=""The JSON string to be cleaned."", required=True
        ),
        BoolInput(
            name=""remove_control_chars"",
            display_name=""Remove Control Characters"",
            info=""Remove control characters from the JSON string."",
            required=False,
        ),
        BoolInput(
            name=""normalize_unicode"",
            display_name=""Normalize Unicode"",
            info=""Normalize Unicode characters in the JSON string."",
            required=False,
        ),
        BoolInput(
            name=""validate_json"",
            display_name=""Validate JSON"",
            info=""Validate the JSON string to ensure it is well-formed."",
            required=False,
        ),
    ]

    outputs = [
        Output(display_name=""Cleaned JSON String"", name=""output"", method=""clean_json""),
    ]

    def clean_json(self) -> Message:
        try:
            from json_repair import repair_json  # type: ignore
        except ImportError:
            raise ImportError(
                ""Could not import the json_repair package."" ""Please install it with `pip install json_repair`.""
            )

        """"""Clean the input JSON string based on provided options and return the cleaned JSON string.""""""
        json_str = self.json_str
        remove_control_chars = self.remove_control_chars
        normalize_unicode = self.normalize_unicode
        validate_json = self.validate_json

        try:
            start = json_str.find(""{"")
            end = json_str.rfind(""}"")
            if start == -1 or end == -1:
                raise ValueError(""Invalid JSON string: Missing '{' or '}'"")
            json_str = json_str[start : end + 1]

            if remove_control_chars:
                json_str = self._remove_control_characters(json_str)
            if normalize_unicode:
                json_str = self._normalize_unicode(json_str)
            if validate_json:
                json_str = self._validate_json(json_str)

            cleaned_json_str = repair_json(json_str)
            result = str(cleaned_json_str)

            self.status = result
            return Message(text=result)
        except Exception as e:
            raise ValueError(f""Error cleaning JSON string: {str(e)}"")

    def _remove_control_characters(self, s: str) -> str:
        """"""Remove control characters from the string.""""""
        return re.sub(r""[\x00-\x1F\x7F]"", """", s)

    def _normalize_unicode(self, s: str) -> str:
        """"""Normalize Unicode characters in the string.""""""
        return unicodedata.normalize(""NFC"", s)

    def _validate_json(self, s: str) -> str:
        """"""Validate the JSON string.""""""
        try:
            json.loads(s)
            return s
        except json.JSONDecodeError as e:
            raise ValueError(f""Invalid JSON string: {str(e)}"")
",JSONCleaner,prototypes,Limpa as strings JSON confusas e às vezes incorretas produzidas pelos LLMs para que elas estejam totalmente de acordo com a especificação JSON.
"from langflow.custom import CustomComponent
from langflow.schema import Data


class ListenComponent(CustomComponent):
    display_name = ""Listen""
    description = ""A component to listen for a notification.""
    name = ""Listen""
    beta: bool = True

    def build_config(self):
        return {
            ""name"": {
                ""display_name"": ""Name"",
                ""info"": ""The name of the notification to listen for."",
            },
        }

    def build(self, name: str) -> Data:
        state = self.get_state(name)
        self._set_successors_ids()
        self.status = state
        return state

    def _set_successors_ids(self):
        self.vertex.is_state = True
        successors = self.vertex.graph.successor_map.get(self.vertex.id, [])
        return successors + self.vertex.graph.activated_vertices
",Listen,prototypes,Um componente para ouvir uma notificação.
"from typing import Optional

from langflow.custom import CustomComponent
from langflow.schema import Data


class NotifyComponent(CustomComponent):
    display_name = ""Notify""
    description = ""A component to generate a notification to Get Notified component.""
    icon = ""Notify""
    name = ""Notify""
    beta: bool = True

    def build_config(self):
        return {
            ""name"": {""display_name"": ""Name"", ""info"": ""The name of the notification.""},
            ""data"": {""display_name"": ""Data"", ""info"": ""The data to store.""},
            ""append"": {
                ""display_name"": ""Append"",
                ""info"": ""If True, the record will be appended to the notification."",
            },
        }

    def build(self, name: str, data: Optional[Data] = None, append: bool = False) -> Data:
        if data and not isinstance(data, Data):
            if isinstance(data, str):
                data = Data(text=data)
            elif isinstance(data, dict):
                data = Data(data=data)
            else:
                data = Data(text=str(data))
        elif not data:
            data = Data(text="""")
        if data:
            if append:
                self.append_state(name, data)
            else:
                self.update_state(name, data)
        else:
            self.status = ""No record provided.""
        self.status = data
        self._set_successors_ids()
        return data

    def _set_successors_ids(self):
        self.vertex.is_state = True
        successors = self.vertex.graph.successor_map.get(self.vertex.id, [])
        return successors + self.vertex.graph.activated_vertices
",Notify,prototypes,Um componente para gerar uma notificação para o componente Get Notified.
"from langflow.custom import Component
from langflow.io import MessageInput
from langflow.schema.message import Message
from langflow.template import Output


class PassMessageComponent(Component):
    display_name = ""Pass""
    description = ""Forwards the input message, unchanged.""
    name = ""Pass""
    icon = ""arrow-right""

    inputs = [
        MessageInput(
            name=""input_message"",
            display_name=""Input Message"",
            info=""The message to be passed forward."",
        ),
        MessageInput(
            name=""ignored_message"",
            display_name=""Ignored Message"",
            info=""A second message to be ignored. Used as a workaround for continuity."",
            advanced=True,
        ),
    ]

    outputs = [
        Output(display_name=""Output Message"", name=""output_message"", method=""pass_message""),
    ]

    def pass_message(self) -> Message:
        self.status = self.input_message
        return self.input_message
",Pass,prototypes,"Encaminha a mensagem de entrada, inalterada."
"from typing import Callable

from langflow.custom import CustomComponent
from langflow.custom.utils import get_function
from langflow.field_typing import Code


class PythonFunctionComponent(CustomComponent):
    display_name = ""Python Function""
    description = ""Define a Python function.""
    icon = ""Python""
    name = ""PythonFunction""
    beta = True

    def build_config(self):
        return {
            ""function_code"": {
                ""display_name"": ""Code"",
                ""info"": ""The code for the function."",
                ""show"": True,
            },
        }

    def build(self, function_code: Code) -> Callable:
        self.status = function_code
        func = get_function(function_code)
        return func
",PythonFunction,prototypes,Define uma função Python.
"from typing import Any, List, Optional

from langflow.base.flow_processing.utils import build_data_from_run_outputs
from langflow.custom import Component
from langflow.graph.schema import RunOutputs
from langflow.io import DropdownInput, MessageTextInput, NestedDictInput, Output
from langflow.schema import Data, dotdict


class RunFlowComponent(Component):
    display_name = ""Run Flow""
    description = ""A component to run a flow.""
    name = ""RunFlow""
    beta: bool = True

    def get_flow_names(self) -> List[str]:
        flow_data = self.list_flows()
        return [flow_data.data[""name""] for flow_data in flow_data]

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        if field_name == ""flow_name"":
            build_config[""flow_name""][""options""] = self.get_flow_names()

        return build_config

    inputs = [
        MessageTextInput(
            name=""input_value"",
            display_name=""Input Value"",
            info=""The input value to be processed by the flow."",
        ),
        DropdownInput(
            name=""flow_name"",
            display_name=""Flow Name"",
            info=""The name of the flow to run."",
            options=[],
            refresh_button=True,
        ),
        NestedDictInput(
            name=""tweaks"",
            display_name=""Tweaks"",
            info=""Tweaks to apply to the flow."",
        ),
    ]

    outputs = [
        Output(display_name=""Run Outputs"", name=""run_outputs"", method=""generate_results""),
    ]

    async def generate_results(self) -> List[Data]:
        results: List[Optional[RunOutputs]] = await self.run_flow(
            inputs={""input_value"": self.input_value}, flow_name=self.flow_name, tweaks=self.tweaks
        )
        if isinstance(results, list):
            data = []
            for result in results:
                if result:
                    data.extend(build_data_from_run_outputs(result))
        else:
            data = build_data_from_run_outputs()(results)

        self.status = data
        return data
",RunFlow,prototypes,Um componente para executar um fluxo.
"from langflow.custom import Component
from langflow.inputs import HandleInput, MessageTextInput
from langflow.schema.message import Message
from langflow.template import Output


class RunnableExecComponent(Component):
    description = ""Execute a runnable. It will try to guess the input and output keys.""
    display_name = ""Runnable Executor""
    name = ""RunnableExecutor""
    beta: bool = True

    inputs = [
        MessageTextInput(name=""input_value"", display_name=""Input"", required=True),
        HandleInput(
            name=""runnable"",
            display_name=""Agent Executor"",
            input_types=[""Chain"", ""AgentExecutor"", ""Agent"", ""Runnable""],
            required=True,
        ),
        MessageTextInput(
            name=""input_key"",
            display_name=""Input Key"",
            value=""input"",
            advanced=True,
        ),
        MessageTextInput(
            name=""output_key"",
            display_name=""Output Key"",
            value=""output"",
            advanced=True,
        ),
    ]

    outputs = [
        Output(
            display_name=""Text"",
            name=""text"",
            method=""build_executor"",
        ),
    ]

    def get_output(self, result, input_key, output_key):
        """"""
        Retrieves the output value from the given result dictionary based on the specified input and output keys.

        Args:
            result (dict): The result dictionary containing the output value.
            input_key (str): The key used to retrieve the input value from the result dictionary.
            output_key (str): The key used to retrieve the output value from the result dictionary.

        Returns:
            tuple: A tuple containing the output value and the status message.

        """"""
        possible_output_keys = [""answer"", ""response"", ""output"", ""result"", ""text""]
        status = """"
        result_value = None

        if output_key in result:
            result_value = result.get(output_key)
        elif len(result) == 2 and input_key in result:
            # get the other key from the result dict
            other_key = [k for k in result if k != input_key][0]
            if other_key == output_key:
                result_value = result.get(output_key)
            else:
                status += f""Warning: The output key is not '{output_key}'. The output key is '{other_key}'.""
                result_value = result.get(other_key)
        elif len(result) == 1:
            result_value = list(result.values())[0]
        elif any(k in result for k in possible_output_keys):
            for key in possible_output_keys:
                if key in result:
                    result_value = result.get(key)
                    status += f""Output key: '{key}'.""
                    break
            if result_value is None:
                result_value = result
                status += f""Warning: The output key is not '{output_key}'.""
        else:
            result_value = result
            status += f""Warning: The output key is not '{output_key}'.""

        return result_value, status

    def get_input_dict(self, runnable, input_key, input_value):
        """"""
        Returns a dictionary containing the input key-value pair for the given runnable.

        Args:
            runnable: The runnable object.
            input_key: The key for the input value.
            input_value: The value for the input key.

        Returns:
            input_dict: A dictionary containing the input key-value pair.
            status: A status message indicating if the input key is not in the runnable's input keys.
        """"""
        input_dict = {}
        status = """"
        if hasattr(runnable, ""input_keys""):
            # Check if input_key is in the runnable's input_keys
            if input_key in runnable.input_keys:
                input_dict[input_key] = input_value
            else:
                input_dict = {k: input_value for k in runnable.input_keys}
                status = f""Warning: The input key is not '{input_key}'. The input key is '{runnable.input_keys}'.""
        return input_dict, status

    def build_executor(self) -> Message:
        input_dict, status = self.get_input_dict(self.runnable, self.input_key, self.input_value)
        result = self.runnable.invoke(input_dict)
        result_value, _status = self.get_output(result, self.input_key, self.output_key)
        status += _status
        status += f""\n\nOutput: {result_value}\n\nRaw Output: {result}""
        self.status = status
        return result_value
",RunnableExecutor,prototypes,Executa um executável. Ele tentará adivinhar as chaves de entrada e saída.
"from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool
from langchain_community.utilities import SQLDatabase

from langflow.custom import CustomComponent
from langflow.field_typing import Text


class SQLExecutorComponent(CustomComponent):
    display_name = ""SQL Executor""
    description = ""Execute SQL query.""
    name = ""SQLExecutor""
    beta: bool = True

    def build_config(self):
        return {
            ""database_url"": {
                ""display_name"": ""Database URL"",
                ""info"": ""The URL of the database."",
            },
            ""include_columns"": {
                ""display_name"": ""Include Columns"",
                ""info"": ""Include columns in the result."",
            },
            ""passthrough"": {
                ""display_name"": ""Passthrough"",
                ""info"": ""If an error occurs, return the query instead of raising an exception."",
            },
            ""add_error"": {
                ""display_name"": ""Add Error"",
                ""info"": ""Add the error to the result."",
            },
        }

    def clean_up_uri(self, uri: str) -> str:
        if uri.startswith(""postgresql://""):
            uri = uri.replace(""postgresql://"", ""postgres://"")
        return uri.strip()

    def build(
        self,
        query: str,
        database_url: str,
        include_columns: bool = False,
        passthrough: bool = False,
        add_error: bool = False,
    ) -> Text:
        error = None
        try:
            database = SQLDatabase.from_uri(database_url)
        except Exception as e:
            raise ValueError(f""An error occurred while connecting to the database: {e}"")
        try:
            tool = QuerySQLDataBaseTool(db=database)
            result = tool.run(query, include_columns=include_columns)
            self.status = result
        except Exception as e:
            result = str(e)
            self.status = result
            if not passthrough:
                raise e
            error = repr(e)

        if add_error and error is not None:
            result = f""{result}\n\nError: {error}\n\nQuery: {query}""
        elif error is not None:
            # Then we won't add the error to the result
            # but since we are in passthrough mode, we will return the query
            result = query

        return result
",SQLExecutor,prototypes,Executar consulta SQL.
"from typing import Any, List, Optional

from langflow.base.flow_processing.utils import build_data_from_result_data
from langflow.custom import CustomComponent
from langflow.graph.graph.base import Graph
from langflow.graph.schema import RunOutputs
from langflow.graph.vertex.base import Vertex
from langflow.helpers.flow import get_flow_inputs
from langflow.schema import Data
from langflow.schema.dotdict import dotdict
from langflow.template.field.base import Input
from loguru import logger


class SubFlowComponent(CustomComponent):
    display_name = ""Sub Flow""
    description = (
        ""Dynamically Generates a Component from a Flow. The output is a list of data with keys 'result' and 'message'.""
    )
    name = ""SubFlow""
    beta: bool = True
    field_order = [""flow_name""]

    def get_flow_names(self) -> List[str]:
        flow_datas = self.list_flows()
        return [flow_data.data[""name""] for flow_data in flow_datas]

    def get_flow(self, flow_name: str) -> Optional[Data]:
        flow_datas = self.list_flows()
        for flow_data in flow_datas:
            if flow_data.data[""name""] == flow_name:
                return flow_data
        return None

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        logger.debug(f""Updating build config with field value {field_value} and field name {field_name}"")
        if field_name == ""flow_name"":
            build_config[""flow_name""][""options""] = self.get_flow_names()
        # Clean up the build config
        for key in list(build_config.keys()):
            if key not in self.field_order + [""code"", ""_type"", ""get_final_results_only""]:
                del build_config[key]
        if field_value is not None and field_name == ""flow_name"":
            try:
                flow_data = self.get_flow(field_value)
                if not flow_data:
                    raise ValueError(f""Flow {field_value} not found."")
                graph = Graph.from_payload(flow_data.data[""data""])
                # Get all inputs from the graph
                inputs = get_flow_inputs(graph)
                # Add inputs to the build config
                build_config = self.add_inputs_to_build_config(inputs, build_config)
            except Exception as e:
                logger.error(f""Error getting flow {field_value}: {str(e)}"")

        return build_config

    def add_inputs_to_build_config(self, inputs: List[Vertex], build_config: dotdict):
        new_fields: list[Input] = []
        for vertex in inputs:
            field = Input(
                display_name=vertex.display_name,
                name=vertex.id,
                info=vertex.description,
                field_type=""str"",
                value=None,
            )
            new_fields.append(field)
        logger.debug(new_fields)
        for field in new_fields:
            build_config[field.name] = field.to_dict()
        return build_config

    def build_config(self):
        return {
            ""input_value"": {
                ""display_name"": ""Input Value"",
                ""multiline"": True,
            },
            ""flow_name"": {
                ""display_name"": ""Flow Name"",
                ""info"": ""The name of the flow to run."",
                ""options"": [],
                ""real_time_refresh"": True,
                ""refresh_button"": True,
            },
            ""tweaks"": {
                ""display_name"": ""Tweaks"",
                ""info"": ""Tweaks to apply to the flow."",
            },
            ""get_final_results_only"": {
                ""display_name"": ""Get Final Results Only"",
                ""info"": ""If False, the output will contain all outputs from the flow."",
                ""advanced"": True,
            },
        }

    async def build(self, flow_name: str, get_final_results_only: bool = True, **kwargs) -> List[Data]:
        tweaks = {key: {""input_value"": value} for key, value in kwargs.items()}
        run_outputs: List[Optional[RunOutputs]] = await self.run_flow(
            tweaks=tweaks,
            flow_name=flow_name,
        )
        if not run_outputs:
            return []
        run_output = run_outputs[0]

        data = []
        if run_output is not None:
            for output in run_output.outputs:
                if output:
                    data.extend(build_data_from_result_data(output, get_final_results_only))

        self.status = data
        logger.debug(data)
        return data
",SubFlow,prototypes,Gera dinamicamente um Componente a partir de um Flow. A saída é uma lista de dados com as chaves 'result' e 'message'.
"from langflow.custom import CustomComponent
from langflow.schema import Data


class UpdateDataComponent(CustomComponent):
    display_name = ""Update Data""
    description = ""Update Data with text-based key/value pairs, similar to updating a Python dictionary.""
    name = ""UpdateData""

    def build_config(self):
        return {
            ""data"": {
                ""display_name"": ""Data"",
                ""info"": ""The record to update."",
            },
            ""new_data"": {
                ""display_name"": ""New Data"",
                ""info"": ""The new data to update the record with."",
                ""input_types"": [""Text""],
            },
        }

    def build(
        self,
        data: Data,
        new_data: dict,
    ) -> Data:
        """"""
        Updates a record with new data.

        Args:
            record (Data): The record to update.
            new_data (dict): The new data to update the record with.

        Returns:
            Data: The updated record.
        """"""
        data.data.update(new_data)
        self.status = data
        return data
",UpdateData,prototypes,"Atualiza o Data com pares chave/valor baseados em texto, semelhante a atualizar um dicionário Python."
"from typing import Optional, cast

from langchain_community.retrievers import AmazonKendraRetriever

from langflow.custom import CustomComponent
from langflow.field_typing import Retriever


class AmazonKendraRetrieverComponent(CustomComponent):
    display_name: str = ""Amazon Kendra Retriever""
    description: str = ""Retriever that uses the Amazon Kendra API.""
    name = ""AmazonKendra""
    icon = ""Amazon""

    def build_config(self):
        return {
            ""index_id"": {""display_name"": ""Index ID""},
            ""region_name"": {""display_name"": ""Region Name""},
            ""credentials_profile_name"": {""display_name"": ""Credentials Profile Name""},
            ""attribute_filter"": {
                ""display_name"": ""Attribute Filter"",
                ""field_type"": ""code"",
            },
            ""top_k"": {""display_name"": ""Top K"", ""field_type"": ""int""},
            ""user_context"": {
                ""display_name"": ""User Context"",
                ""field_type"": ""code"",
            },
            ""code"": {""show"": False},
        }

    def build(
        self,
        index_id: str,
        top_k: int = 3,
        region_name: Optional[str] = None,
        credentials_profile_name: Optional[str] = None,
        attribute_filter: Optional[dict] = None,
        user_context: Optional[dict] = None,
    ) -> Retriever:  # type: ignore[type-var]
        try:
            output = AmazonKendraRetriever(
                index_id=index_id,
                top_k=top_k,
                region_name=region_name,
                credentials_profile_name=credentials_profile_name,
                attribute_filter=attribute_filter,
                user_context=user_context,
            )  # type: ignore
        except Exception as e:
            raise ValueError(""Could not connect to AmazonKendra API."") from e
        return cast(Retriever, output)
",AmazonKendra,retrievers,Recuperador que usa a API da Amazon Kendra.
"from typing import List, cast

from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.field_typing import Retriever
from langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, MultilineInput, SecretStrInput
from langflow.schema import Data


class CohereRerankComponent(LCVectorStoreComponent):
    display_name = ""Cohere Rerank""
    description = ""Rerank documents using the Cohere API and a retriever.""
    name = ""CohereRerank""
    icon = ""Cohere""

    inputs = [
        MultilineInput(
            name=""search_query"",
            display_name=""Search Query"",
        ),
        DropdownInput(
            name=""model"",
            display_name=""Model"",
            options=[
                ""rerank-english-v3.0"",
                ""rerank-multilingual-v3.0"",
                ""rerank-english-v2.0"",
                ""rerank-multilingual-v2.0"",
            ],
            value=""rerank-english-v3.0"",
        ),
        SecretStrInput(name=""api_key"", display_name=""API Key""),
        IntInput(name=""top_n"", display_name=""Top N"", value=3),
        MessageTextInput(name=""user_agent"", display_name=""User Agent"", value=""langflow"", advanced=True),
        HandleInput(name=""retriever"", display_name=""Retriever"", input_types=[""Retriever""]),
    ]

    def build_base_retriever(self) -> Retriever:  # type: ignore[type-var]
        cohere_reranker = CohereRerank(
            cohere_api_key=self.api_key, model=self.model, top_n=self.top_n, user_agent=self.user_agent
        )
        retriever = ContextualCompressionRetriever(base_compressor=cohere_reranker, base_retriever=self.retriever)
        return cast(Retriever, retriever)

    async def search_documents(self) -> List[Data]:  # type: ignore
        retriever = self.build_base_retriever()
        documents = await retriever.ainvoke(self.search_query)
        data = self.to_data(documents)
        self.status = data
        return data
",CohereRerank,retrievers,Reordena documentos usando a API da Cohere e um recuperador.
"from typing import Optional, cast

from langchain_community.retrievers import MetalRetriever
from metal_sdk.metal import Metal  # type: ignore

from langflow.custom import CustomComponent
from langflow.field_typing import Retriever


class MetalRetrieverComponent(CustomComponent):
    display_name: str = ""Metal Retriever""
    description: str = ""Retriever that uses the Metal API.""
    name = ""MetalRetriever""

    def build_config(self):
        return {
            ""api_key"": {""display_name"": ""API Key"", ""password"": True},
            ""client_id"": {""display_name"": ""Client ID"", ""password"": True},
            ""index_id"": {""display_name"": ""Index ID""},
            ""params"": {""display_name"": ""Parameters""},
            ""code"": {""show"": False},
        }

    def build(self, api_key: str, client_id: str, index_id: str, params: Optional[dict] = None) -> Retriever:  # type: ignore[type-var]
        try:
            metal = Metal(api_key=api_key, client_id=client_id, index_id=index_id)
        except Exception as e:
            raise ValueError(""Could not connect to Metal API."") from e
        return cast(Retriever, MetalRetriever(client=metal, params=params or {}))
",MetalRetriever,retrievers,Recuperador que usa a API do Metal.
"from typing import Optional

from langchain.retrievers import MultiQueryRetriever

from langflow.custom import CustomComponent
from langflow.field_typing import BaseRetriever, LanguageModel, PromptTemplate, Text


class MultiQueryRetrieverComponent(CustomComponent):
    display_name = ""MultiQueryRetriever""
    description = ""Initialize from llm using default template.""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/MultiQueryRetriever""
    name = ""MultiQueryRetriever""

    def build_config(self):
        return {
            ""llm"": {""display_name"": ""LLM""},
            ""prompt"": {
                ""display_name"": ""Prompt"",
                ""default"": {
                    ""input_variables"": [""question""],
                    ""input_types"": {},
                    ""output_parser"": None,
                    ""partial_variables"": {},
                    ""template"": ""You are an AI language model assistant. Your task is \n""
                    ""to generate 3 different versions of the given user \n""
                    ""question to retrieve relevant documents from a vector database. \n""
                    ""By generating multiple perspectives on the user question, \n""
                    ""your goal is to help the user overcome some of the limitations \n""
                    ""of distance-based similarity search. Provide these alternative \n""
                    ""questions separated by newlines. Original question: {question}"",
                    ""template_format"": ""f-string"",
                    ""validate_template"": False,
                    ""_type"": ""prompt"",
                },
            },
            ""retriever"": {""display_name"": ""Retriever""},
            ""parser_key"": {""display_name"": ""Parser Key"", ""default"": ""lines""},
        }

    def build(
        self,
        llm: LanguageModel,
        retriever: BaseRetriever,
        prompt: Optional[Text] = None,
        parser_key: str = ""lines"",
    ) -> MultiQueryRetriever:
        if not prompt:
            return MultiQueryRetriever.from_llm(llm=llm, retriever=retriever, parser_key=parser_key)
        else:
            prompt_template = PromptTemplate.from_template(prompt)
            return MultiQueryRetriever.from_llm(
                llm=llm, retriever=retriever, prompt=prompt_template, parser_key=parser_key
            )
",MultiQueryRetriever,retrievers,Inicializar a partir de llm usando o modelo padrão.
"from typing import Any, List, cast

from langchain.retrievers import ContextualCompressionRetriever

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.field_typing import Retriever
from langflow.io import DropdownInput, HandleInput, MultilineInput, SecretStrInput, StrInput
from langflow.schema import Data
from langflow.schema.dotdict import dotdict


class NvidiaRerankComponent(LCVectorStoreComponent):
    display_name = ""NVIDIA Rerank""
    description = ""Rerank documents using the NVIDIA API and a retriever.""
    icon = ""NVIDIA""

    inputs = [
        MultilineInput(
            name=""search_query"",
            display_name=""Search Query"",
        ),
        StrInput(
            name=""base_url"",
            display_name=""Base URL"",
            value=""https://integrate.api.nvidia.com/v1"",
            refresh_button=True,
            info=""The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1."",
        ),
        DropdownInput(
            name=""model"", display_name=""Model"", options=[""nv-rerank-qa-mistral-4b:1""], value=""nv-rerank-qa-mistral-4b:1""
        ),
        SecretStrInput(name=""api_key"", display_name=""API Key""),
        HandleInput(name=""retriever"", display_name=""Retriever"", input_types=[""Retriever""]),
    ]

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):
        if field_name == ""base_url"" and field_value:
            try:
                build_model = self.build_model()
                ids = [model.id for model in build_model.available_models]
                build_config[""model""][""options""] = ids
                build_config[""model""][""value""] = ids[0]
            except Exception as e:
                raise ValueError(f""Error getting model names: {e}"")
        return build_config

    def build_model(self):
        try:
            from langchain_nvidia_ai_endpoints import NVIDIARerank
        except ImportError:
            raise ImportError(""Please install langchain-nvidia-ai-endpoints to use the NVIDIA model."")
        return NVIDIARerank(api_key=self.api_key, model=self.model, base_url=self.base_url)

    def build_base_retriever(self) -> Retriever:  # type: ignore[type-var]
        nvidia_reranker = self.build_model()
        retriever = ContextualCompressionRetriever(base_compressor=nvidia_reranker, base_retriever=self.retriever)
        return cast(Retriever, retriever)

    async def search_documents(self) -> List[Data]:  # type: ignore
        retriever = self.build_base_retriever()
        documents = await retriever.ainvoke(self.search_query)
        data = self.to_data(documents)
        self.status = data
        return data
",NvidiaRerank,retrievers,Reordena documentos usando a API da NVIDIA e um recuperador.
"# from langflow.field_typing import Data
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain_core.vectorstores import VectorStore

from langflow.custom import CustomComponent
from langflow.field_typing import LanguageModel, Text
from langflow.schema import Data
from langflow.schema.message import Message


class SelfQueryRetrieverComponent(CustomComponent):
    display_name: str = ""Self Query Retriever""
    description: str = ""Retriever that uses a vector store and an LLM to generate the vector store queries.""
    name = ""SelfQueryRetriever""
    icon = ""LangChain""

    def build_config(self):
        return {
            ""query"": {
                ""display_name"": ""Query"",
                ""input_types"": [""Message"", ""Text""],
                ""info"": ""Query to be passed as input."",
            },
            ""vectorstore"": {
                ""display_name"": ""Vector Store"",
                ""info"": ""Vector Store to be passed as input."",
            },
            ""attribute_infos"": {
                ""display_name"": ""Metadata Field Info"",
                ""info"": ""Metadata Field Info to be passed as input."",
            },
            ""document_content_description"": {
                ""display_name"": ""Document Content Description"",
                ""info"": ""Document Content Description to be passed as input."",
            },
            ""llm"": {
                ""display_name"": ""LLM"",
                ""info"": ""LLM to be passed as input."",
            },
        }

    def build(
        self,
        query: Message,
        vectorstore: VectorStore,
        attribute_infos: list[Data],
        document_content_description: Text,
        llm: LanguageModel,
    ) -> Data:
        metadata_field_infos = [AttributeInfo(**value.data) for value in attribute_infos]
        self_query_retriever = SelfQueryRetriever.from_llm(
            llm=llm,
            vectorstore=vectorstore,
            document_contents=document_content_description,
            metadata_field_info=metadata_field_infos,
            enable_limit=True,
        )

        if isinstance(query, Message):
            input_text = query.text
        elif isinstance(query, str):
            input_text = query

        if not isinstance(query, str):
            raise ValueError(f""Query type {type(query)} not supported."")
        documents = self_query_retriever.invoke(input=input_text)
        data = [Data.from_document(document) for document in documents]
        self.status = data
        return data
",SelfQueryRetriever,retrievers,Recuperador que usa um vector store e um LLM para gerar as consultas do vector store.
"import json
from typing import List, cast

from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain_core.vectorstores import VectorStore

from langflow.custom import CustomComponent
from langflow.field_typing import Retriever
from langflow.field_typing.constants import LanguageModel


class VectaraSelfQueryRetriverComponent(CustomComponent):
    """"""
    A custom component for implementing Vectara Self Query Retriever using a vector store.
    """"""

    display_name: str = ""Vectara Self Query Retriever for Vectara Vector Store""
    description: str = ""Implementation of Vectara Self Query Retriever""
    documentation = ""https://python.langchain.com/docs/integrations/retrievers/self_query/vectara_self_query""
    name = ""VectaraSelfQueryRetriver""
    icon = ""Vectara""

    field_config = {
        ""code"": {""show"": True},
        ""vectorstore"": {""display_name"": ""Vector Store"", ""info"": ""Input Vectara Vectore Store""},
        ""llm"": {""display_name"": ""LLM"", ""info"": ""For self query retriever""},
        ""document_content_description"": {
            ""display_name"": ""Document Content Description"",
            ""info"": ""For self query retriever"",
        },
        ""metadata_field_info"": {
            ""display_name"": ""Metadata Field Info"",
            ""info"": 'Each metadata field info is a string in the form of key value pair dictionary containing additional search metadata.\nExample input: {""name"":""speech"",""description"":""what name of the speech"",""type"":""string or list[string]""}.\nThe keys should remain constant(name, description, type)',
        },
    }

    def build(
        self,
        vectorstore: VectorStore,
        document_content_description: str,
        llm: LanguageModel,
        metadata_field_info: List[str],
    ) -> Retriever:  # type: ignore
        metadata_field_obj = []

        for meta in metadata_field_info:
            meta_obj = json.loads(meta)
            if ""name"" not in meta_obj or ""description"" not in meta_obj or ""type"" not in meta_obj:
                raise Exception(""Incorrect metadata field info format."")
            attribute_info = AttributeInfo(
                name=meta_obj[""name""],
                description=meta_obj[""description""],
                type=meta_obj[""type""],
            )
            metadata_field_obj.append(attribute_info)

        return cast(
            Retriever,
            SelfQueryRetriever.from_llm(
                llm, vectorstore, document_content_description, metadata_field_obj, verbose=True
            ),
        )
",VectaraSelfQueryRetriver,retrievers,Implementação do Recuperador de Autoconsula de Vectara
"from langchain_core.vectorstores import VectorStoreRetriever

from langflow.custom import CustomComponent
from langflow.field_typing import VectorStore


class VectoStoreRetrieverComponent(CustomComponent):
    display_name = ""VectorStore Retriever""
    description = ""A vector store retriever""
    name = ""VectorStoreRetriever""

    def build_config(self):
        return {
            ""vectorstore"": {""display_name"": ""Vector Store"", ""type"": VectorStore},
        }

    def build(self, vectorstore: VectorStore) -> VectorStoreRetriever:
        return vectorstore.as_retriever()
",VectorStoreRetriever,retrievers,Um recuperador de vector store
"from typing import Any

from langchain_text_splitters import CharacterTextSplitter, TextSplitter

from langflow.base.textsplitters.model import LCTextSplitterComponent
from langflow.inputs import DataInput, IntInput, MessageTextInput
from langflow.utils.util import unescape_string


class CharacterTextSplitterComponent(LCTextSplitterComponent):
    display_name = ""CharacterTextSplitter""
    description = ""Split text by number of characters.""
    documentation = ""https://docs.langflow.org/components/text-splitters#charactertextsplitter""
    name = ""CharacterTextSplitter""

    inputs = [
        IntInput(
            name=""chunk_size"",
            display_name=""Chunk Size"",
            info=""The maximum length of each chunk."",
            value=1000,
        ),
        IntInput(
            name=""chunk_overlap"",
            display_name=""Chunk Overlap"",
            info=""The amount of overlap between chunks."",
            value=200,
        ),
        DataInput(
            name=""data_input"",
            display_name=""Input"",
            info=""The texts to split."",
            input_types=[""Document"", ""Data""],
        ),
        MessageTextInput(
            name=""separator"",
            display_name=""Separator"",
            info='The characters to split on.\nIf left empty defaults to ""\\n\\n"".',
        ),
    ]

    def get_data_input(self) -> Any:
        return self.data_input

    def build_text_splitter(self) -> TextSplitter:
        if self.separator:
            separator = unescape_string(self.separator)
        else:
            separator = ""\n\n""
        return CharacterTextSplitter(
            chunk_overlap=self.chunk_overlap,
            chunk_size=self.chunk_size,
            separator=separator,
        )
",CharacterTextSplitter,textsplitters,Dividir texto por número de caracteres.
"from typing import Any

from langchain_text_splitters import Language, RecursiveCharacterTextSplitter, TextSplitter

from langflow.base.textsplitters.model import LCTextSplitterComponent
from langflow.inputs import IntInput, DataInput, DropdownInput


class LanguageRecursiveTextSplitterComponent(LCTextSplitterComponent):
    display_name: str = ""Language Recursive Text Splitter""
    description: str = ""Split text into chunks of a specified length based on language.""
    documentation: str = ""https://docs.langflow.org/components/text-splitters#languagerecursivetextsplitter""
    name = ""LanguageRecursiveTextSplitter""

    inputs = [
        IntInput(
            name=""chunk_size"",
            display_name=""Chunk Size"",
            info=""The maximum length of each chunk."",
            value=1000,
        ),
        IntInput(
            name=""chunk_overlap"",
            display_name=""Chunk Overlap"",
            info=""The amount of overlap between chunks."",
            value=200,
        ),
        DataInput(
            name=""data_input"",
            display_name=""Input"",
            info=""The texts to split."",
            input_types=[""Document"", ""Data""],
        ),
        DropdownInput(
            name=""code_language"", display_name=""Code Language"", options=[x.value for x in Language], value=""python""
        ),
    ]

    def get_data_input(self) -> Any:
        return self.data_input

    def build_text_splitter(self) -> TextSplitter:
        return RecursiveCharacterTextSplitter.from_language(
            language=Language(self.code_language),
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
        )
",LanguageRecursiveTextSplitter,textsplitters,Dividir o texto em chunks de comprimento especificado com base no idioma.
"from typing import Any
from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter
from langflow.base.textsplitters.model import LCTextSplitterComponent
from langflow.inputs.inputs import DataInput, IntInput, MessageTextInput
from langflow.utils.util import unescape_string


class RecursiveCharacterTextSplitterComponent(LCTextSplitterComponent):
    display_name: str = ""Recursive Character Text Splitter""
    description: str = ""Split text trying to keep all related text together.""
    documentation: str = ""https://docs.langflow.org/components/text-splitters#recursivecharactertextsplitter""
    name = ""RecursiveCharacterTextSplitter""

    inputs = [
        IntInput(
            name=""chunk_size"",
            display_name=""Chunk Size"",
            info=""The maximum length of each chunk."",
            value=1000,
        ),
        IntInput(
            name=""chunk_overlap"",
            display_name=""Chunk Overlap"",
            info=""The amount of overlap between chunks."",
            value=200,
        ),
        DataInput(
            name=""data_input"",
            display_name=""Input"",
            info=""The texts to split."",
            input_types=[""Document"", ""Data""],
        ),
        MessageTextInput(
            name=""separators"",
            display_name=""Separators"",
            info='The characters to split on.\nIf left empty defaults to [""\\n\\n"", ""\\n"", "" "", """"].',
            is_list=True,
        ),
    ]

    def get_data_input(self) -> Any:
        return self.data_input

    def build_text_splitter(self) -> TextSplitter:
        if not self.separators:
            separators: list[str] | None = None
        else:
            # check if the separators list has escaped characters
            # if there are escaped characters, unescape them
            separators = [unescape_string(x) for x in self.separators]

        return RecursiveCharacterTextSplitter(
            separators=separators,
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
        )
",RecursiveCharacterTextSplitter,textsplitters,Dividir o texto tentando manter todo o texto relacionado junto.
"from typing import List, Union

from langchain_community.agent_toolkits.base import BaseToolkit
from langchain_core.tools import Tool, tool
from metaphor_python import Metaphor  # type: ignore

from langflow.custom import CustomComponent


class MetaphorToolkit(CustomComponent):
    display_name: str = ""Metaphor""
    description: str = ""Metaphor Toolkit""
    documentation = ""https://python.langchain.com/docs/integrations/tools/metaphor_search""
    beta: bool = True
    name = ""Metaphor""
    # api key should be password = True
    field_config = {
        ""metaphor_api_key"": {""display_name"": ""Metaphor API Key"", ""password"": True},
        ""code"": {""advanced"": True},
    }

    def build(
        self,
        metaphor_api_key: str,
        use_autoprompt: bool = True,
        search_num_results: int = 5,
        similar_num_results: int = 5,
    ) -> Union[Tool, BaseToolkit]:
        # If documents, then we need to create a Vectara instance using .from_documents
        client = Metaphor(api_key=metaphor_api_key)

        @tool
        def search(query: str):
            """"""Call search engine with a query.""""""
            return client.search(query, use_autoprompt=use_autoprompt, num_results=search_num_results)

        @tool
        def get_contents(ids: List[str]):
            """"""Get contents of a webpage.

            The ids passed in should be a list of ids as fetched from `search`.
            """"""
            return client.get_contents(ids)

        @tool
        def find_similar(url: str):
            """"""Get search results similar to a given URL.

            The url passed in should be a URL returned from `search`
            """"""
            return client.find_similar(url, num_results=similar_num_results)

        return [search, get_contents, find_similar]  # type: ignore
",Metaphor,toolkits,Ferramenta Metaphor
"from langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreInfo
from langflow.custom import Component
from langflow.inputs import HandleInput, MultilineInput, MessageTextInput
from langflow.template import Output


class VectorStoreInfoComponent(Component):
    display_name = ""VectorStoreInfo""
    description = ""Information about a VectorStore""
    name = ""VectorStoreInfo""

    inputs = [
        MessageTextInput(
            name=""vectorstore_name"",
            display_name=""Name"",
            info=""Name of the VectorStore"",
            required=True,
        ),
        MultilineInput(
            name=""vectorstore_description"",
            display_name=""Description"",
            info=""Description of the VectorStore"",
            required=True,
        ),
        HandleInput(
            name=""input_vectorstore"",
            display_name=""Vector Store"",
            input_types=[""VectorStore""],
            required=True,
        ),
    ]

    outputs = [
        Output(display_name=""Vector Store Info"", name=""info"", method=""build_info""),
    ]

    def build_info(self) -> VectorStoreInfo:
        self.status = {
            ""name"": self.vectorstore_name,
            ""description"": self.vectorstore_description,
        }
        return VectorStoreInfo(
            vectorstore=self.input_vectorstore, description=self.vectorstore_description, name=self.vectorstore_name
        )
",VectorStoreInfo,toolkits,Informações sobre um VectorStore
"from typing import List, cast

from langchain_community.tools.bing_search import BingSearchResults
from langchain_community.utilities import BingSearchAPIWrapper

from langflow.base.langchain_utilities.model import LCToolComponent
from langflow.field_typing import Tool
from langflow.inputs import IntInput, MessageTextInput, MultilineInput, SecretStrInput
from langflow.schema import Data


class BingSearchAPIComponent(LCToolComponent):
    display_name = ""Bing Search API""
    description = ""Call the Bing Search API.""
    name = ""BingSearchAPI""

    inputs = [
        SecretStrInput(name=""bing_subscription_key"", display_name=""Bing Subscription Key""),
        MultilineInput(
            name=""input_value"",
            display_name=""Input"",
        ),
        MessageTextInput(name=""bing_search_url"", display_name=""Bing Search URL"", advanced=True),
        IntInput(name=""k"", display_name=""Number of results"", value=4, required=True),
    ]

    def run_model(self) -> List[Data]:
        if self.bing_search_url:
            wrapper = BingSearchAPIWrapper(
                bing_search_url=self.bing_search_url, bing_subscription_key=self.bing_subscription_key
            )
        else:
            wrapper = BingSearchAPIWrapper(bing_subscription_key=self.bing_subscription_key)  # type: ignore
        results = wrapper.results(query=self.input_value, num_results=self.k)
        data = [Data(data=result, text=result[""snippet""]) for result in results]
        self.status = data
        return data

    def build_tool(self) -> Tool:
        if self.bing_search_url:
            wrapper = BingSearchAPIWrapper(
                bing_search_url=self.bing_search_url, bing_subscription_key=self.bing_subscription_key
            )
        else:
            wrapper = BingSearchAPIWrapper(bing_subscription_key=self.bing_subscription_key)  # type: ignore
        return cast(Tool, BingSearchResults(api_wrapper=wrapper, num_results=self.k))
",BingSearchAPI,tools,Chama a API de Pesquisa do Bing.
"from typing import Union

from langchain_core.tools import Tool

from langflow.base.langchain_utilities.model import LCToolComponent
from langflow.inputs import SecretStrInput, MultilineInput, IntInput
from langflow.schema import Data


class GoogleSearchAPIComponent(LCToolComponent):
    display_name = ""Google Search API""
    description = ""Call Google Search API.""
    name = ""GoogleSearchAPI""

    inputs = [
        SecretStrInput(name=""google_api_key"", display_name=""Google API Key"", required=True),
        SecretStrInput(name=""google_cse_id"", display_name=""Google CSE ID"", required=True),
        MultilineInput(
            name=""input_value"",
            display_name=""Input"",
        ),
        IntInput(name=""k"", display_name=""Number of results"", value=4, required=True),
    ]

    def run_model(self) -> Union[Data, list[Data]]:
        wrapper = self._build_wrapper()
        results = wrapper.results(query=self.input_value, num_results=self.k)
        data = [Data(data=result, text=result[""snippet""]) for result in results]
        self.status = data
        return data

    def build_tool(self) -> Tool:
        wrapper = self._build_wrapper()
        return Tool(
            name=""google_search"",
            description=""Search Google for recent results."",
            func=wrapper.run,
        )

    def _build_wrapper(self):
        try:
            from langchain_google_community import GoogleSearchAPIWrapper  # type: ignore
        except ImportError:
            raise ImportError(""Please install langchain-google-community to use GoogleSearchAPIWrapper."")
        return GoogleSearchAPIWrapper(google_api_key=self.google_api_key, google_cse_id=self.google_cse_id, k=self.k)
",GoogleSearchAPI,tools,Chama a API de Pesquisa do Google.
"from typing import Union

from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper

from langflow.base.langchain_utilities.model import LCToolComponent
from langflow.inputs import SecretStrInput, MultilineInput, IntInput
from langflow.schema import Data
from langflow.field_typing import Tool


class GoogleSerperAPIComponent(LCToolComponent):
    display_name = ""Google Serper API""
    description = ""Call the Serper.dev Google Search API.""
    name = ""GoogleSerperAPI""

    inputs = [
        SecretStrInput(name=""serper_api_key"", display_name=""Serper API Key"", required=True),
        MultilineInput(
            name=""input_value"",
            display_name=""Input"",
        ),
        IntInput(name=""k"", display_name=""Number of results"", value=4, required=True),
    ]

    def run_model(self) -> Union[Data, list[Data]]:
        wrapper = self._build_wrapper()
        results = wrapper.results(query=self.input_value)
        list_results = results.get(""organic"", [])
        data = [Data(data=result, text=result[""snippet""]) for result in list_results]
        self.status = data
        return data

    def build_tool(self) -> Tool:
        wrapper = self._build_wrapper()
        return Tool(
            name=""google_search"",
            description=""Search Google for recent results."",
            func=wrapper.run,
        )

    def _build_wrapper(self):
        return GoogleSerperAPIWrapper(serper_api_key=self.serper_api_key, k=self.k)
",GoogleSerperAPI,tools,Chama a API de Pesquisa do Google da Serper.dev.
"import ast
from typing import Any, Dict, List, Optional

from langchain.agents import Tool
from langchain_core.tools import StructuredTool

from langflow.custom import CustomComponent
from langflow.schema.dotdict import dotdict


class PythonCodeStructuredTool(CustomComponent):
    display_name = ""PythonCodeTool""
    description = ""structuredtool dataclass code to tool""
    documentation = ""https://python.langchain.com/docs/modules/tools/custom_tools/#structuredtool-dataclass""
    name = ""PythonCodeStructuredTool""
    icon = ""ðŸ""
    field_order = [""name"", ""description"", ""tool_code"", ""return_direct"", ""tool_function"", ""tool_class""]

    def build_config(self) -> Dict[str, Any]:
        return {
            ""tool_code"": {
                ""display_name"": ""Tool Code"",
                ""info"": ""Enter the dataclass code."",
                ""placeholder"": ""def my_function(args):\n    pass"",
                ""multiline"": True,
                ""refresh_button"": True,
                ""field_type"": ""code"",
            },
            ""name"": {
                ""display_name"": ""Tool Name"",
                ""info"": ""Enter the name of the tool."",
            },
            ""description"": {
                ""display_name"": ""Description"",
                ""info"": ""Provide a brief description of what the tool does."",
            },
            ""return_direct"": {
                ""display_name"": ""Return Directly"",
                ""info"": ""Should the tool return the function output directly?"",
            },
            ""tool_function"": {
                ""display_name"": ""Tool Function"",
                ""info"": ""Select the function for additional expressions."",
                ""options"": [],
                ""refresh_button"": True,
            },
            ""tool_class"": {
                ""display_name"": ""Tool Class"",
                ""info"": ""Select the class for additional expressions."",
                ""options"": [],
                ""refresh_button"": True,
                ""required"": False,
            },
        }

    def parse_source_name(self, code: str) -> Dict:
        parsed_code = ast.parse(code)
        class_names = [node.name for node in parsed_code.body if isinstance(node, ast.ClassDef)]
        function_names = [node.name for node in parsed_code.body if isinstance(node, ast.FunctionDef)]
        return {""class"": class_names, ""function"": function_names}

    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None) -> dotdict:
        if field_name == ""tool_code"" or field_name == ""tool_function"" or field_name == ""tool_class"":
            try:
                names = self.parse_source_name(build_config.tool_code.value)
                build_config.tool_class.options = names[""class""]
                build_config.tool_function.options = names[""function""]
            except Exception as e:
                self.status = f""Failed to extract class names: {str(e)}""
                build_config.tool_class.options = [""Failed to parse"", str(e)]
                build_config.tool_function.options = []
        return build_config

    async def build(
        self,
        tool_code: str,
        name: str,
        description: str,
        tool_function: List[str],
        return_direct: bool,
        tool_class: Optional[List[str]] = None,
    ) -> Tool:
        local_namespace = {}  # type: ignore
        exec(tool_code, globals(), local_namespace)

        func = local_namespace[tool_function]
        _class = None

        if tool_class:
            _class = local_namespace[tool_class]

        tool = StructuredTool.from_function(
            func=func, args_schema=_class, name=name, description=description, return_direct=return_direct
        )
        return tool  # type: ignore
",PythonCodeStructuredTool,tools,código dataclass de structuredtool para ferramenta
"import importlib
from langchain_experimental.utilities import PythonREPL

from langflow.base.tools.base import build_status_from_tool
from langflow.custom import CustomComponent
from langchain_core.tools import Tool


class PythonREPLToolComponent(CustomComponent):
    display_name = ""Python REPL Tool""
    description = ""A tool for running Python code in a REPL environment.""
    name = ""PythonREPLTool""

    def build_config(self):
        return {
            ""name"": {""display_name"": ""Name"", ""info"": ""The name of the tool.""},
            ""description"": {""display_name"": ""Description"", ""info"": ""A description of the tool.""},
            ""global_imports"": {
                ""display_name"": ""Global Imports"",
                ""info"": ""A list of modules to import globally, e.g. ['math', 'numpy']."",
            },
        }

    def get_globals(self, globals: list[str]) -> dict:
        """"""
        Retrieves the global variables from the specified modules.

        Args:
            globals (list[str]): A list of module names.

        Returns:
            dict: A dictionary containing the global variables from the specified modules.
        """"""
        global_dict = {}
        for module in globals:
            try:
                imported_module = importlib.import_module(module)
                global_dict[imported_module.__name__] = imported_module
            except ImportError:
                raise ImportError(f""Could not import module {module}"")
        return global_dict

    def build(
        self,
        name: str = ""python_repl"",
        description: str = ""A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`."",
        global_imports: list[str] = [""math""],
    ) -> Tool:
        """"""
        Builds a Python REPL tool.

        Args:
            name (str, optional): The name of the tool. Defaults to ""python_repl"".
            description (str, optional): The description of the tool. Defaults to ""A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`. "".
            global_imports (list[str], optional): A list of global imports to be available in the Python REPL. Defaults to [""math""].

        Returns:
            Tool: The built Python REPL tool.
        """"""
        _globals = self.get_globals(global_imports)
        python_repl = PythonREPL(_globals=_globals)
        tool = Tool(
            name=name,
            description=description,
            func=python_repl.run,
        )
        self.status = build_status_from_tool(tool)
        return tool
",PythonREPLTool,tools,Uma ferramenta para executar código Python em um ambiente REPL.
"from langchain_core.tools import create_retriever_tool

from langflow.custom import CustomComponent
from langflow.field_typing import BaseRetriever, Tool


class RetrieverToolComponent(CustomComponent):
    display_name = ""RetrieverTool""
    description = ""Tool for interacting with retriever""
    name = ""RetrieverTool""

    def build_config(self):
        return {
            ""retriever"": {
                ""display_name"": ""Retriever"",
                ""info"": ""Retriever to interact with"",
                ""type"": BaseRetriever,
            },
            ""name"": {""display_name"": ""Name"", ""info"": ""Name of the tool""},
            ""description"": {""display_name"": ""Description"", ""info"": ""Description of the tool""},
        }

    def build(
        self,
        retriever: BaseRetriever,
        name: str,
        description: str,
    ) -> Tool:
        return create_retriever_tool(
            retriever=retriever,
            name=name,
            description=description,
        )
",RetrieverTool,tools,Ferramenta para interagir com o recuperador
"from typing import Union

from langchain_community.utilities.searchapi import SearchApiAPIWrapper

from langflow.base.langchain_utilities.model import LCToolComponent
from langflow.inputs import SecretStrInput, MultilineInput, DictInput, MessageTextInput
from langflow.schema import Data
from langflow.field_typing import Tool


class SearchAPIComponent(LCToolComponent):
    display_name: str = ""Search API""
    description: str = ""Call the searchapi.io API""
    name = ""SearchAPI""
    documentation: str = ""https://www.searchapi.io/docs/google""

    inputs = [
        MessageTextInput(name=""engine"", display_name=""Engine"", value=""google""),
        SecretStrInput(name=""api_key"", display_name=""SearchAPI API Key"", required=True),
        MultilineInput(
            name=""input_value"",
            display_name=""Input"",
        ),
        DictInput(name=""search_params"", display_name=""Search parameters"", advanced=True, is_list=True),
    ]

    def run_model(self) -> Union[Data, list[Data]]:
        wrapper = self._build_wrapper()
        results = wrapper.results(query=self.input_value, **(self.search_params or {}))
        list_results = results.get(""organic_results"", [])
        data = [Data(data=result, text=result[""snippet""]) for result in list_results]
        self.status = data
        return data

    def build_tool(self) -> Tool:
        wrapper = self._build_wrapper()
        return Tool(
            name=""search_api"",
            description=""Search for recent results."",
            func=lambda x: wrapper.run(query=x, **(self.search_params or {})),
        )

    def _build_wrapper(self):
        return SearchApiAPIWrapper(engine=self.engine, searchapi_api_key=self.api_key)
",SearchAPI,tools,Chama a API da searchapi.io
"from langchain_community.utilities.serpapi import SerpAPIWrapper

from langflow.base.langchain_utilities.model import LCToolComponent
from langflow.inputs import SecretStrInput, DictInput, MultilineInput
from langflow.schema import Data
from langflow.field_typing import Tool


class SerpAPIComponent(LCToolComponent):
    display_name = ""Serp Search API""
    description = ""Call Serp Search API""
    name = ""SerpAPI""

    inputs = [
        SecretStrInput(name=""serpapi_api_key"", display_name=""SerpAPI API Key"", required=True),
        MultilineInput(
            name=""input_value"",
            display_name=""Input"",
        ),
        DictInput(name=""search_params"", display_name=""Parameters"", advanced=True, is_list=True),
    ]

    def run_model(self) -> list[Data]:
        wrapper = self._build_wrapper()
        results = wrapper.results(self.input_value)
        list_results = results.get(""organic_results"", [])
        data = [Data(data=result, text=result[""snippet""]) for result in list_results]
        self.status = data
        return data

    def build_tool(self) -> Tool:
        wrapper = self._build_wrapper()
        return Tool(name=""search_api"", description=""Search for recent results."", func=wrapper.run)

    def _build_wrapper(self) -> SerpAPIWrapper:
        if self.search_params:
            return SerpAPIWrapper(  # type: ignore
                serpapi_api_key=self.serpapi_api_key,
                params=self.search_params,
            )
        return SerpAPIWrapper(  # type: ignore
            serpapi_api_key=self.serpapi_api_key
        )
",SerpAPI,tools,Chama a API Serp Search
"from typing import cast
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities.wikipedia import WikipediaAPIWrapper

from langflow.base.langchain_utilities.model import LCToolComponent
from langflow.field_typing import Tool
from langflow.inputs import BoolInput, IntInput, MessageTextInput, MultilineInput
from langflow.schema import Data


class WikipediaAPIComponent(LCToolComponent):
    display_name = ""Wikipedia API""
    description = ""Call Wikipedia API.""
    name = ""WikipediaAPI""

    inputs = [
        MultilineInput(
            name=""input_value"",
            display_name=""Input"",
        ),
        MessageTextInput(name=""lang"", display_name=""Language"", value=""en""),
        IntInput(name=""k"", display_name=""Number of results"", value=4, required=True),
        BoolInput(name=""load_all_available_meta"", display_name=""Load all available meta"", value=False, advanced=True),
        IntInput(
            name=""doc_content_chars_max"", display_name=""Document content characters max"", value=4000, advanced=True
        ),
    ]

    def run_model(self) -> list[Data]:
        wrapper = self._build_wrapper()
        docs = wrapper.load(self.input_value)
        data = [Data.from_document(doc) for doc in docs]
        self.status = data
        return data

    def build_tool(self) -> Tool:
        wrapper = self._build_wrapper()
        return cast(Tool, WikipediaQueryRun(api_wrapper=wrapper))

    def _build_wrapper(self) -> WikipediaAPIWrapper:
        return WikipediaAPIWrapper(  # type: ignore
            top_k_results=self.k,
            lang=self.lang,
            load_all_available_meta=self.load_all_available_meta,
            doc_content_chars_max=self.doc_content_chars_max,
        )
",WikipediaAPI,tools,Chama a API do Wikipedia.
"from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper

from langflow.base.langchain_utilities.model import LCToolComponent
from langflow.field_typing import Tool
from langflow.inputs import MultilineInput, SecretStrInput
from langflow.schema import Data


class WolframAlphaAPIComponent(LCToolComponent):
    display_name = ""WolframAlphaAPI""
    description = ""Call Wolfram Alpha API.""
    name = ""WolframAlphaAPI""

    inputs = [
        MultilineInput(
            name=""input_value"",
            display_name=""Input"",
        ),
        SecretStrInput(name=""app_id"", display_name=""App ID"", required=True),
    ]

    def run_model(self) -> list[Data]:
        wrapper = self._build_wrapper()
        result_str = wrapper.run(self.input_value)
        data = [Data(text=result_str)]
        self.status = data
        return data

    def build_tool(self) -> Tool:
        wrapper = self._build_wrapper()
        return Tool(name=""wolfram_alpha_api"", description=""Answers mathematical questions."", func=wrapper.run)

    def _build_wrapper(self) -> WolframAlphaAPIWrapper:
        return WolframAlphaAPIWrapper(wolfram_alpha_appid=self.app_id)  # type: ignore
",WolframAlphaAPI,tools,Chama a API do Wolfram Alpha.
"from typing import cast

from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool

from langflow.custom import Component
from langflow.field_typing import Tool
from langflow.io import Output


class YfinanceToolComponent(Component):
    display_name = ""Yahoo Finance News Tool""
    description = ""Tool for interacting with Yahoo Finance News.""
    name = ""YFinanceTool""

    outputs = [
        Output(display_name=""Tool"", name=""tool"", method=""build_tool""),
    ]

    def build_tool(self) -> Tool:
        return cast(Tool, YahooFinanceNewsTool())
",YfinanceTool,tools,Ferramenta para interagir com as Notícias de Finanças da Yahoo.
"from langchain_core.vectorstores import VectorStore
from loguru import logger

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers import docs_to_data
from langflow.inputs import DictInput, FloatInput
from langflow.io import (
    BoolInput,
    DataInput,
    DropdownInput,
    HandleInput,
    IntInput,
    MultilineInput,
    SecretStrInput,
    StrInput,
)
from langflow.schema import Data


class AstraVectorStoreComponent(LCVectorStoreComponent):
    display_name: str = ""Astra DB""
    description: str = ""Implementation of Vector Store using Astra DB with search capabilities""
    documentation: str = ""https://python.langchain.com/docs/integrations/vectorstores/astradb""
    name = ""AstraDB""
    icon: str = ""AstraDB""

    _cached_vectorstore: VectorStore | None = None

    inputs = [
        StrInput(
            name=""collection_name"",
            display_name=""Collection Name"",
            info=""The name of the collection within Astra DB where the vectors will be stored."",
            required=True,
        ),
        SecretStrInput(
            name=""token"",
            display_name=""Astra DB Application Token"",
            info=""Authentication token for accessing Astra DB."",
            value=""ASTRA_DB_APPLICATION_TOKEN"",
            required=True,
        ),
        SecretStrInput(
            name=""api_endpoint"",
            display_name=""API Endpoint"",
            info=""API endpoint URL for the Astra DB service."",
            value=""ASTRA_DB_API_ENDPOINT"",
            required=True,
        ),
        MultilineInput(
            name=""search_input"",
            display_name=""Search Input"",
        ),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        StrInput(
            name=""namespace"",
            display_name=""Namespace"",
            info=""Optional namespace within Astra DB to use for the collection."",
            advanced=True,
        ),
        DropdownInput(
            name=""metric"",
            display_name=""Metric"",
            info=""Optional distance metric for vector comparisons in the vector store."",
            options=[""cosine"", ""dot_product"", ""euclidean""],
            advanced=True,
        ),
        IntInput(
            name=""batch_size"",
            display_name=""Batch Size"",
            info=""Optional number of data to process in a single batch."",
            advanced=True,
        ),
        IntInput(
            name=""bulk_insert_batch_concurrency"",
            display_name=""Bulk Insert Batch Concurrency"",
            info=""Optional concurrency level for bulk insert operations."",
            advanced=True,
        ),
        IntInput(
            name=""bulk_insert_overwrite_concurrency"",
            display_name=""Bulk Insert Overwrite Concurrency"",
            info=""Optional concurrency level for bulk insert operations that overwrite existing data."",
            advanced=True,
        ),
        IntInput(
            name=""bulk_delete_concurrency"",
            display_name=""Bulk Delete Concurrency"",
            info=""Optional concurrency level for bulk delete operations."",
            advanced=True,
        ),
        DropdownInput(
            name=""setup_mode"",
            display_name=""Setup Mode"",
            info=""Configuration mode for setting up the vector store, with options like 'Sync', 'Async', or 'Off'."",
            options=[""Sync"", ""Async"", ""Off""],
            advanced=True,
            value=""Sync"",
        ),
        BoolInput(
            name=""pre_delete_collection"",
            display_name=""Pre Delete Collection"",
            info=""Boolean flag to determine whether to delete the collection before creating a new one."",
            advanced=True,
        ),
        StrInput(
            name=""metadata_indexing_include"",
            display_name=""Metadata Indexing Include"",
            info=""Optional list of metadata fields to include in the indexing."",
            advanced=True,
        ),
        HandleInput(
            name=""embedding"",
            display_name=""Embedding or Astra Vectorize"",
            input_types=[""Embeddings"", ""dict""],
            info=""Allows either an embedding model or an Astra Vectorize configuration."",  # TODO: This should be optional, but need to refactor langchain-astradb first.
        ),
        StrInput(
            name=""metadata_indexing_exclude"",
            display_name=""Metadata Indexing Exclude"",
            info=""Optional list of metadata fields to exclude from the indexing."",
            advanced=True,
        ),
        StrInput(
            name=""collection_indexing_policy"",
            display_name=""Collection Indexing Policy"",
            info=""Optional dictionary defining the indexing policy for the collection."",
            advanced=True,
        ),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            advanced=True,
            value=4,
        ),
        DropdownInput(
            name=""search_type"",
            display_name=""Search Type"",
            info=""Search type to use"",
            options=[""Similarity"", ""Similarity with score threshold"", ""MMR (Max Marginal Relevance)""],
            value=""Similarity"",
            advanced=True,
        ),
        FloatInput(
            name=""search_score_threshold"",
            display_name=""Search Score Threshold"",
            info=""Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')"",
            value=0,
            advanced=True,
        ),
        DictInput(
            name=""search_filter"",
            display_name=""Search Metadata Filter"",
            info=""Optional dictionary of filters to apply to the search query."",
            advanced=True,
            is_list=True,
        ),
    ]

    def _build_vector_store(self):
        # cache the vector store to avoid re-initializing and ingest data again
        if self._cached_vectorstore:
            return self._cached_vectorstore

        try:
            from langchain_astradb import AstraDBVectorStore
            from langchain_astradb.utils.astradb import SetupMode
        except ImportError:
            raise ImportError(
                ""Could not import langchain Astra DB integration package. ""
                ""Please install it with `pip install langchain-astradb`.""
            )

        try:
            if not self.setup_mode:
                self.setup_mode = self._inputs[""setup_mode""].options[0]

            setup_mode_value = SetupMode[self.setup_mode.upper()]
        except KeyError:
            raise ValueError(f""Invalid setup mode: {self.setup_mode}"")

        if not isinstance(self.embedding, dict):
            embedding_dict = {""embedding"": self.embedding}
        else:
            from astrapy.info import CollectionVectorServiceOptions

            dict_options = self.embedding.get(""collection_vector_service_options"", {})
            dict_options[""authentication""] = {
                k: v for k, v in dict_options.get(""authentication"", {}).items() if k and v
            }
            dict_options[""parameters""] = {k: v for k, v in dict_options.get(""parameters"", {}).items() if k and v}
            embedding_dict = {
                ""collection_vector_service_options"": CollectionVectorServiceOptions.from_dict(dict_options)
            }
            collection_embedding_api_key = self.embedding.get(""collection_embedding_api_key"")
            if collection_embedding_api_key:
                embedding_dict[""collection_embedding_api_key""] = collection_embedding_api_key

        vector_store_kwargs = {
            **embedding_dict,
            ""collection_name"": self.collection_name,
            ""token"": self.token,
            ""api_endpoint"": self.api_endpoint,
            ""namespace"": self.namespace or None,
            ""metric"": self.metric or None,
            ""batch_size"": self.batch_size or None,
            ""bulk_insert_batch_concurrency"": self.bulk_insert_batch_concurrency or None,
            ""bulk_insert_overwrite_concurrency"": self.bulk_insert_overwrite_concurrency or None,
            ""bulk_delete_concurrency"": self.bulk_delete_concurrency or None,
            ""setup_mode"": setup_mode_value,
            ""pre_delete_collection"": self.pre_delete_collection or False,
        }

        if self.metadata_indexing_include:
            vector_store_kwargs[""metadata_indexing_include""] = self.metadata_indexing_include
        elif self.metadata_indexing_exclude:
            vector_store_kwargs[""metadata_indexing_exclude""] = self.metadata_indexing_exclude
        elif self.collection_indexing_policy:
            vector_store_kwargs[""collection_indexing_policy""] = self.collection_indexing_policy

        try:
            vector_store = AstraDBVectorStore(**vector_store_kwargs)
        except Exception as e:
            raise ValueError(f""Error initializing AstraDBVectorStore: {str(e)}"") from e

        self._add_documents_to_vector_store(vector_store)

        self._cached_vectorstore = vector_store

        return vector_store

    def _add_documents_to_vector_store(self, vector_store):
        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                raise ValueError(""Vector Store Inputs must be Data objects."")

        if documents:
            logger.debug(f""Adding {len(documents)} documents to the Vector Store."")
            try:
                vector_store.add_documents(documents)
            except Exception as e:
                raise ValueError(f""Error adding documents to AstraDBVectorStore: {str(e)}"") from e
        else:
            logger.debug(""No documents to add to the Vector Store."")

    def _map_search_type(self):
        if self.search_type == ""Similarity with score threshold"":
            return ""similarity_score_threshold""
        elif self.search_type == ""MMR (Max Marginal Relevance)"":
            return ""mmr""
        else:
            return ""similarity""

    def _build_search_args(self):
        args = {
            ""k"": self.number_of_results,
            ""score_threshold"": self.search_score_threshold,
        }

        if self.search_filter:
            clean_filter = {k: v for k, v in self.search_filter.items() if k and v}
            if len(clean_filter) > 0:
                args[""filter""] = clean_filter
        return args

    def search_documents(self) -> list[Data]:
        vector_store = self._build_vector_store()

        logger.debug(f""Search input: {self.search_input}"")
        logger.debug(f""Search type: {self.search_type}"")
        logger.debug(f""Number of results: {self.number_of_results}"")

        if self.search_input and isinstance(self.search_input, str) and self.search_input.strip():
            try:
                search_type = self._map_search_type()
                search_args = self._build_search_args()

                docs = vector_store.search(query=self.search_input, search_type=search_type, **search_args)
            except Exception as e:
                raise ValueError(f""Error performing search in AstraDBVectorStore: {str(e)}"") from e

            logger.debug(f""Retrieved documents: {len(docs)}"")

            data = docs_to_data(docs)
            logger.debug(f""Converted documents to data: {len(data)}"")
            self.status = data
            return data
        else:
            logger.debug(""No search input provided. Skipping search."")
            return []

    def get_retriever_kwargs(self):
        search_args = self._build_search_args()
        return {
            ""search_type"": self._map_search_type(),
            ""search_kwargs"": search_args,
        }

    def build_vector_store(self):
        vector_store = self._build_vector_store()
        return vector_store
",AstraDB,vectorstores,Implementação de Vector Store usando Astra DB com capacidades de pesquisa
"from typing import List

from langchain_community.vectorstores import Cassandra
from loguru import logger

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.inputs import BoolInput, DictInput, FloatInput
from langflow.io import (
    DataInput,
    DropdownInput,
    HandleInput,
    IntInput,
    MessageTextInput,
    MultilineInput,
    SecretStrInput,
)
from langflow.schema import Data


class CassandraVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""Cassandra""
    description = ""Cassandra Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/cassandra""
    name = ""Cassandra""
    icon = ""Cassandra""

    _cached_vectorstore: Cassandra | None = None

    inputs = [
        MessageTextInput(
            name=""database_ref"",
            display_name=""Contact Points / Astra Database ID"",
            info=""Contact points for the database (or AstraDB database ID)"",
            required=True,
        ),
        MessageTextInput(
            name=""username"", display_name=""Username"", info=""Username for the database (leave empty for AstraDB).""
        ),
        SecretStrInput(
            name=""token"",
            display_name=""Password / AstraDB Token"",
            info=""User password for the database (or AstraDB token)."",
            required=True,
        ),
        MessageTextInput(
            name=""keyspace"",
            display_name=""Keyspace"",
            info=""Table Keyspace (or AstraDB namespace)."",
            required=True,
        ),
        MessageTextInput(
            name=""table_name"",
            display_name=""Table Name"",
            info=""The name of the table (or AstraDB collection) where vectors will be stored."",
            required=True,
        ),
        IntInput(
            name=""ttl_seconds"",
            display_name=""TTL Seconds"",
            info=""Optional time-to-live for the added texts."",
            advanced=True,
        ),
        IntInput(
            name=""batch_size"",
            display_name=""Batch Size"",
            info=""Optional number of data to process in a single batch."",
            value=16,
            advanced=True,
        ),
        DropdownInput(
            name=""setup_mode"",
            display_name=""Setup Mode"",
            info=""Configuration mode for setting up the Cassandra table, with options like 'Sync', 'Async', or 'Off'."",
            options=[""Sync"", ""Async"", ""Off""],
            value=""Sync"",
            advanced=True,
        ),
        DictInput(
            name=""cluster_kwargs"",
            display_name=""Cluster arguments"",
            info=""Optional dictionary of additional keyword arguments for the Cassandra cluster."",
            advanced=True,
            is_list=True,
        ),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
        DropdownInput(
            name=""search_type"",
            display_name=""Search Type"",
            info=""Search type to use"",
            options=[""Similarity"", ""Similarity with score threshold"", ""MMR (Max Marginal Relevance)""],
            value=""Similarity"",
            advanced=True,
        ),
        FloatInput(
            name=""search_score_threshold"",
            display_name=""Search Score Threshold"",
            info=""Minimum similarity score threshold for search results. (when using 'Similarity with score threshold')"",
            value=0,
            advanced=True,
        ),
        DictInput(
            name=""search_filter"",
            display_name=""Search Metadata Filter"",
            info=""Optional dictionary of filters to apply to the search query."",
            advanced=True,
            is_list=True,
        ),
        MessageTextInput(
            name=""body_search"",
            display_name=""Search Body"",
            info=""Document textual search terms to apply to the search query."",
            advanced=True,
        ),
        BoolInput(
            name=""enable_body_search"",
            display_name=""Enable Body Search"",
            info=""Flag to enable body search. This must be enabled BEFORE the table is created."",
            value=False,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> Cassandra:
        return self._build_cassandra()

    def _build_cassandra(self) -> Cassandra:
        if self._cached_vectorstore:
            return self._cached_vectorstore
        try:
            import cassio
            from langchain_community.utilities.cassandra import SetupMode
        except ImportError:
            raise ImportError(
                ""Could not import cassio integration package. "" ""Please install it with `pip install cassio`.""
            )

        from uuid import UUID

        database_ref = self.database_ref

        try:
            UUID(self.database_ref)
            is_astra = True
        except ValueError:
            is_astra = False
            if "","" in self.database_ref:
                # use a copy because we can't change the type of the parameter
                database_ref = self.database_ref.split("","")

        if is_astra:
            cassio.init(
                database_id=database_ref,
                token=self.token,
                cluster_kwargs=self.cluster_kwargs,
            )
        else:
            cassio.init(
                contact_points=database_ref,
                username=self.username,
                password=self.token,
                cluster_kwargs=self.cluster_kwargs,
            )
        documents = []

        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if self.enable_body_search:
            body_index_options = [(""index_analyzer"", ""STANDARD"")]
        else:
            body_index_options = None

        if self.setup_mode == ""Off"":
            setup_mode = SetupMode.OFF
        elif self.setup_mode == ""Sync"":
            setup_mode = SetupMode.SYNC
        else:
            setup_mode = SetupMode.ASYNC

        if documents:
            logger.debug(f""Adding {len(documents)} documents to the Vector Store."")
            table = Cassandra.from_documents(
                documents=documents,
                embedding=self.embedding,
                table_name=self.table_name,
                keyspace=self.keyspace,
                ttl_seconds=self.ttl_seconds or None,
                batch_size=self.batch_size,
                body_index_options=body_index_options,
            )
        else:
            logger.debug(""No documents to add to the Vector Store."")
            table = Cassandra(
                embedding=self.embedding,
                table_name=self.table_name,
                keyspace=self.keyspace,
                ttl_seconds=self.ttl_seconds or None,
                body_index_options=body_index_options,
                setup_mode=setup_mode,
            )
        self._cached_vectorstore = table
        return table

    def _map_search_type(self):
        if self.search_type == ""Similarity with score threshold"":
            return ""similarity_score_threshold""
        elif self.search_type == ""MMR (Max Marginal Relevance)"":
            return ""mmr""
        else:
            return ""similarity""

    def search_documents(self) -> List[Data]:
        vector_store = self._build_cassandra()

        logger.debug(f""Search input: {self.search_query}"")
        logger.debug(f""Search type: {self.search_type}"")
        logger.debug(f""Number of results: {self.number_of_results}"")

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            try:
                search_type = self._map_search_type()
                search_args = self._build_search_args()

                logger.debug(f""Search args: {str(search_args)}"")

                docs = vector_store.search(query=self.search_query, search_type=search_type, **search_args)
            except KeyError as e:
                if ""content"" in str(e):
                    raise ValueError(
                        ""You should ingest data through Langflow (or LangChain) to query it in Langflow. Your collection does not contain a field name 'content'.""
                    )
                else:
                    raise e

            logger.debug(f""Retrieved documents: {len(docs)}"")

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []

    def _build_search_args(self):
        args = {
            ""k"": self.number_of_results,
            ""score_threshold"": self.search_score_threshold,
        }

        if self.search_filter:
            clean_filter = {k: v for k, v in self.search_filter.items() if k and v}
            if len(clean_filter) > 0:
                args[""filter""] = clean_filter
        if self.body_search:
            if not self.enable_body_search:
                raise ValueError(""You should enable body search when creating the table to search the body field."")
            args[""body_search""] = self.body_search
        return args

    def get_retriever_kwargs(self):
        search_args = self._build_search_args()
        return {
            ""search_type"": self._map_search_type(),
            ""search_kwargs"": search_args,
        }
",Cassandra,vectorstores,Vector Store Cassandra com capacidades de pesquisa
"from copy import deepcopy
from typing import TYPE_CHECKING

from chromadb.config import Settings
from langchain_chroma.vectorstores import Chroma
from loguru import logger

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.base.vectorstores.utils import chroma_collection_to_data
from langflow.io import BoolInput, DataInput, DropdownInput, HandleInput, IntInput, StrInput, MultilineInput
from langflow.schema import Data

if TYPE_CHECKING:
    from langchain_chroma import Chroma


class ChromaVectorStoreComponent(LCVectorStoreComponent):
    """"""
    Chroma Vector Store with search capabilities
    """"""

    display_name: str = ""Chroma DB""
    description: str = ""Chroma Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/docs/integrations/vectorstores/chroma""
    name = ""Chroma""
    icon = ""Chroma""

    inputs = [
        StrInput(
            name=""collection_name"",
            display_name=""Collection Name"",
            value=""langflow"",
        ),
        StrInput(
            name=""persist_directory"",
            display_name=""Persist Directory"",
        ),
        MultilineInput(
            name=""search_query"",
            display_name=""Search Query"",
        ),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        StrInput(
            name=""chroma_server_cors_allow_origins"",
            display_name=""Server CORS Allow Origins"",
            advanced=True,
        ),
        StrInput(
            name=""chroma_server_host"",
            display_name=""Server Host"",
            advanced=True,
        ),
        IntInput(
            name=""chroma_server_http_port"",
            display_name=""Server HTTP Port"",
            advanced=True,
        ),
        IntInput(
            name=""chroma_server_grpc_port"",
            display_name=""Server gRPC Port"",
            advanced=True,
        ),
        BoolInput(
            name=""chroma_server_ssl_enabled"",
            display_name=""Server SSL Enabled"",
            advanced=True,
        ),
        BoolInput(
            name=""allow_duplicates"",
            display_name=""Allow Duplicates"",
            advanced=True,
            info=""If false, will not add documents that are already in the Vector Store."",
        ),
        DropdownInput(
            name=""search_type"",
            display_name=""Search Type"",
            options=[""Similarity"", ""MMR""],
            value=""Similarity"",
            advanced=True,
        ),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            advanced=True,
            value=10,
        ),
        IntInput(
            name=""limit"",
            display_name=""Limit"",
            advanced=True,
            info=""Limit the number of records to compare when Allow Duplicates is False."",
        ),
    ]

    def build_vector_store(self) -> Chroma:
        """"""
        Builds the Chroma object.
        """"""
        try:
            from chromadb import Client
            from langchain_chroma import Chroma
        except ImportError:
            raise ImportError(
                ""Could not import Chroma integration package. "" ""Please install it with `pip install langchain-chroma`.""
            )
        # Chroma settings
        chroma_settings = None
        client = None
        if self.chroma_server_host:
            chroma_settings = Settings(
                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],
                chroma_server_host=self.chroma_server_host,
                chroma_server_http_port=self.chroma_server_http_port or None,
                chroma_server_grpc_port=self.chroma_server_grpc_port or None,
                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,
            )
            client = Client(settings=chroma_settings)

        # Check persist_directory and expand it if it is a relative path
        if self.persist_directory is not None:
            persist_directory = self.resolve_path(self.persist_directory)
        else:
            persist_directory = None

        chroma = Chroma(
            persist_directory=persist_directory,
            client=client,
            embedding_function=self.embedding,
            collection_name=self.collection_name,
        )

        self._add_documents_to_vector_store(chroma)
        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))
        return chroma

    def _add_documents_to_vector_store(self, vector_store: ""Chroma"") -> None:
        """"""
        Adds documents to the Vector Store.
        """"""
        if not self.ingest_data:
            self.status = """"
            return

        _stored_documents_without_id = []
        if self.allow_duplicates:
            stored_data = []
        else:
            stored_data = chroma_collection_to_data(vector_store.get(self.limit))
            for value in deepcopy(stored_data):
                del value.id
                _stored_documents_without_id.append(value)

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                if _input not in _stored_documents_without_id:
                    documents.append(_input.to_lc_document())
            else:
                raise ValueError(""Vector Store Inputs must be Data objects."")

        if documents and self.embedding is not None:
            logger.debug(f""Adding {len(documents)} documents to the Vector Store."")
            vector_store.add_documents(documents)
        else:
            logger.debug(""No documents to add to the Vector Store."")
",Chroma,vectorstores,Vector Store Chroma com capacidades de pesquisa
"from datetime import timedelta
from typing import List

from langchain_community.vectorstores import CouchbaseVectorStore

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput
from langflow.schema import Data


class CouchbaseVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""Couchbase""
    description = ""Couchbase Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/v0.1/docs/integrations/document_loaders/couchbase/""
    name = ""Couchbase""
    icon = ""Couchbase""

    inputs = [
        SecretStrInput(
            name=""couchbase_connection_string"", display_name=""Couchbase Cluster connection string"", required=True
        ),
        StrInput(name=""couchbase_username"", display_name=""Couchbase username"", required=True),
        SecretStrInput(name=""couchbase_password"", display_name=""Couchbase password"", required=True),
        StrInput(name=""bucket_name"", display_name=""Bucket Name"", required=True),
        StrInput(name=""scope_name"", display_name=""Scope Name"", required=True),
        StrInput(name=""collection_name"", display_name=""Collection Name"", required=True),
        StrInput(name=""index_name"", display_name=""Index Name"", required=True),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> CouchbaseVectorStore:
        return self._build_couchbase()

    def _build_couchbase(self) -> CouchbaseVectorStore:
        try:
            from couchbase.auth import PasswordAuthenticator  # type: ignore
            from couchbase.cluster import Cluster  # type: ignore
            from couchbase.options import ClusterOptions  # type: ignore
        except ImportError as e:
            raise ImportError(
                ""Failed to import Couchbase dependencies. Install it using `pip install langflow[couchbase] --pre`""
            ) from e

        try:
            auth = PasswordAuthenticator(self.couchbase_username, self.couchbase_password)
            options = ClusterOptions(auth)
            cluster = Cluster(self.couchbase_connection_string, options)

            cluster.wait_until_ready(timedelta(seconds=5))
        except Exception as e:
            raise ValueError(f""Failed to connect to Couchbase: {e}"")

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents:
            couchbase_vs = CouchbaseVectorStore.from_documents(
                documents=documents,
                cluster=cluster,
                bucket_name=self.bucket_name,
                scope_name=self.scope_name,
                collection_name=self.collection_name,
                embedding=self.embedding,
                index_name=self.index_name,
            )

        else:
            couchbase_vs = CouchbaseVectorStore(
                cluster=cluster,
                bucket_name=self.bucket_name,
                scope_name=self.scope_name,
                collection_name=self.collection_name,
                embedding=self.embedding,
                index_name=self.index_name,
            )

        return couchbase_vs

    def search_documents(self) -> List[Data]:
        vector_store = self._build_couchbase()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",Couchbase,vectorstores,Vector Store Couchbase com capacidades de pesquisa
"from typing import List

from langchain_community.vectorstores import FAISS
from loguru import logger

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import BoolInput, DataInput, HandleInput, IntInput, MultilineInput, StrInput
from langflow.schema import Data


class FaissVectorStoreComponent(LCVectorStoreComponent):
    """"""
    FAISS Vector Store with search capabilities
    """"""

    display_name: str = ""FAISS""
    description: str = ""FAISS Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss""
    name = ""FAISS""
    icon = ""FAISS""

    inputs = [
        StrInput(
            name=""index_name"",
            display_name=""Index Name"",
            value=""langflow_index"",
        ),
        StrInput(
            name=""persist_directory"",
            display_name=""Persist Directory"",
            info=""Path to save the FAISS index. It will be relative to where Langflow is running."",
        ),
        MultilineInput(
            name=""search_query"",
            display_name=""Search Query"",
        ),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        BoolInput(
            name=""allow_dangerous_deserialization"",
            display_name=""Allow Dangerous Deserialization"",
            info=""Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data."",
            advanced=True,
            value=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            advanced=True,
            value=4,
        ),
    ]

    def build_vector_store(self) -> FAISS:
        """"""
        Builds the FAISS object.
        """"""
        if not self.persist_directory:
            raise ValueError(""Folder path is required to save the FAISS index."")
        path = self.resolve_path(self.persist_directory)

        documents = []

        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)
        faiss.save_local(str(path), self.index_name)

        return faiss

    def search_documents(self) -> List[Data]:
        """"""
        Search for documents in the FAISS vector store.
        """"""
        if not self.persist_directory:
            raise ValueError(""Folder path is required to load the FAISS index."")
        path = self.resolve_path(self.persist_directory)

        vector_store = FAISS.load_local(
            folder_path=path,
            embeddings=self.embedding,
            index_name=self.index_name,
            allow_dangerous_deserialization=self.allow_dangerous_deserialization,
        )

        if not vector_store:
            raise ValueError(""Failed to load the FAISS index."")

        logger.debug(f""Search input: {self.search_query}"")
        logger.debug(f""Number of results: {self.number_of_results}"")

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            logger.debug(f""Retrieved documents: {len(docs)}"")

            data = docs_to_data(docs)
            logger.debug(f""Converted documents to data: {len(data)}"")
            logger.debug(data)
            return data  # Return the search results data
        else:
            logger.debug(""No search input provided. Skipping search."")
            return []
",FAISS,vectorstores,Vector Store FAISS com capacidades de pesquisa
"from typing import List

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import (
    DataInput,
    StrInput,
    IntInput,
    FloatInput,
    BoolInput,
    DictInput,
    MultilineInput,
    DropdownInput,
    SecretStrInput,
    HandleInput,
)
from langflow.schema import Data


class MilvusVectorStoreComponent(LCVectorStoreComponent):
    """"""Milvus vector store with search capabilities""""""

    display_name: str = ""Milvus""
    description: str = ""Milvus vector store with search capabilities""
    documentation = ""https://python.langchain.com/docs/integrations/vectorstores/milvus""
    name = ""Milvus""
    icon = ""Milvus""

    inputs = [
        StrInput(name=""collection_name"", display_name=""Collection Name"", value=""langflow""),
        StrInput(name=""collection_description"", display_name=""Collection Description"", value=""""),
        StrInput(
            name=""uri"",
            display_name=""Connection URI"",
            value=""http://localhost:19530"",
        ),
        SecretStrInput(
            name=""password"",
            display_name=""Connection Password"",
            value="""",
            info=""Ignore this field if no password is required to make connection."",
        ),
        DictInput(name=""connection_args"", display_name=""Other Connection Arguments"", advanced=True),
        StrInput(name=""primary_field"", display_name=""Primary Field Name"", value=""pk""),
        StrInput(name=""text_field"", display_name=""Text Field Name"", value=""text""),
        StrInput(name=""vector_field"", display_name=""Vector Field Name"", value=""vector""),
        DropdownInput(
            name=""consistency_level"",
            display_name=""Consistencey Level"",
            options=[""Bounded"", ""Session"", ""Strong"", ""Eventual""],
            value=""Session"",
            advanced=True,
        ),
        DictInput(name=""index_params"", display_name=""Index Parameters"", advanced=True),
        DictInput(name=""search_params"", display_name=""Search Parameters"", advanced=True),
        BoolInput(name=""drop_old"", display_name=""Drop Old Collection"", value=False, advanced=True),
        FloatInput(name=""timeout"", display_name=""Timeout"", advanced=True),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self):
        try:
            from langchain_milvus.vectorstores import Milvus as LangchainMilvus
        except ImportError:
            raise ImportError(
                ""Could not import Milvus integration package. "" ""Please install it with `pip install langchain-milvus`.""
            )
        self.connection_args.update(uri=self.uri, token=self.password)
        milvus_store = LangchainMilvus(
            embedding_function=self.embedding,
            collection_name=self.collection_name,
            collection_description=self.collection_description,
            connection_args=self.connection_args,
            consistency_level=self.consistency_level,
            index_params=self.index_params,
            search_params=self.search_params,
            drop_old=self.drop_old,
            auto_id=True,
            primary_field=self.primary_field,
            text_field=self.text_field,
            vector_field=self.vector_field,
            timeout=self.timeout,
        )

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents:
            milvus_store.add_documents(documents)

        return milvus_store

    def search_documents(self) -> List[Data]:
        vector_store = self.build_vector_store()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",Milvus,vectorstores,Vector store Milvus com capacidades de pesquisa
"from typing import List

from langchain_community.vectorstores import MongoDBAtlasVectorSearch

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput
from langflow.schema import Data


class MongoVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""MongoDB Atlas""
    description = ""MongoDB Atlas Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/mongodb_atlas""
    name = ""MongoDBAtlasVector""
    icon = ""MongoDB""

    inputs = [
        SecretStrInput(name=""mongodb_atlas_cluster_uri"", display_name=""MongoDB Atlas Cluster URI"", required=True),
        StrInput(name=""db_name"", display_name=""Database Name"", required=True),
        StrInput(name=""collection_name"", display_name=""Collection Name"", required=True),
        StrInput(name=""index_name"", display_name=""Index Name"", required=True),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> MongoDBAtlasVectorSearch:
        return self._build_mongodb_atlas()

    def _build_mongodb_atlas(self) -> MongoDBAtlasVectorSearch:
        try:
            from pymongo import MongoClient
        except ImportError:
            raise ImportError(""Please install pymongo to use MongoDB Atlas Vector Store"")

        try:
            mongo_client: MongoClient = MongoClient(self.mongodb_atlas_cluster_uri)
            collection = mongo_client[self.db_name][self.collection_name]
        except Exception as e:
            raise ValueError(f""Failed to connect to MongoDB Atlas: {e}"")

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

            if documents:
                vector_store = MongoDBAtlasVectorSearch.from_documents(
                    documents=documents, embedding=self.embedding, collection=collection, index_name=self.index_name
                )
            else:
                vector_store = MongoDBAtlasVectorSearch(
                    embedding=self.embedding,
                    collection=collection,
                    index_name=self.index_name,
                )
        else:
            vector_store = MongoDBAtlasVectorSearch(
                embedding=self.embedding,
                collection=collection,
                index_name=self.index_name,
            )

        return vector_store

    def search_documents(self) -> List[Data]:
        from bson import ObjectId

        vector_store = self._build_mongodb_atlas()

        if self.search_query and isinstance(self.search_query, str):
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )
            for doc in docs:
                doc.metadata = {
                    key: str(value) if isinstance(value, ObjectId) else value for key, value in doc.metadata.items()
                }

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",MongoDBAtlasVector,vectorstores,Vector Store MongoDB Atlas com capacidades de pesquisa
"from typing import List

from langchain_community.vectorstores import PGVector

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput
from langflow.schema import Data


class PGVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""PGVector""
    description = ""PGVector Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/""
    name = ""pgvector""
    icon = ""PGVector""

    inputs = [
        SecretStrInput(name=""pg_server_url"", display_name=""PostgreSQL Server Connection String"", required=True),
        StrInput(name=""collection_name"", display_name=""Table"", required=True),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingestion Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
    ]

    def build_vector_store(self) -> PGVector:
        return self._build_pgvector()

    def _build_pgvector(self) -> PGVector:
        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents:
            pgvector = PGVector.from_documents(
                embedding=self.embedding,
                documents=documents,
                collection_name=self.collection_name,
                connection_string=self.pg_server_url,
            )
        else:
            pgvector = PGVector.from_existing_index(
                embedding=self.embedding,
                collection_name=self.collection_name,
                connection_string=self.pg_server_url,
            )

        return pgvector

    def search_documents(self) -> List[Data]:
        vector_store = self._build_pgvector()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",pgvector,vectorstores,Vector Store PGVector com capacidades de pesquisa
"from typing import List

from langchain_pinecone import Pinecone

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import (
    DropdownInput,
    HandleInput,
    IntInput,
    StrInput,
    SecretStrInput,
    DataInput,
    MultilineInput,
)
from langflow.schema import Data


class PineconeVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""Pinecone""
    description = ""Pinecone Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/""
    name = ""Pinecone""
    icon = ""Pinecone""
    pinecone_instance = None

    inputs = [
        StrInput(name=""index_name"", display_name=""Index Name"", required=True),
        StrInput(name=""namespace"", display_name=""Namespace"", info=""Namespace for the index.""),
        DropdownInput(
            name=""distance_strategy"",
            display_name=""Distance Strategy"",
            options=[""Cosine"", ""Euclidean"", ""Dot Product""],
            value=""Cosine"",
            advanced=True,
        ),
        SecretStrInput(name=""pinecone_api_key"", display_name=""Pinecone API Key"", required=True),
        StrInput(
            name=""text_key"",
            display_name=""Text Key"",
            info=""Key in the record to use as text."",
            value=""text"",
            advanced=True,
        ),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> Pinecone:
        return self._build_pinecone()

    def _build_pinecone(self) -> Pinecone:
        if self.pinecone_instance is not None:
            return self.pinecone_instance
        from langchain_pinecone._utilities import DistanceStrategy
        from langchain_pinecone.vectorstores import Pinecone

        distance_strategy = self.distance_strategy.replace("" "", ""_"").upper()
        _distance_strategy = DistanceStrategy[distance_strategy]

        pinecone = Pinecone(
            index_name=self.index_name,
            embedding=self.embedding,
            text_key=self.text_key,
            namespace=self.namespace,
            distance_strategy=_distance_strategy,
            pinecone_api_key=self.pinecone_api_key,
        )

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents:
            pinecone.add_documents(documents)
        self.pinecone_instance = pinecone
        return pinecone

    def search_documents(self) -> List[Data]:
        vector_store = self._build_pinecone()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",Pinecone,vectorstores,Vector Store Pinecone com capacidades de pesquisa
"from typing import List

from langchain_community.vectorstores import Qdrant
from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import (
    DropdownInput,
    HandleInput,
    IntInput,
    StrInput,
    SecretStrInput,
    DataInput,
    MultilineInput,
)
from langflow.schema import Data
from langchain.embeddings.base import Embeddings


class QdrantVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""Qdrant""
    description = ""Qdrant Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/qdrant""
    icon = ""Qdrant""

    inputs = [
        StrInput(name=""collection_name"", display_name=""Collection Name"", required=True),
        StrInput(name=""host"", display_name=""Host"", value=""localhost"", advanced=True),
        IntInput(name=""port"", display_name=""Port"", value=6333, advanced=True),
        IntInput(name=""grpc_port"", display_name=""gRPC Port"", value=6334, advanced=True),
        SecretStrInput(name=""api_key"", display_name=""API Key"", advanced=True),
        StrInput(name=""prefix"", display_name=""Prefix"", advanced=True),
        IntInput(name=""timeout"", display_name=""Timeout"", advanced=True),
        StrInput(name=""path"", display_name=""Path"", advanced=True),
        StrInput(name=""url"", display_name=""URL"", advanced=True),
        DropdownInput(
            name=""distance_func"",
            display_name=""Distance Function"",
            options=[""Cosine"", ""Euclidean"", ""Dot Product""],
            value=""Cosine"",
            advanced=True,
        ),
        StrInput(name=""content_payload_key"", display_name=""Content Payload Key"", value=""page_content"", advanced=True),
        StrInput(name=""metadata_payload_key"", display_name=""Metadata Payload Key"", value=""metadata"", advanced=True),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> Qdrant:
        return self._build_qdrant()

    def _build_qdrant(self) -> Qdrant:
        qdrant_kwargs = {
            ""collection_name"": self.collection_name,
            ""content_payload_key"": self.content_payload_key,
            ""metadata_payload_key"": self.metadata_payload_key,
        }

        server_kwargs = {
            ""host"": self.host if self.host else None,
            ""port"": int(self.port),  # Garantir que port seja um inteiro
            ""grpc_port"": int(self.grpc_port),  # Garantir que grpc_port seja um inteiro
            ""api_key"": self.api_key,
            ""prefix"": self.prefix,
            ""timeout"": int(self.timeout) if self.timeout else None,  # Garantir que timeout seja um inteiro
            ""path"": self.path if self.path else None,
            ""url"": self.url if self.url else None,
        }

        server_kwargs = {k: v for k, v in server_kwargs.items() if v is not None}
        documents = []

        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if not isinstance(self.embedding, Embeddings):
            raise ValueError(""Invalid embedding object"")

        if documents:
            qdrant = Qdrant.from_documents(documents, embedding=self.embedding, **qdrant_kwargs)
        else:
            from qdrant_client import QdrantClient

            client = QdrantClient(**server_kwargs)
            qdrant = Qdrant(embeddings=self.embedding, client=client, **qdrant_kwargs)

        return qdrant

    def search_documents(self) -> List[Data]:
        vector_store = self._build_qdrant()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",Qdrant,vectorstores,Vector Store Qdrant com capacidades de pesquisa
"from typing import List

from langchain_community.vectorstores.redis import Redis

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput
from langflow.schema import Data
from langchain.text_splitter import CharacterTextSplitter


class RedisVectorStoreComponent(LCVectorStoreComponent):
    """"""
    A custom component for implementing a Vector Store using Redis.
    """"""

    display_name: str = ""Redis""
    description: str = ""Implementation of Vector Store using Redis""
    documentation = ""https://python.langchain.com/docs/integrations/vectorstores/redis""
    name = ""Redis""

    inputs = [
        SecretStrInput(name=""redis_server_url"", display_name=""Redis Server Connection String"", required=True),
        StrInput(
            name=""redis_index_name"",
            display_name=""Redis Index"",
        ),
        StrInput(name=""code"", display_name=""Code"", advanced=True),
        StrInput(
            name=""schema"",
            display_name=""Schema"",
        ),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
    ]

    def build_vector_store(self) -> Redis:
        documents = []

        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)
        with open(""docuemnts.txt"", ""w"") as f:
            f.write(str(documents))

        if not documents:
            if self.schema is None:
                raise ValueError(""If no documents are provided, a schema must be provided."")
            redis_vs = Redis.from_existing_index(
                embedding=self.embedding,
                index_name=self.redis_index_name,
                schema=self.schema,
                key_prefix=None,
                redis_url=self.redis_server_url,
            )
        else:
            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
            docs = text_splitter.split_documents(documents)
            redis_vs = Redis.from_documents(
                documents=docs,
                embedding=self.embedding,
                redis_url=self.redis_server_url,
                index_name=self.redis_index_name,
            )
        return redis_vs

    def search_documents(self) -> List[Data]:
        vector_store = self.build_vector_store()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",Redis,vectorstores,Implementação de Vector Store usando Redis
"from typing import List

from langchain_community.vectorstores import SupabaseVectorStore
from supabase.client import Client, create_client

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput
from langflow.schema import Data


class SupabaseVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""Supabase""
    description = ""Supabase Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/v0.2/docs/integrations/vectorstores/supabase/""
    name = ""SupabaseVectorStore""
    icon = ""Supabase""

    inputs = [
        StrInput(name=""supabase_url"", display_name=""Supabase URL"", required=True),
        SecretStrInput(name=""supabase_service_key"", display_name=""Supabase Service Key"", required=True),
        StrInput(name=""table_name"", display_name=""Table Name"", advanced=True),
        StrInput(name=""query_name"", display_name=""Query Name""),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> SupabaseVectorStore:
        return self._build_supabase()

    def _build_supabase(self) -> SupabaseVectorStore:
        supabase: Client = create_client(self.supabase_url, supabase_key=self.supabase_service_key)

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents:
            supabase_vs = SupabaseVectorStore.from_documents(
                documents=documents,
                embedding=self.embedding,
                query_name=self.query_name,
                client=supabase,
                table_name=self.table_name,
            )
        else:
            supabase_vs = SupabaseVectorStore(
                client=supabase,
                embedding=self.embedding,
                table_name=self.table_name,
                query_name=self.query_name,
            )

        return supabase_vs

    def search_documents(self) -> List[Data]:
        vector_store = self._build_supabase()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",SupabaseVectorStore,vectorstores,Vector Store Supabase com capacidades de pesquisa
"from typing import List

from langchain_community.vectorstores import UpstashVectorStore

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput
from langflow.schema import Data


class UpstashVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""Upstash""
    description = ""Upstash Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/v0.2/docs/integrations/vectorstores/upstash/""
    name = ""Upstash""
    icon = ""Upstash""

    inputs = [
        StrInput(name=""index_url"", display_name=""Index URL"", info=""The URL of the Upstash index."", required=True),
        SecretStrInput(
            name=""index_token"", display_name=""Index Token"", info=""The token for the Upstash index."", required=True
        ),
        StrInput(
            name=""text_key"",
            display_name=""Text Key"",
            info=""The key in the record to use as text."",
            value=""text"",
            advanced=True,
        ),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(
            name=""embedding"",
            display_name=""Embedding"",
            input_types=[""Embeddings""],
            info=""To use Upstash's embeddings, don't provide an embedding."",
        ),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> UpstashVectorStore:
        return self._build_upstash()

    def _build_upstash(self) -> UpstashVectorStore:
        use_upstash_embedding = self.embedding is None

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents:
            if use_upstash_embedding:
                upstash_vs = UpstashVectorStore(
                    embedding=use_upstash_embedding,
                    text_key=self.text_key,
                    index_url=self.index_url,
                    index_token=self.index_token,
                )
                upstash_vs.add_documents(documents)
            else:
                upstash_vs = UpstashVectorStore.from_documents(
                    documents=documents,
                    embedding=self.embedding,
                    text_key=self.text_key,
                    index_url=self.index_url,
                    index_token=self.index_token,
                )
        else:
            upstash_vs = UpstashVectorStore(
                embedding=self.embedding or use_upstash_embedding,
                text_key=self.text_key,
                index_url=self.index_url,
                index_token=self.index_token,
            )

        return upstash_vs

    def search_documents(self) -> List[Data]:
        vector_store = self._build_upstash()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",Upstash,vectorstores,Vector Store Upstash com capacidades de pesquisa
"from typing import TYPE_CHECKING, List

from langchain_community.vectorstores import Vectara
from loguru import logger

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import HandleInput, IntInput, MessageTextInput, SecretStrInput, StrInput
from langflow.schema import Data

if TYPE_CHECKING:
    from langchain_community.vectorstores import Vectara


class VectaraVectorStoreComponent(LCVectorStoreComponent):
    """"""
    Vectara Vector Store with search capabilities
    """"""

    display_name: str = ""Vectara""
    description: str = ""Vectara Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/vectara""
    name = ""Vectara""
    icon = ""Vectara""

    inputs = [
        StrInput(name=""vectara_customer_id"", display_name=""Vectara Customer ID"", required=True),
        StrInput(name=""vectara_corpus_id"", display_name=""Vectara Corpus ID"", required=True),
        SecretStrInput(name=""vectara_api_key"", display_name=""Vectara API Key"", required=True),
        HandleInput(
            name=""embedding"",
            display_name=""Embedding"",
            input_types=[""Embeddings""],
        ),
        HandleInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            input_types=[""Document"", ""Data""],
            is_list=True,
        ),
        MessageTextInput(
            name=""search_query"",
            display_name=""Search Query"",
        ),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
    ]

    def build_vector_store(self) -> ""Vectara"":
        """"""
        Builds the Vectara object.
        """"""
        try:
            from langchain_community.vectorstores import Vectara
        except ImportError:
            raise ImportError(""Could not import Vectara. Please install it with `pip install langchain-community`."")

        vectara = Vectara(
            vectara_customer_id=self.vectara_customer_id,
            vectara_corpus_id=self.vectara_corpus_id,
            vectara_api_key=self.vectara_api_key,
        )

        self._add_documents_to_vector_store(vectara)
        return vectara

    def _add_documents_to_vector_store(self, vector_store: ""Vectara"") -> None:
        """"""
        Adds documents to the Vector Store.
        """"""
        if not self.ingest_data:
            self.status = ""No documents to add to Vectara""
            return

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents:
            logger.debug(f""Adding {len(documents)} documents to Vectara."")
            vector_store.add_documents(documents)
            self.status = f""Added {len(documents)} documents to Vectara""
        else:
            logger.debug(""No documents to add to Vectara."")
            self.status = ""No valid documents to add to Vectara""

    def search_documents(self) -> List[Data]:
        vector_store = self.build_vector_store()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = f""Found {len(data)} results for the query: {self.search_query}""
            return data
        else:
            self.status = ""No search query provided""
            return []
",Vectara,vectorstores,Vector Store Vectara com capacidades de pesquisa
"from langflow.custom import Component
from langflow.field_typing.range_spec import RangeSpec
from langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output
from langflow.schema.message import Message


class VectaraRagComponent(Component):
    display_name = ""Vectara RAG""
    description = ""Vectara's full end to end RAG""
    documentation = ""https://docs.vectara.com/docs""
    icon = ""Vectara""
    name = ""VectaraRAG""
    SUMMARIZER_PROMPTS = [
        ""vectara-summary-ext-24-05-sml"",
        ""vectara-summary-ext-24-05-med-omni"",
        ""vectara-summary-ext-24-05-large"",
        ""vectara-summary-ext-24-05-med"",
        ""vectara-summary-ext-v1.3.0"",
    ]

    RERANKER_TYPES = [""mmr"", ""rerank_multilingual_v1"", ""none""]

    field_order = [""vectara_customer_id"", ""vectara_corpus_id"", ""vectara_api_key"", ""search_query"", ""reranker""]
    #  return {
    #         ""vectara_customer_id"": {""display_name"": ""Vectara Customer ID"", ""field_type"": ""str"", ""required"": True},
    #         ""vectara_corpus_id"": {""display_name"": ""Vectara Corpus ID"", ""field_type"": ""str"", ""required"": True},
    #         ""vectara_api_key"": {""display_name"": ""Vectara API Key"", ""field_type"": ""str"", ""required"": True},
    #         ""search_query"": {""display_name"": ""Search Query"", ""field_type"": ""str"", ""info"": ""The query to receive an answer on."", ""required"": True},
    #         ""lexical_interpolation"": {""display_name"": ""Hybrid Search Factor"", ""field_type"": ""float"", ""value"": 0.005, ""info"": ""How much to weigh lexical scores compared to the embedding score. 0 means lexical search is not used at all, and 1 means only lexical search is used."", ""advanced"": True},
    #         ""filter"": {""display_name"": ""Metadata Filters"", ""field_type"": ""str"", ""value"": '', ""info"": ""The filter string to narrow the search to according to metadata attributes."", ""advanced"": True},
    #         ""reranker"": {""display_name"": ""Reranker Type"", ""options"": RERANKER_TYPES, ""value"": RERANKER_TYPES[0], ""info"": ""How to rerank the retrieved search results.""},
    #         ""reranker_k"": {""display_name"": ""Number of Results to Rerank"", ""field_type"": ""int"", ""value"": 50, ""advanced"": True},
    #         ""diversity_bias"": {""display_name"": ""Diversity Bias"", ""field_type"": ""float"", ""value"": 0.2, ""info"": ""Ranges from 0 to 1, with higher values indicating greater diversity (only applies to MMR reranker)."", ""advanced"": True},
    #         ""max_results"": {""display_name"": ""Max Results to Summarize"", ""field_type"": ""int"", ""value"": 7, ""info"": ""The maximum number of search results to be available to the prompt."", ""advanced"": True},
    #         ""response_lang"": {""display_name"": ""Response Language"", ""field_type"": ""str"", ""value"": ""eng"", ""info"": ""Use the ISO 639-1 or 639-3 language code or auto to automatically detect the language."", ""advanced"": True},
    #         ""prompt"": {""display_name"": ""Prompt Name"", ""options"": SUMMARIZER_PROMPTS, ""value"": SUMMARIZER_PROMPTS[0], ""info"": ""Only vectara-summary-ext-24-05-sml is for Growth customers; all other prompts are for Scale customers only."", ""advanced"": True}
    #     }

    inputs = [
        MessageTextInput(name=""search_query"", display_name=""Search Query"", info=""The query to receive an answer on.""),
        FloatInput(
            name=""lexical_interpolation"",
            display_name=""Hybrid Search Factor"",
            range_spec=RangeSpec(min=0.005, max=0.1, step=0.005),
            value=0.005,
            info=""How much to weigh lexical scores compared to the embedding score. 0 means lexical search is not used at all, and 1 means only lexical search is used."",
        ),
        MessageTextInput(
            name=""filter"",
            display_name=""Metadata Filters"",
            value="""",
            info=""The filter string to narrow the search to according to metadata attributes."",
        ),
        DropdownInput(
            name=""reranker"",
            display_name=""Reranker Type"",
            options=RERANKER_TYPES,
            value=RERANKER_TYPES[0],
            info=""How to rerank the retrieved search results."",
        ),
        IntInput(
            name=""reranker_k"",
            display_name=""Number of Results to Rerank"",
            value=50,
            range_spec=RangeSpec(min=1, max=100, step=1),
        ),
        FloatInput(
            name=""diversity_bias"",
            display_name=""Diversity Bias"",
            value=0.2,
            range_spec=RangeSpec(min=0, max=1, step=0.01),
            info=""Ranges from 0 to 1, with higher values indicating greater diversity (only applies to MMR reranker)."",
        ),
        IntInput(
            name=""max_results"",
            display_name=""Max Results to Summarize"",
            value=7,
            range_spec=RangeSpec(min=1, max=100, step=1),
        ),
        DropdownInput(
            name=""response_lang"",
            display_name=""Response Language"",
            options=[""auto"", ""eng"", ""deu"", ""fra"", ""ita"", ""nld"", ""por"", ""rus"", ""spa"", ""zho""],
            value=""eng"",
            info=""Use the ISO 639-1 or 639-3 language code or auto to automatically detect the language."",
        ),
        DropdownInput(
            name=""prompt"",
            display_name=""Prompt Name"",
            options=SUMMARIZER_PROMPTS,
            value=SUMMARIZER_PROMPTS[0],
            info=""Only vectara-summary-ext-24-05-sml is for Growth customers; all other prompts are for Scale customers only."",
        ),
    ]

    outputs = [
        Output(name=""answer"", display_name=""Answer"", method=""generate_response""),
    ]

    def generate_response(
        self,
    ) -> Message:
        text_output = """"

        try:
            from langchain_community.vectorstores import Vectara
            from langchain_community.vectorstores.vectara import RerankConfig, SummaryConfig, VectaraQueryConfig
        except ImportError:
            raise ImportError(""Could not import Vectara. Please install it with `pip install langchain-community`."")

        vectara = Vectara(self.vectara_customer_id, self.vectara_corpus_id, self.vectara_api_key)
        rerank_config = RerankConfig(self.reranker, self.reranker_k, self.diversity_bias)
        summary_config = SummaryConfig(
            is_enabled=True, max_results=self.max_results, response_lang=self.response_lang, prompt_name=self.prompt
        )
        config = VectaraQueryConfig(
            lambda_val=self.lexical_interpolation,
            filter=self.filter,
            summary_config=summary_config,
            rerank_config=rerank_config,
        )
        rag = vectara.as_rag(config)
        response = rag.invoke(self.search_query)

        text_output = response[""answer""]

        return Message(text=text_output)
",vectara_rag,vectorstores,RAG completo da ponta à ponta da Vectara
"from typing import List

import weaviate  # type: ignore
from langchain_community.vectorstores import Weaviate

from langflow.base.vectorstores.model import LCVectorStoreComponent
from langflow.helpers.data import docs_to_data
from langflow.io import BoolInput, HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput
from langflow.schema import Data


class WeaviateVectorStoreComponent(LCVectorStoreComponent):
    display_name = ""Weaviate""
    description = ""Weaviate Vector Store with search capabilities""
    documentation = ""https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/weaviate""
    name = ""Weaviate""
    icon = ""Weaviate""

    inputs = [
        StrInput(name=""url"", display_name=""Weaviate URL"", value=""http://localhost:8080"", required=True),
        SecretStrInput(name=""api_key"", display_name=""API Key"", required=False),
        StrInput(name=""index_name"", display_name=""Index Name"", required=True),
        StrInput(name=""text_key"", display_name=""Text Key"", value=""text"", advanced=True),
        MultilineInput(name=""search_query"", display_name=""Search Query""),
        DataInput(
            name=""ingest_data"",
            display_name=""Ingest Data"",
            is_list=True,
        ),
        HandleInput(name=""embedding"", display_name=""Embedding"", input_types=[""Embeddings""]),
        IntInput(
            name=""number_of_results"",
            display_name=""Number of Results"",
            info=""Number of results to return."",
            value=4,
            advanced=True,
        ),
        BoolInput(name=""search_by_text"", display_name=""Search By Text"", advanced=True),
    ]

    def build_vector_store(self) -> Weaviate:
        return self._build_weaviate()

    def _build_weaviate(self) -> Weaviate:
        if self.api_key:
            auth_config = weaviate.AuthApiKey(api_key=self.api_key)
            client = weaviate.Client(url=self.url, auth_client_secret=auth_config)
        else:
            client = weaviate.Client(url=self.url)

        documents = []
        for _input in self.ingest_data or []:
            if isinstance(_input, Data):
                documents.append(_input.to_lc_document())
            else:
                documents.append(_input)

        if documents and self.embedding:
            return Weaviate.from_documents(
                client=client,
                index_name=self.index_name,
                documents=documents,
                embedding=self.embedding,
                by_text=self.search_by_text,
            )

        return Weaviate(
            client=client,
            index_name=self.index_name,
            text_key=self.text_key,
            embedding=self.embedding,
            by_text=self.search_by_text,
        )

    def search_documents(self) -> List[Data]:
        vector_store = self._build_weaviate()

        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():
            docs = vector_store.similarity_search(
                query=self.search_query,
                k=self.number_of_results,
            )

            data = docs_to_data(docs)
            self.status = data
            return data
        else:
            return []
",Weaviate,vectorstores,Vector Store Weaviate com capacidades de pesquisa
